# Tutoriels Machine Learning - Scripts DÃ©taillÃ©s

Ce dossier contient des tutoriels complets en Python pour implÃ©menter, optimiser et valider diffÃ©rents modÃ¨les ML.

## ğŸ“š Tutoriels Disponibles

### âœ… Tuto_01_Regression_Lineaire.py (~800 lignes)
**RÃ©gression linÃ©aire, Ridge et Lasso**
- ThÃ©orie et formules mathÃ©matiques (OLS, L1, L2)
- Pourquoi normaliser les donnÃ©es
- RÃ©gression linÃ©aire simple, Ridge, Lasso
- Cross-validation et choix d'hyperparamÃ¨tres
- Analyse des rÃ©sidus et diagnostics
- Feature importance et interprÃ©tation des coefficients
- Visualisations complÃ¨tes (rÃ©sidus, Q-Q plot, prÃ©dictions)
- **6 observations dÃ©taillÃ©es** sur normalisation, rÃ©gularisation, overfitting

### âœ… Tuto_02_Classification_Complete.py (~1000 lignes)
**Logistic Regression, Decision Trees**
- Pourquoi et quand utiliser chaque modÃ¨le (tableau dÃ©cisionnel)
- Cas d'usage : prÃ©diction de risque crÃ©dit
- Logistic Regression avec rÃ©gularisation
- Decision Trees avec contrÃ´le de profondeur
- Gestion du dÃ©sÃ©quilibre de classes (class_weight='balanced')
- MÃ©triques : Accuracy, Precision, Recall, F1-Score, ROC-AUC
- Confusion Matrix et interprÃ©tation mÃ©tier
- **12 observations dÃ©taillÃ©es** sur :
  - Normalisation pour Logistic Regression
  - InterprÃ©tation probabilitÃ©s et coefficients
  - Overfitting des arbres
  - Class imbalance et impact Precision/Recall

### âœ… Tuto_03_Random_Forest_XGBoost.py (~1100 lignes)
**MÃ©thodes d'ensemble : Random Forest, XGBoost, LightGBM**
- Pourquoi et quand utiliser ensembles vs modÃ¨les simples
- Cas d'usage : prÃ©diction de churn client
- Random Forest : n_estimators, max_depth, feature importance
- XGBoost : learning_rate, early stopping, tree-based models
- LightGBM : rapiditÃ© sur gros volumes
- Comparaison des 3 mÃ©thodes (performance, rapiditÃ©)
- **7 observations dÃ©taillÃ©es** sur :
  - Baseline importance (modÃ¨le simple)
  - Feature importance et redondance
  - Early stopping pour Ã©viter overfitting
  - Trade-off vitesse/performance

### âœ… Tuto_04_Neural_Networks.py (~1300 lignes)
**RÃ©seaux de neurones denses (MLP)**
- Pourquoi NN vs modÃ¨les classiques (relations non-linÃ©aires complexes)
- Quand utiliser : grandes datasets, features nombreuses, non-linÃ©aritÃ©
- Architecture : couches cachÃ©es, nombre de neurones
- Fonctions d'activation : ReLU, Sigmoid, Tanh
- Optimiseurs : SGD, Adam, RMSprop (comparaison dÃ©taillÃ©e)
- RÃ©gularisation : Dropout, L2, Batch Normalization
- Learning rate et convergence
- **6 observations dÃ©taillÃ©es** sur :
  - Importance CRUCIALE de la normalisation
  - Choix de l'optimiseur (Adam > SGD en gÃ©nÃ©ral)
  - Impact de Dropout sur overfitting
  - Learning rate et oscillations

### âœ… Tuto_05_CNN_Images.py (~1400 lignes)
**RÃ©seaux de neurones convolutifs pour images**
- Pourquoi CNN pour images vs Dense Networks
- Principe de la convolution (explication visuelle avec dÃ©monstration code)
- Architecture : Convolution â†’ Pooling â†’ Dense
- Data Augmentation : rotation, shift, zoom (avec visualisations)
- Cas d'usage : classification MNIST (chiffres manuscrits)
- Architecture VGG-like pour MNIST
- Visualisation des feature maps (ce que le rÃ©seau "voit")
- **7 observations dÃ©taillÃ©es** sur :
  - Avantages convolution (invariance, partage de poids)
  - Pooling et rÃ©duction de dimensionnalitÃ©
  - Data Augmentation pour gÃ©nÃ©ralisation
  - InterprÃ©tation des feature maps

### âœ… Tuto_06_Clustering.py (~1200 lignes)
**Apprentissage non supervisÃ© : K-Means, DBSCAN, HiÃ©rarchique**
- DiffÃ©rence supervisÃ©/non supervisÃ©
- Pourquoi et quand utiliser chaque algorithme (tableaux comparatifs)
- Cas d'usage : segmentation client e-commerce
- K-Means : Elbow Method, Silhouette Score pour choisir k
- DBSCAN : k-distance plot pour eps optimal, dÃ©tection outliers
- Clustering hiÃ©rarchique : dendrogramme, linkage methods
- PCA pour visualisation 2D
- Ã‰valuation : Silhouette, Davies-Bouldin Index
- InterprÃ©tation mÃ©tier : nommer et actionner les segments
- **7 observations dÃ©taillÃ©es** sur :
  - InterprÃ©tation Elbow Method et Silhouette
  - Profils de clusters et stratÃ©gies marketing
  - DBSCAN vs K-Means (outliers, forme clusters)
  - Validation mÃ©tier vs mÃ©triques techniques

## ğŸ¯ Format des Tutoriels

Chaque tutoriel suit cette structure :

```
1. THÃ‰ORIE
   - Formules mathÃ©matiques
   - Principes et hypothÃ¨ses
   - Cas d'usage

2. PRÃ‰PARATION DES DONNÃ‰ES
   - Chargement
   - Exploration (EDA)
   - Nettoyage
   - Feature engineering
   - Normalisation

3. BASELINE
   - ModÃ¨le simple de rÃ©fÃ©rence

4. MODÃ‰LISATION
   - EntraÃ®nement
   - ParamÃ¨tres expliquÃ©s
   - PrÃ©dictions

5. VALIDATION
   - Cross-validation
   - MÃ©triques dÃ©taillÃ©es

6. OPTIMISATION
   - HyperparamÃ¨tres
   - Comparaison de variantes

7. Ã‰VALUATION FINALE
   - Test set
   - Analyse des erreurs

8. INTERPRÃ‰TATION
   - Importance des features
   - Diagnostics

9. SAUVEGARDE
   - SÃ©rialisation du modÃ¨le
```

## ğŸ’¡ Comment Utiliser les Tutoriels

### Option 1 : ExÃ©cuter comme script Python

```bash
python Tuto_01_Regression_Lineaire.py
```

### Option 2 : Convertir en Jupyter Notebook

```bash
# Installer p2j si nÃ©cessaire
pip install p2j

# Convertir
p2j Tuto_01_Regression_Lineaire.py
```

### Option 3 : Copier dans Jupyter

1. Ouvrir Jupyter Notebook
2. CrÃ©er un nouveau notebook
3. Copier le code par sections
4. ExÃ©cuter cellule par cellule

## ğŸ“– Concepts ClÃ©s ExpliquÃ©s dans les Tutoriels

### ğŸ”§ Fonctions et ParamÃ¨tres DÃ©taillÃ©s

**Preprocessing :**
- `StandardScaler()` : Pourquoi normaliser, impact sur convergence
- `train_test_split()` : Stratification, random_state
- `SimpleImputer()` : Gestion valeurs manquantes

**ModÃ¨les Classiques :**
- `LinearRegression()` : OLS, hypothÃ¨ses, interprÃ©tation coefficients
- `Ridge(alpha=...)` : RÃ©gularisation L2, choix d'alpha, cross-validation
- `Lasso(alpha=...)` : RÃ©gularisation L1, sÃ©lection de features
- `LogisticRegression()` : ProbabilitÃ©s, C (inverse rÃ©gularisation), class_weight
- `DecisionTreeClassifier()` : max_depth, min_samples_split, overfitting
- `RandomForestClassifier()` : n_estimators, max_features, feature importance
- `XGBClassifier()` : learning_rate, n_estimators, early_stopping_rounds
- `LGBMClassifier()` : num_leaves, boosting_type, rapiditÃ©

**MÃ©thodes d'Ensemble :**
- Bagging (Random Forest) vs Boosting (XGBoost/LightGBM)
- Early stopping : Ã©viter overfitting
- Feature importance : interpretation et diagnostics

**Deep Learning :**
- `Sequential()` : Construction modÃ¨le Keras
- `Dense(units, activation)` : Couches fully connected
- `Conv2D(filters, kernel_size)` : Convolution pour images
- `MaxPooling2D()` : RÃ©duction dimensionnalitÃ©
- `Dropout(rate)` : RÃ©gularisation
- `BatchNormalization()` : StabilitÃ© entraÃ®nement
- Optimizers : `Adam`, `SGD`, `RMSprop` (comparaison)
- `ImageDataGenerator()` : Data augmentation
- `fit()` : batch_size, epochs, validation_split, callbacks

**Clustering :**
- `KMeans(n_clusters)` : CentroÃ¯des, inertie
- `DBSCAN(eps, min_samples)` : DensitÃ©, outliers
- `AgglomerativeClustering()` : HiÃ©rarchique, linkage methods
- `PCA(n_components)` : RÃ©duction dimensionnalitÃ©, variance expliquÃ©e
- `silhouette_score()` : QualitÃ© clusters
- `dendrogram()` : Visualisation hiÃ©rarchie

### ğŸ“Š MÃ©triques d'Ã‰valuation

**RÃ©gression :**
- MSE, RMSE, MAE : InterprÃ©tation et unitÃ©s
- RÂ² : Variance expliquÃ©e
- Analyse rÃ©sidus : homoscÃ©dasticitÃ©, normalitÃ©

**Classification :**
- Accuracy : Quand utiliser (classes Ã©quilibrÃ©es)
- Precision : Minimiser faux positifs
- Recall : Minimiser faux nÃ©gatifs
- F1-Score : Compromis Precision/Recall
- ROC-AUC : Discrimination classes
- Confusion Matrix : InterprÃ©tation mÃ©tier
- Log-Loss : QualitÃ© probabilitÃ©s

**Clustering :**
- Silhouette Score : SÃ©paration clusters
- Davies-Bouldin Index : Chevauchement
- Inertie : Distance aux centroÃ¯des
- Elbow Method : Choix k optimal

### ğŸ“Š Visualisations GÃ©nÃ©rÃ©es par Tutoriel

**Tuto 01 (RÃ©gression) :**
- Distribution features et target
- Matrice de corrÃ©lation
- PrÃ©dictions vs rÃ©alitÃ©
- RÃ©sidus (distribution, Q-Q plot)
- Cross-validation scores
- Comparaison coefficients Ridge/Lasso
- Feature importance

**Tuto 02 (Classification) :**
- Distribution classes
- Courbes ROC (avec AUC)
- Confusion matrices
- Feature importance
- FrontiÃ¨res de dÃ©cision (2D)
- Comparaison Logistic vs Tree

**Tuto 03 (Ensembles) :**
- Comparaison performances (barplot)
- Feature importance (3 modÃ¨les)
- Learning curves (early stopping)
- Temps d'entraÃ®nement
- Confusion matrices comparÃ©es

**Tuto 04 (Neural Networks) :**
- Loss curves (train/validation)
- Comparaison optimizers
- Impact Dropout
- Impact learning rate
- Architecture rÃ©seau (diagramme)

**Tuto 05 (CNN) :**
- DÃ©monstration convolution (visuelle)
- Exemples data augmentation (avant/aprÃ¨s)
- Architecture CNN (diagramme)
- Feature maps (visualisation)
- Loss/Accuracy curves
- Erreurs de classification

**Tuto 06 (Clustering) :**
- Elbow Method + Silhouette
- Silhouette plot par cluster
- PCA 2D (visualisation clusters)
- k-distance plot (DBSCAN)
- Dendrogramme (hiÃ©rarchique)
- Boxplots features par cluster
- Comparaison 3 mÃ©thodes

## ğŸ”§ DÃ©pendances

### Installation ComplÃ¨te

```bash
# Core ML et Data Science
pip install numpy pandas matplotlib seaborn scikit-learn scipy

# Deep Learning (Tuto 04, 05)
pip install tensorflow keras

# Gradient Boosting (Tuto 03)
pip install xgboost lightgbm

# Optionnel : Sauvegarde modÃ¨les
pip install joblib
```

### Versions RecommandÃ©es

- Python : 3.8+
- NumPy : 1.21+
- Pandas : 1.3+
- Scikit-learn : 1.0+
- TensorFlow : 2.8+
- XGBoost : 1.5+
- LightGBM : 3.3+

## ğŸ“Š Sorties GÃ©nÃ©rÃ©es

Chaque tutoriel gÃ©nÃ¨re :
- **Graphiques** (.png) sauvegardÃ©s dans le mÃªme dossier
- **ModÃ¨les** (.pkl) prÃªts pour dÃ©ploiement
- **Rapports** dans la console avec mÃ©triques dÃ©taillÃ©es

## ğŸ“ Utilisation PÃ©dagogique

Ces tutoriels sont conÃ§us pour :
- âœ… Apprendre en pratiquant
- âœ… Comprendre chaque paramÃ¨tre et fonction
- âœ… Voir l'impact de chaque choix
- âœ… Suivre les meilleures pratiques
- âœ… Avoir un template rÃ©utilisable

## ğŸ’¡ Conseils d'Utilisation

1. **DÃ©butant** : ExÃ©cuter le script complet et lire les sorties
2. **IntermÃ©diaire** : Modifier les paramÃ¨tres et observer les changements
3. **AvancÃ©** : Adapter le code Ã  vos propres donnÃ©es

### Parcours d'Apprentissage RecommandÃ©

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1 : FONDAMENTAUX                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Tuto_01_Regression_Lineaire.py                      â”‚
â”‚     â†’ Comprendre normalisation, train/test, mÃ©triques   â”‚
â”‚  2. Tuto_02_Classification_Complete.py                  â”‚
â”‚     â†’ MÃ©triques classification, class imbalance         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 2 : MÃ‰THODES AVANCÃ‰ES                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  3. Tuto_03_Random_Forest_XGBoost.py                    â”‚
â”‚     â†’ Ensembles, feature importance, early stopping     â”‚
â”‚  4. Tuto_06_Clustering.py                               â”‚
â”‚     â†’ Non supervisÃ©, segmentation, PCA                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 3 : DEEP LEARNING                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  5. Tuto_04_Neural_Networks.py                          â”‚
â”‚     â†’ Architecture NN, optimizers, rÃ©gularisation       â”‚
â”‚  6. Tuto_05_CNN_Images.py                               â”‚
â”‚     â†’ Convolution, augmentation, feature maps           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š RÃ©capitulatif des Tutoriels

| Tutoriel | Lignes | Observations | Cas d'Usage | Temps EstimÃ© |
|----------|--------|--------------|-------------|--------------|
| 01 - RÃ©gression | ~800 | 6 | Prix immobilier | 1-2h |
| 02 - Classification | ~1000 | 12 | Risque crÃ©dit | 1.5-2h |
| 03 - Ensembles | ~1100 | 7 | Churn client | 1.5-2h |
| 04 - Neural Networks | ~1300 | 6 | Classification gÃ©nÃ©rale | 2-3h |
| 05 - CNN | ~1400 | 7 | Chiffres MNIST | 2-3h |
| 06 - Clustering | ~1200 | 7 | Segmentation client | 1.5-2h |
| **TOTAL** | **~6800** | **45** | **6 domaines** | **10-14h** |

## ğŸ¯ CompÃ©tences Acquises

AprÃ¨s avoir complÃ©tÃ© les 6 tutoriels, vous maÃ®triserez :

âœ… **Preprocessing et Feature Engineering**
- Normalisation (StandardScaler, MinMaxScaler)
- Gestion valeurs manquantes
- CrÃ©ation de features
- Train/test split stratifiÃ©

âœ… **ModÃ¨les de Machine Learning**
- RÃ©gression : Linear, Ridge, Lasso
- Classification : Logistic, Trees, Random Forest, XGBoost
- Clustering : K-Means, DBSCAN, HiÃ©rarchique
- Deep Learning : Dense Networks, CNN

âœ… **Optimisation et Validation**
- Cross-validation (K-Fold, Stratified)
- Grid Search hyperparamÃ¨tres
- Early stopping
- RÃ©gularisation (L1, L2, Dropout)

âœ… **Ã‰valuation et Diagnostics**
- MÃ©triques : RÂ², MSE, Accuracy, Precision, Recall, F1, ROC-AUC, Silhouette
- Confusion Matrix
- Analyse rÃ©sidus
- Feature importance
- Overfitting/Underfitting detection

âœ… **Visualisation**
- Matplotlib et Seaborn
- Courbes ROC, learning curves
- Feature maps (CNN)
- Dendrogrammes, PCA
- Diagnostics visuels

âœ… **Production**
- Sauvegarde modÃ¨les (joblib, pickle)
- Pipeline de preprocessing
- Scoring nouveaux points
- InterprÃ©tation mÃ©tier

## ğŸ”— Ressources ComplÃ©mentaires

- [00_Guide_Projet_ML.md](../00_Guide_Projet_ML.md) - Checklist complÃ¨te projet ML
- [00_Guide_Decision_ML.md](../00_Guide_Decision_ML.md) - Quel modÃ¨le choisir ?
- [00_Workflows_ML.md](../00_Workflows_ML.md) - Workflows Ã©tape par Ã©tape

## ğŸ“§ Support

Pour toute question sur les tutoriels :
- Consulter la documentation Scikit-Learn : https://scikit-learn.org/
- TensorFlow/Keras : https://www.tensorflow.org/
- Lire les commentaires dÃ©taillÃ©s dans le code (sections "OBSERVATION")
- VÃ©rifier les guides principaux (00_Guide_*.md)

## ğŸ† Prochaines Ã‰tapes

AprÃ¨s avoir maÃ®trisÃ© ces tutoriels :

1. **Appliquer sur vos propres donnÃ©es**
   - Adapter les scripts Ã  votre contexte mÃ©tier
   - ExpÃ©rimenter avec diffÃ©rents paramÃ¨tres

2. **Participer Ã  des compÃ©titions Kaggle**
   - Mettre en pratique sur problÃ¨mes rÃ©els
   - Apprendre des kernels de la communautÃ©

3. **Approfondir des sujets spÃ©cifiques**
   - Transfer Learning (VGG, ResNet, BERT)
   - SÃ©ries temporelles (LSTM, Prophet)
   - NLP (Transformers, BERT, GPT)
   - Reinforcement Learning

4. **DÃ©ployer en production**
   - APIs avec Flask/FastAPI
   - Conteneurisation (Docker)
   - MLOps (MLflow, Kubeflow)

---

**ğŸ¯ Formation complÃ¨te de 6 tutoriels totalisant ~6800 lignes de code commentÃ© et 45 observations dÃ©taillÃ©es !**

**Objectif : MaÃ®triser le ML en comprenant chaque dÃ©tail, paramÃ¨tre et dÃ©cision !**
