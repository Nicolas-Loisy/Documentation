"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TUTORIEL 06 : CLUSTERING ET APPRENTISSAGE NON SUPERVISÃ‰
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“š OBJECTIFS :
    - Comprendre les diffÃ©rents algorithmes de clustering
    - Savoir choisir entre K-Means, DBSCAN, et Clustering HiÃ©rarchique
    - Ã‰valuer la qualitÃ© des clusters (Silhouette, Elbow Method)
    - RÃ©duire la dimensionnalitÃ© pour visualiser (PCA)
    - InterprÃ©ter les segments obtenus

ğŸ¯ CAS D'USAGE :
    - Segmentation client (marketing)
    - DÃ©tection d'anomalies
    - Organisation de documents
    - Analyse de comportements
    - Compression d'images

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PARTIE 1 : THÃ‰ORIE ET DÃ‰CISION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

print("="*80)
print("PARTIE 1 : POURQUOI UTILISER LE CLUSTERING ?")
print("="*80)

print("""
ğŸ” CLUSTERING = APPRENTISSAGE NON SUPERVISÃ‰

   Pas de labels (y) ! On cherche des groupes naturels dans les donnÃ©es.

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  DIFFÃ‰RENCE FONDAMENTALE                                     â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  SupervisÃ©   : X, y  â†’ ModÃ¨le prÃ©dit y                      â”‚
   â”‚  Non supervisÃ© : X   â†’ ModÃ¨le trouve des groupes            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’¡ POURQUOI UTILISER LE CLUSTERING ?

   1. SEGMENTATION CLIENT
      â†’ Identifier des profils clients similaires
      â†’ Marketing ciblÃ©, personnalisation

   2. DÃ‰TECTION D'ANOMALIES
      â†’ Points qui n'appartiennent Ã  aucun cluster
      â†’ Fraude, dÃ©faillances techniques

   3. COMPRESSION/ORGANISATION
      â†’ Regrouper documents similaires
      â†’ Compression d'images (K-means sur couleurs)

   4. PREPROCESSING
      â†’ CrÃ©er de nouvelles features (cluster_id)
      â†’ Identifier des sous-populations avant modÃ¨le supervisÃ©
""")

print("\n" + "="*80)
print("COMPARAISON DES ALGORITHMES DE CLUSTERING")
print("="*80)

print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ALGORITHME    â”‚   K-MEANS    â”‚   DBSCAN     â”‚  HIÃ‰RARCHIQUE   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Principe        â”‚ CentroÃ¯des   â”‚ DensitÃ©      â”‚ AgglomÃ©ration   â”‚
â”‚                 â”‚ + distance   â”‚ spatiale     â”‚ successive      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Forme clusters  â”‚ SphÃ©riques   â”‚ Arbitraires  â”‚ Arbitraires     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Nb clusters     â”‚ Ã€ SPÃ‰CIFIER  â”‚ AUTOMATIQUE  â”‚ Ã€ SPÃ‰CIFIER     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Outliers        â”‚ NON          â”‚ OUI (noise)  â”‚ NON (ou isolÃ©s) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ScalabilitÃ©     â”‚ âœ“âœ“âœ“ Rapide   â”‚ âœ“âœ“ Moyen     â”‚ âœ— Lent (>10k)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ComplexitÃ©      â”‚ O(nÂ·kÂ·i)     â”‚ O(n log n)   â”‚ O(nÂ²) ou O(nÂ³)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… UTILISER K-MEANS QUAND :
   - Nombre de clusters connu approximativement
   - Clusters de forme sphÃ©rique/convexe
   - Besoin de RAPIDITÃ‰ (gros volumes)
   - Toutes les donnÃ©es sont pertinentes (pas d'outliers)

âœ… UTILISER DBSCAN QUAND :
   - Nombre de clusters INCONNU
   - Clusters de forme ARBITRAIRE (spirales, anneaux...)
   - PrÃ©sence d'OUTLIERS Ã  identifier
   - DensitÃ© variable dans l'espace

âœ… UTILISER CLUSTERING HIÃ‰RARCHIQUE QUAND :
   - Besoin de VISUALISER la hiÃ©rarchie (dendrogramme)
   - Petit dataset (<10 000 points)
   - Exploration : tester plusieurs nb de clusters
   - Besoin de clusters imbriquÃ©s
""")

print("\n" + "="*80)
print("COMMENT CHOISIR LE NOMBRE DE CLUSTERS ?")
print("="*80)

print("""
ğŸ”§ MÃ‰THODES POUR K-MEANS :

1. ELBOW METHOD (MÃ©thode du coude)
   â†’ Tracer inertie (somme distancesÂ²) vs k
   â†’ Chercher le "coude" oÃ¹ la dÃ©croissance ralentit

   Inertie
      â”‚    \\
      â”‚     \\___    â† "Coude" ici : k optimal
      â”‚          \\___
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> k

2. SILHOUETTE SCORE
   â†’ Mesure combien chaque point est proche de son cluster
      vs clusters voisins
   â†’ Score de -1 (mal classÃ©) Ã  +1 (bien classÃ©)
   â†’ Viser score > 0.5

   Formule : s = (b - a) / max(a, b)
   oÃ¹ :
     a = distance moyenne intra-cluster
     b = distance moyenne au cluster voisin le plus proche

3. BUSINESS KNOWLEDGE
   â†’ Le nombre de segments doit avoir du SENS mÃ©tier
   â†’ Ex : 3-5 segments clients (Bronze/Silver/Gold/Platinum)

ğŸ”§ POUR DBSCAN : CHOISIR eps ET min_samples

   eps         : Rayon de voisinage
   min_samples : Nb min de points pour former un cluster

   MÃ©thode :
   1. Calculer k-distance plot (distance au k-Ã¨me voisin)
   2. Chercher le "coude" â†’ valeur eps
   3. min_samples â‰ˆ 2 Ã— nb_features (rÃ¨gle empirique)
""")

print("\n" + "="*80)
print("PARTIE 2 : PRÃ‰PARATION DES DONNÃ‰ES")
print("="*80)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_blobs, make_moons
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, silhouette_samples, davies_bouldin_score
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import cdist
import warnings
warnings.filterwarnings('ignore')

# Configuration
np.random.seed(42)
plt.style.use('seaborn-v0_8-darkgrid')

print("""
ğŸ“Š CAS D'USAGE : SEGMENTATION CLIENT

Contexte : Entreprise e-commerce avec donnÃ©es clients
Objectif : Identifier des segments pour stratÃ©gie marketing ciblÃ©e

Features disponibles :
  - Age : Ã‚ge du client
  - Income : Revenu annuel (kâ‚¬)
  - SpendingScore : Score de dÃ©penses (1-100)
  - Recency : Jours depuis dernier achat
  - Frequency : Nombre d'achats sur l'annÃ©e
""")

# GÃ©nÃ©ration de donnÃ©es rÃ©alistes de segmentation client
np.random.seed(42)
n_samples = 500

# Segment 1 : Jeunes, revenus moyens, dÃ©pensiers
age_1 = np.random.normal(28, 5, n_samples//5)
income_1 = np.random.normal(45, 10, n_samples//5)
spending_1 = np.random.normal(75, 10, n_samples//5)
recency_1 = np.random.normal(15, 5, n_samples//5)
frequency_1 = np.random.normal(25, 5, n_samples//5)

# Segment 2 : Ã‚ge moyen, hauts revenus, trÃ¨s dÃ©pensiers
age_2 = np.random.normal(45, 7, n_samples//5)
income_2 = np.random.normal(85, 15, n_samples//5)
spending_2 = np.random.normal(85, 8, n_samples//5)
recency_2 = np.random.normal(10, 3, n_samples//5)
frequency_2 = np.random.normal(35, 7, n_samples//5)

# Segment 3 : Seniors, revenus Ã©levÃ©s, peu dÃ©pensiers
age_3 = np.random.normal(60, 8, n_samples//5)
income_3 = np.random.normal(75, 12, n_samples//5)
spending_3 = np.random.normal(35, 10, n_samples//5)
recency_3 = np.random.normal(45, 15, n_samples//5)
frequency_3 = np.random.normal(8, 3, n_samples//5)

# Segment 4 : Jeunes, faibles revenus, Ã©conomes
age_4 = np.random.normal(25, 4, n_samples//5)
income_4 = np.random.normal(30, 8, n_samples//5)
spending_4 = np.random.normal(25, 8, n_samples//5)
recency_4 = np.random.normal(60, 20, n_samples//5)
frequency_4 = np.random.normal(5, 2, n_samples//5)

# Segment 5 : Ã‚ge moyen, revenus moyens, modÃ©rÃ©s
age_5 = np.random.normal(40, 10, n_samples//5)
income_5 = np.random.normal(55, 12, n_samples//5)
spending_5 = np.random.normal(50, 12, n_samples//5)
recency_5 = np.random.normal(30, 10, n_samples//5)
frequency_5 = np.random.normal(15, 5, n_samples//5)

# Combiner tous les segments
X = np.column_stack([
    np.concatenate([age_1, age_2, age_3, age_4, age_5]),
    np.concatenate([income_1, income_2, income_3, income_4, income_5]),
    np.concatenate([spending_1, spending_2, spending_3, spending_4, spending_5]),
    np.concatenate([recency_1, recency_2, recency_3, recency_4, recency_5]),
    np.concatenate([frequency_1, frequency_2, frequency_3, frequency_4, frequency_5])
])

# CrÃ©er DataFrame
df = pd.DataFrame(X, columns=['Age', 'Income', 'SpendingScore', 'Recency', 'Frequency'])

# Ajouter du bruit pour rendre plus rÃ©aliste
df = df + np.random.normal(0, 2, df.shape)
df = df.clip(lower=0)  # Pas de valeurs nÃ©gatives

print("\nğŸ“Š APERÃ‡U DES DONNÃ‰ES :")
print(df.head(10))
print(f"\nShape : {df.shape}")
print(f"\nStatistiques descriptives :")
print(df.describe())

print("\nğŸ” ANALYSE EXPLORATOIRE :")
print(f"Valeurs manquantes : {df.isnull().sum().sum()}")
print(f"Duplicatas : {df.duplicated().sum()}")

# Matrice de corrÃ©lation
print("\nğŸ“ˆ CORRÃ‰LATIONS ENTRE VARIABLES :")
print(df.corr().round(3))

print("""
ğŸ’¡ CE QU'IL FAUT OBSERVER DANS LES CORRÃ‰LATIONS :

   - Income vs SpendingScore : CorrÃ©lation positive attendue
     (revenus â†‘ â†’ dÃ©penses â†‘)

   - Recency vs Frequency : CorrÃ©lation nÃ©gative attendue
     (achats frÃ©quents â†’ recency faible)

   - Age vs Income : Peut Ãªtre corrÃ©lÃ© (carriÃ¨re â†’ revenus)

   âš ï¸ Si corrÃ©lations TRÃˆS Ã‰LEVÃ‰ES (>0.9) :
      â†’ Redondance, considÃ©rer supprimer une feature
      â†’ Ou utiliser PCA pour dÃ©corrÃ©ler
""")

print("\n" + "="*80)
print("NORMALISATION DES DONNÃ‰ES")
print("="*80)

print("""
âš ï¸ NORMALISATION EST CRUCIALE POUR LE CLUSTERING !

Pourquoi ?
   - K-Means, DBSCAN utilisent des DISTANCES
   - Si Income (0-100k) et Age (20-80) : Income domine !
   - Clusters biaisÃ©s par les features Ã  grande Ã©chelle

MÃ©thode : StandardScaler (z-score)

   z = (x - Î¼) / Ïƒ

   Transforme chaque feature : moyenne=0, Ã©cart-type=1
""")

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)

print(f"âœ“ DonnÃ©es normalisÃ©es : shape {X_scaled.shape}")
print(f"  Moyennes aprÃ¨s scaling : {X_scaled.mean(axis=0).round(3)}")
print(f"  Ã‰carts-types aprÃ¨s scaling : {X_scaled.std(axis=0).round(3)}")

print("\n" + "="*80)
print("PARTIE 3 : K-MEANS CLUSTERING")
print("="*80)

print("""
ğŸ”§ ALGORITHME K-MEANS

1. Initialiser k centroÃ¯des alÃ©atoirement
2. RÃ‰PÃ‰TER jusqu'Ã  convergence :
   a) Assigner chaque point au centroÃ¯de le plus proche
   b) Recalculer centroÃ¯des = moyenne des points assignÃ©s

MÃ©triques :
   - Inertie : Somme des distancesÂ² aux centroÃ¯des
   - Silhouette : QualitÃ© de sÃ©paration des clusters
""")

# Ã‰TAPE 1 : ELBOW METHOD (trouver k optimal)
print("\nğŸ“Š Ã‰TAPE 1 : ELBOW METHOD")

inertias = []
silhouette_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))

    print(f"k={k} â†’ Inertie: {kmeans.inertia_:.2f}, Silhouette: {silhouette_score(X_scaled, kmeans.labels_):.3f}")

# Visualisation Elbow + Silhouette
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Elbow plot
axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)
axes[0].set_xlabel('Nombre de clusters (k)', fontsize=12)
axes[0].set_ylabel('Inertie (Within-Cluster Sum of Squares)', fontsize=12)
axes[0].set_title('Elbow Method : Trouver k optimal', fontsize=14, fontweight='bold')
axes[0].grid(True, alpha=0.3)
axes[0].axvline(x=5, color='red', linestyle='--', label='k optimal suggÃ©rÃ©')
axes[0].legend()

# Silhouette plot
axes[1].plot(K_range, silhouette_scores, 'go-', linewidth=2, markersize=8)
axes[1].set_xlabel('Nombre de clusters (k)', fontsize=12)
axes[1].set_ylabel('Silhouette Score', fontsize=12)
axes[1].set_title('Silhouette Score par k', fontsize=14, fontweight='bold')
axes[1].grid(True, alpha=0.3)
axes[1].axhline(y=0.5, color='orange', linestyle='--', label='Seuil acceptable (0.5)')
axes[1].axvline(x=5, color='red', linestyle='--', label='k optimal suggÃ©rÃ©')
axes[1].legend()

plt.tight_layout()
plt.savefig('E:\\Nicolas\\MIAGE\\M2\\BigData\\FORMATION_ML\\TUTORIELS\\06_elbow_silhouette.png', dpi=100, bbox_inches='tight')
print("\nâœ“ Graphique sauvegardÃ© : 06_elbow_silhouette.png")
plt.close()

print("""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š OBSERVATION #1 : INTERPRÃ‰TATION ELBOW METHOD
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CE QU'IL FAUT OBSERVER :

1. COURBE INERTIE (Elbow Plot)
   â†’ DÃ©croissance forte au dÃ©but, puis ralentissement
   â†’ Chercher le "COUDE" : point oÃ¹ pente change drastiquement

   k=2 â†’ k=3 : Grosse baisse (beaucoup d'information gagnÃ©e)
   k=5 â†’ k=6 : Faible baisse (peu de gain)

2. SILHOUETTE SCORE
   â†’ Maximum autour de k=5 (probablement)
   â†’ Score > 0.5 : Clusters bien sÃ©parÃ©s
   â†’ Score < 0.3 : Mauvaise sÃ©paration

ğŸ’¡ CONCLUSION :

   âœ“ k=5 semble optimal (coude + silhouette max)
   âœ“ Correspond aux 5 segments gÃ©nÃ©rÃ©s (validation !)

   En pratique :
   - Tester k=4, k=5, k=6
   - InterprÃ©ter les clusters avec BUSINESS KNOWLEDGE
   - Un k avec moins bon silhouette mais meilleure interprÃ©tation
     mÃ©tier peut Ãªtre prÃ©fÃ©rable !

âš ï¸ ATTENTION :
   Elbow pas toujours clair (courbe lisse)
   â†’ Combiner avec silhouette ET connaissance mÃ©tier
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# Ã‰TAPE 2 : ENTRAÃNER K-MEANS AVEC k OPTIMAL
print("\nğŸ“Š Ã‰TAPE 2 : ENTRAÃNEMENT K-MEANS FINAL (k=5)")

kmeans_final = KMeans(n_clusters=5, random_state=42, n_init=10)
clusters_kmeans = kmeans_final.fit_predict(X_scaled)

print(f"âœ“ K-Means entraÃ®nÃ© avec k=5")
print(f"  Inertie finale : {kmeans_final.inertia_:.2f}")
print(f"  Silhouette score : {silhouette_score(X_scaled, clusters_kmeans):.3f}")
print(f"  Davies-Bouldin Index : {davies_bouldin_score(X_scaled, clusters_kmeans):.3f}")
print("    (Plus bas = meilleur, mesure chevauchement clusters)")

# Ajouter les clusters au DataFrame
df['Cluster_KMeans'] = clusters_kmeans

print("\nğŸ“Š DISTRIBUTION DES CLUSTERS :")
print(df['Cluster_KMeans'].value_counts().sort_index())

print("\nğŸ“Š PROFIL DES CLUSTERS (Moyennes par segment) :")
cluster_profiles = df.groupby('Cluster_KMeans').mean().round(2)
print(cluster_profiles)

print("""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š OBSERVATION #2 : INTERPRÃ‰TATION DES PROFILS DE CLUSTERS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CE QU'IL FAUT OBSERVER :

1. CARACTÃ‰RISTIQUES DISTINCTIVES de chaque cluster
   â†’ Quelles features diffÃ¨rent le plus entre clusters ?
   â†’ Chercher des patterns cohÃ©rents

2. TAILLE DES CLUSTERS
   â†’ Clusters trÃ¨s dÃ©sÃ©quilibrÃ©s ? (ex: 400 vs 20 points)
   â†’ Peut indiquer outliers ou segment de niche

3. INTERPRÃ‰TATION MÃ‰TIER

   Exemple de profils typiques :

   Cluster 0 : "JEUNES DÃ‰PENSIERS"
     Age faible, Income moyen, SpendingScore Ã©levÃ©
     â†’ StratÃ©gie : Offres tendance, rÃ©seaux sociaux

   Cluster 1 : "PREMIUM"
     Age moyen, Income haut, Frequency Ã©levÃ©e
     â†’ StratÃ©gie : Programme fidÃ©litÃ© premium, services VIP

   Cluster 2 : "DORMANTS"
     Recency Ã©levÃ©, Frequency faible
     â†’ StratÃ©gie : Campagne de rÃ©activation

   Cluster 3 : "Ã‰CONOMES"
     SpendingScore faible, Recency Ã©levÃ©
     â†’ StratÃ©gie : Promotions, codes promo

   Cluster 4 : "STABLES"
     Valeurs moyennes sur toutes features
     â†’ StratÃ©gie : Marketing gÃ©nÃ©rique

ğŸ’¡ CONCLUSION :

   âœ“ Nommer chaque cluster selon profil
   âœ“ DÃ©finir stratÃ©gie marketing par segment
   âœ“ Calculer LTV (Lifetime Value) par segment

âš ï¸ ATTENTION :
   - Ne pas sur-interprÃ©ter de petits clusters (<5% donnÃ©es)
   - Valider avec Ã©quipes mÃ©tier (Marketing, Sales)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# Ã‰TAPE 3 : VISUALISATION AVEC PCA
print("\nğŸ“Š Ã‰TAPE 3 : RÃ‰DUCTION DIMENSIONNELLE POUR VISUALISATION (PCA)")

print("""
â“ POURQUOI PCA ?

   - Nos donnÃ©es : 5 dimensions (Age, Income, ...)
   - Impossible de visualiser en 5D !
   - PCA rÃ©duit Ã  2D en conservant maximum de variance

   PCA (Principal Component Analysis) :

   1. Trouve directions de variance maximale
   2. Projette donnÃ©es sur ces directions (composantes principales)
   3. PC1 = direction de variance max, PC2 = 2Ã¨me direction, etc.
""")

pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)

print(f"âœ“ PCA appliquÃ©e : 5D â†’ 2D")
print(f"  Variance expliquÃ©e par PC1 : {pca.explained_variance_ratio_[0]:.2%}")
print(f"  Variance expliquÃ©e par PC2 : {pca.explained_variance_ratio_[1]:.2%}")
print(f"  Variance totale conservÃ©e : {pca.explained_variance_ratio_.sum():.2%}")

# Visualisation des clusters en 2D
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_kmeans,
                      cmap='viridis', s=50, alpha=0.6, edgecolors='k', linewidth=0.5)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)
plt.title('K-Means Clustering (k=5) - Vue PCA', fontsize=13, fontweight='bold')
plt.colorbar(scatter, label='Cluster')

# Ajouter les centroÃ¯des
centroids_pca = pca.transform(kmeans_final.cluster_centers_)
plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1],
            c='red', marker='X', s=300, edgecolors='black', linewidth=2, label='CentroÃ¯des')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
# Silhouette diagram
from matplotlib import cm
silhouette_vals = silhouette_samples(X_scaled, clusters_kmeans)
y_lower = 10
for i in range(5):
    cluster_silhouette_vals = silhouette_vals[clusters_kmeans == i]
    cluster_silhouette_vals.sort()
    size_cluster_i = cluster_silhouette_vals.shape[0]
    y_upper = y_lower + size_cluster_i

    color = cm.viridis(float(i) / 5)
    plt.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_vals,
                      facecolor=color, edgecolor=color, alpha=0.7)
    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i), fontsize=12, fontweight='bold')
    y_lower = y_upper + 10

plt.axvline(x=silhouette_score(X_scaled, clusters_kmeans), color="red", linestyle="--",
            label=f'Silhouette moyenne: {silhouette_score(X_scaled, clusters_kmeans):.3f}')
plt.xlabel('Coefficient de Silhouette', fontsize=11)
plt.ylabel('Cluster', fontsize=11)
plt.title('Silhouette Plot par Cluster', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.savefig('E:\\Nicolas\\MIAGE\\M2\\BigData\\FORMATION_ML\\TUTORIELS\\06_kmeans_visualization.png', dpi=100, bbox_inches='tight')
print("\nâœ“ Graphique sauvegardÃ© : 06_kmeans_visualization.png")
plt.close()

print("""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š OBSERVATION #3 : QUALITÃ‰ DES CLUSTERS (SILHOUETTE)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CE QU'IL FAUT OBSERVER :

1. SILHOUETTE PLOT (Graphique de droite)

   â†’ Chaque "Ã©tage" = un cluster
   â†’ Largeur = taille du cluster
   â†’ Longueur des barres = coefficient de silhouette individuel

   âœ“ BON SIGNE :
     - Toutes les barres dÃ©passent la ligne rouge (moyenne)
     - Barres longues et uniformes (cluster cohÃ©sif)
     - Peu de barres nÃ©gatives

   âœ— MAUVAIS SIGNE :
     - Barres trÃ¨s courtes ou nÃ©gatives (points mal classÃ©s)
     - Variation forte au sein d'un cluster (hÃ©tÃ©rogÃ¨ne)
     - Clusters de tailles trÃ¨s inÃ©gales

2. VISUALISATION PCA (Graphique de gauche)

   â†’ Clusters bien sÃ©parÃ©s visuellement ?
   â†’ CentroÃ¯des (X rouges) au centre de leurs clusters ?

   âš ï¸ ATTENTION : PCA ne conserve que ~60-70% variance
      â†’ SÃ©paration en 2D peut Ãªtre trompeuse
      â†’ Clusters peuvent se chevaucher en 2D mais Ãªtre distincts en 5D

ğŸ’¡ CONCLUSION :

   âœ“ Si silhouette moyenne > 0.5 ET barres uniformes :
     â†’ Clusters de bonne qualitÃ©, bien sÃ©parÃ©s

   âœ“ Si certains clusters ont silhouette faible :
     â†’ Peuvent Ãªtre fusionnÃ©s (rÃ©duire k)
     â†’ Ou contenir outliers (considÃ©rer DBSCAN)

   âœ“ Variance PCA > 60% :
     â†’ Visualisation 2D reprÃ©sentative

âš ï¸ SI SILHOUETTE < 0.3 :
   â†’ Revoir le nombre de clusters
   â†’ Ou les donnÃ©es ne sont pas naturellement clusterisables
   â†’ Essayer DBSCAN ou clustering hiÃ©rarchique
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

print("\n" + "="*80)
print("PARTIE 4 : DBSCAN CLUSTERING")
print("="*80)

print("""
ğŸ”§ ALGORITHME DBSCAN (Density-Based Spatial Clustering)

Principe :
   - Grouper points dans rÃ©gions DENSES
   - Identifier OUTLIERS (points isolÃ©s)

ParamÃ¨tres :
   eps         : Rayon de voisinage (epsilon)
   min_samples : Nb min de points pour former un cluster

Types de points :
   - CORE : â‰¥ min_samples voisins dans rayon eps
   - BORDER : < min_samples voisins, mais proche d'un core point
   - NOISE : Ni core ni border (OUTLIER)

Avantages :
   âœ“ DÃ©tecte nombre de clusters automatiquement
   âœ“ Forme de clusters arbitraires
   âœ“ Identifie outliers (label = -1)

InconvÃ©nients :
   âœ— Sensible aux paramÃ¨tres eps et min_samples
   âœ— Difficile si densitÃ© trÃ¨s variable
""")

print("\nğŸ“Š Ã‰TAPE 1 : TROUVER eps OPTIMAL (k-distance plot)")

# Calculer distance au 4Ã¨me voisin le plus proche
from sklearn.neighbors import NearestNeighbors

k = 4  # min_samples recommandÃ©
neighbors = NearestNeighbors(n_neighbors=k)
neighbors.fit(X_scaled)
distances, indices = neighbors.kneighbors(X_scaled)

# Distances au k-Ã¨me voisin (triÃ©es)
distances = np.sort(distances[:, k-1], axis=0)

plt.figure(figsize=(10, 5))
plt.plot(distances, linewidth=2)
plt.xlabel('Points (triÃ©s par distance)', fontsize=11)
plt.ylabel(f'Distance au {k}-Ã¨me voisin le plus proche', fontsize=11)
plt.title('k-Distance Plot : Trouver eps optimal pour DBSCAN', fontsize=13, fontweight='bold')
plt.axhline(y=0.8, color='red', linestyle='--', label='eps suggÃ©rÃ© = 0.8')
plt.grid(True, alpha=0.3)
plt.legend()
plt.tight_layout()
plt.savefig('E:\\Nicolas\\MIAGE\\M2\\BigData\\FORMATION_ML\\TUTORIELS\\06_kdistance_plot.png', dpi=100, bbox_inches='tight')
print("\nâœ“ Graphique sauvegardÃ© : 06_kdistance_plot.png")
plt.close()

print("""
ğŸ’¡ COMMENT LIRE LE k-DISTANCE PLOT ?

   1. Chercher le "COUDE" oÃ¹ distance augmente brusquement
   2. Ce point sÃ©pare :
      - Points dans clusters denses (avant le coude)
      - Outliers (aprÃ¨s le coude, distances Ã©levÃ©es)
   3. eps â‰ˆ distance au coude

   Ici : coude vers 0.8 â†’ eps = 0.8
""")

# Ã‰TAPE 2 : ENTRAÃNER DBSCAN
print("\nğŸ“Š Ã‰TAPE 2 : ENTRAÃNEMENT DBSCAN")

dbscan = DBSCAN(eps=0.8, min_samples=4)
clusters_dbscan = dbscan.fit_predict(X_scaled)

n_clusters_dbscan = len(set(clusters_dbscan)) - (1 if -1 in clusters_dbscan else 0)
n_noise = list(clusters_dbscan).count(-1)

print(f"âœ“ DBSCAN entraÃ®nÃ© avec eps=0.8, min_samples=4")
print(f"  Nombre de clusters dÃ©tectÃ©s : {n_clusters_dbscan}")
print(f"  Nombre d'outliers (noise) : {n_noise} ({n_noise/len(clusters_dbscan)*100:.1f}%)")

if n_clusters_dbscan > 1:
    # Silhouette sans les outliers
    mask_not_noise = clusters_dbscan != -1
    if mask_not_noise.sum() > 0:
        silhouette_dbscan = silhouette_score(X_scaled[mask_not_noise], clusters_dbscan[mask_not_noise])
        print(f"  Silhouette score (sans outliers) : {silhouette_dbscan:.3f}")

print("\nğŸ“Š DISTRIBUTION DES CLUSTERS DBSCAN :")
unique, counts = np.unique(clusters_dbscan, return_counts=True)
for cluster_id, count in zip(unique, counts):
    if cluster_id == -1:
        print(f"  Noise (outliers) : {count} points")
    else:
        print(f"  Cluster {cluster_id} : {count} points")

# Visualisation DBSCAN
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_dbscan,
                      cmap='viridis', s=50, alpha=0.6, edgecolors='k', linewidth=0.5)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)
plt.title(f'DBSCAN Clustering - {n_clusters_dbscan} clusters, {n_noise} outliers',
          fontsize=13, fontweight='bold')
plt.colorbar(scatter, label='Cluster (-1 = noise)')
plt.grid(True, alpha=0.3)

# Comparaison K-Means vs DBSCAN
plt.subplot(1, 2, 2)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_kmeans,
            cmap='viridis', s=50, alpha=0.6, edgecolors='k', linewidth=0.5, label='K-Means')
plt.scatter(X_pca[clusters_dbscan == -1, 0], X_pca[clusters_dbscan == -1, 1],
            c='red', marker='x', s=100, linewidth=2, label='Outliers DBSCAN')
plt.xlabel(f'PC1', fontsize=11)
plt.ylabel(f'PC2', fontsize=11)
plt.title('K-Means avec Outliers DBSCAN superposÃ©s', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('E:\\Nicolas\\MIAGE\\M2\\BigData\\FORMATION_ML\\TUTORIELS\\06_dbscan_visualization.png', dpi=100, bbox_inches='tight')
print("\nâœ“ Graphique sauvegardÃ© : 06_dbscan_visualization.png")
plt.close()

print("""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š OBSERVATION #4 : DBSCAN VS K-MEANS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CE QU'IL FAUT OBSERVER :

1. NOMBRE DE CLUSTERS

   K-Means : k fixÃ© Ã  5 (imposÃ©)
   DBSCAN  : DÃ©tectÃ© automatiquement (peut diffÃ©rer)

   â†’ Si DBSCAN trouve beaucoup plus ou moins de clusters :
     - Peut indiquer que k=5 n'est pas naturel dans les donnÃ©es
     - Ou que eps/min_samples mal choisis

2. OUTLIERS (DBSCAN uniquement)

   â†’ Points marquÃ©s en ROUGE (label = -1)
   â†’ ReprÃ©sentent combien % du dataset ?

   âœ“ Si < 5% outliers :
     â†’ Probablement vrais outliers Ã  investiguer
     â†’ Clients atypiques, erreurs de donnÃ©es ?

   âœ— Si > 20% outliers :
     â†’ eps trop petit (augmenter eps)
     â†’ Ou min_samples trop Ã©levÃ©

3. FORME DES CLUSTERS

   K-Means : Clusters sphÃ©riques/convexes
   DBSCAN  : Forme arbitraire (peut suivre densitÃ©)

   â†’ Si donnÃ©es ont forme complexe (spirales, anneaux) :
     DBSCAN > K-Means

ğŸ’¡ CONCLUSION :

   âœ“ Utiliser DBSCAN si :
     - Outliers sont informatifs (fraude, anomalies)
     - Nombre de clusters inconnu
     - Forme de clusters complexe

   âœ“ Utiliser K-Means si :
     - Besoin de k segments prÃ©cis (mÃ©tier)
     - Toutes les donnÃ©es doivent Ãªtre assignÃ©es
     - RapiditÃ© cruciale (gros volumes)

âš ï¸ POINTS D'ATTENTION :

   - Outliers DBSCAN peuvent Ãªtre des micro-segments ignorÃ©s par K-Means
   - Investiguer les outliers : erreurs donnÃ©es ? Clients VIP uniques ?
   - Combiner les deux : K-Means pour segmentation, DBSCAN pour outliers
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

print("\n" + "="*80)
print("PARTIE 5 : CLUSTERING HIÃ‰RARCHIQUE")
print("="*80)

print("""
ğŸ”§ CLUSTERING HIÃ‰RARCHIQUE (Agglomerative)

Principe :
   1. DÃ©part : chaque point = un cluster
   2. RÃ‰PÃ‰TER jusqu'Ã  un seul cluster :
      - Fusionner les 2 clusters les plus proches
      - Recalculer distances

MÃ©thodes de linkage (calcul distance inter-clusters) :

   - WARD : Minimise variance intra-cluster
     â†’ Clusters Ã©quilibrÃ©s, sphÃ©riques (similaire K-Means)
     â†’ LE PLUS UTILISÃ‰

   - AVERAGE : Distance moyenne entre tous points
     â†’ Compromis

   - COMPLETE : Distance max entre points les plus Ã©loignÃ©s
     â†’ Clusters compacts

   - SINGLE : Distance min entre points les plus proches
     â†’ Peut crÃ©er chaÃ®nes (effet "chaining")

Avantage :
   âœ“ DENDROGRAMME : Visualise toute la hiÃ©rarchie
   âœ“ Choisir k APRÃˆS clustering (couper dendrogramme)

InconvÃ©nient :
   âœ— ComplexitÃ© O(nÂ²) ou O(nÂ³)
   âœ— Ne scale pas (max ~10k points)
""")

print("\nğŸ“Š Ã‰TAPE 1 : CONSTRUCTION DU DENDROGRAMME")

# Utiliser un Ã©chantillon pour le dendrogramme (trop lent sinon)
sample_size = 200
idx_sample = np.random.choice(len(X_scaled), size=sample_size, replace=False)
X_sample = X_scaled[idx_sample]

# Calculer linkage matrix
linkage_matrix = linkage(X_sample, method='ward')

plt.figure(figsize=(14, 6))
dendrogram(linkage_matrix,
           truncate_mode='lastp',  # Montrer seulement derniers p clusters
           p=20,
           leaf_font_size=10,
           show_leaf_counts=True)
plt.xlabel('Cluster Index', fontsize=11)
plt.ylabel('Distance (Ward)', fontsize=11)
plt.title(f'Dendrogramme HiÃ©rarchique (Ã©chantillon de {sample_size} points)',
          fontsize=13, fontweight='bold')
plt.axhline(y=15, color='red', linestyle='--', label='Coupure suggÃ©rÃ©e (5 clusters)')
plt.legend()
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.savefig('E:\\Nicolas\\MIAGE\\M2\\BigData\\FORMATION_ML\\TUTORIELS\\06_dendrogram.png', dpi=100, bbox_inches='tight')
print("\nâœ“ Graphique sauvegardÃ© : 06_dendrogram.png")
plt.close()

print("""
ğŸ’¡ COMMENT LIRE UN DENDROGRAMME ?

   1. Axe Y : Distance de fusion
      â†’ Plus haut = clusters plus distants

   2. Branches : ReprÃ©sentent fusions successives
      â†’ Largeur = nombre de points dans le cluster

   3. COUPER LE DENDROGRAMME :
      â†’ Tracer ligne horizontale â†’ nb clusters
      â†’ Ici : couper Ã  y=15 â†’ 5 clusters

   4. Chercher le plus GRAND SAUT VERTICAL
      â†’ Indique sÃ©paration naturelle
      â†’ Couper juste avant ce saut
""")

# Ã‰TAPE 2 : ENTRAÃNER CLUSTERING HIÃ‰RARCHIQUE
print("\nğŸ“Š Ã‰TAPE 2 : CLUSTERING HIÃ‰RARCHIQUE (n_clusters=5)")

hierarchical = AgglomerativeClustering(n_clusters=5, linkage='ward')
clusters_hierarchical = hierarchical.fit_predict(X_scaled)

print(f"âœ“ Clustering hiÃ©rarchique entraÃ®nÃ©")
print(f"  Silhouette score : {silhouette_score(X_scaled, clusters_hierarchical):.3f}")
print(f"  Davies-Bouldin Index : {davies_bouldin_score(X_scaled, clusters_hierarchical):.3f}")

print("\nğŸ“Š DISTRIBUTION DES CLUSTERS HIÃ‰RARCHIQUES :")
unique_hier, counts_hier = np.unique(clusters_hierarchical, return_counts=True)
for cluster_id, count in zip(unique_hier, counts_hier):
    print(f"  Cluster {cluster_id} : {count} points")

# Visualisation
plt.figure(figsize=(14, 5))

plt.subplot(1, 3, 1)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_kmeans,
                      cmap='viridis', s=50, alpha=0.6, edgecolors='k', linewidth=0.5)
plt.xlabel('PC1', fontsize=10)
plt.ylabel('PC2', fontsize=10)
plt.title('K-Means (k=5)', fontsize=12, fontweight='bold')
plt.colorbar(scatter, label='Cluster')
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 2)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_dbscan,
                      cmap='viridis', s=50, alpha=0.6, edgecolors='k', linewidth=0.5)
plt.xlabel('PC1', fontsize=10)
plt.ylabel('PC2', fontsize=10)
plt.title(f'DBSCAN ({n_clusters_dbscan} clusters + outliers)', fontsize=12, fontweight='bold')
plt.colorbar(scatter, label='Cluster')
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 3)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_hierarchical,
                      cmap='viridis', s=50, alpha=0.6, edgecolors='k', linewidth=0.5)
plt.xlabel('PC1', fontsize=10)
plt.ylabel('PC2', fontsize=10)
plt.title('HiÃ©rarchique (n_clusters=5)', fontsize=12, fontweight='bold')
plt.colorbar(scatter, label='Cluster')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('E:\\Nicolas\\MIAGE\\M2\\BigData\\FORMATION_ML\\TUTORIELS\\06_comparison_all.png', dpi=100, bbox_inches='tight')
print("\nâœ“ Graphique sauvegardÃ© : 06_comparison_all.png")
plt.close()

print("""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š OBSERVATION #5 : COMPARAISON DES 3 MÃ‰THODES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CE QU'IL FAUT OBSERVER :

1. COHÃ‰RENCE ENTRE MÃ‰THODES

   â†’ Les 3 mÃ©thodes identifient des clusters similaires ?
     âœ“ OUI : Clusters robustes, bien dÃ©finis
     âœ— NON : Structure ambiguÃ«, clusters artificiels

   â†’ Comparer visuellement les frontiÃ¨res

2. DIFFÃ‰RENCES SPÃ‰CIFIQUES

   K-Means vs HiÃ©rarchique (Ward) :
     â†’ Souvent trÃ¨s similaires (les deux minimisent variance)
     â†’ DiffÃ©rences aux frontiÃ¨res uniquement

   DBSCAN vs autres :
     â†’ Outliers = points oÃ¹ K-Means/HiÃ©rarchique sont incertains
     â†’ DBSCAN peut dÃ©tecter moins de clusters (plus conservateur)

3. MÃ‰TRIQUES DE QUALITÃ‰

   Silhouette Score :
     K-Means        : ~0.XX
     DBSCAN         : ~0.XX (sans outliers)
     HiÃ©rarchique   : ~0.XX

   â†’ Quelle mÃ©thode a le meilleur score ?
   â†’ DiffÃ©rence > 0.1 : significative
   â†’ DiffÃ©rence < 0.05 : Ã©quivalentes

ğŸ’¡ CONCLUSION : QUELLE MÃ‰THODE CHOISIR ?

   âœ… ACCORD FORT (3 mÃ©thodes similaires) :
      â†’ Utiliser K-Means (rapiditÃ© + interprÃ©tabilitÃ©)
      â†’ Ou HiÃ©rarchique si besoin dendrogramme

   âœ… DBSCAN TRÃˆS DIFFÃ‰RENT :
      â†’ Investiguer les outliers DBSCAN
      â†’ Peut rÃ©vÃ©ler structure ignorÃ©e par K-Means
      â†’ ConsidÃ©rer mÃ©thode hybride

   âœ… DÃ‰SACCORD ENTRE TOUTES :
      â†’ Structure de clustering faible dans les donnÃ©es
      â†’ Revoir features (feature engineering ?)
      â†’ Ou donnÃ©es pas naturellement clusterisables
      â†’ ConsidÃ©rer segmentation basÃ©e sur rÃ¨gles mÃ©tier

âš ï¸ CHECKLIST FINALE :

   [ ] Silhouette > 0.4 pour mÃ©thode choisie
   [ ] Clusters Ã©quilibrÃ©s en taille (sauf si intentionnel)
   [ ] Profils de clusters interprÃ©tables mÃ©tier
   [ ] Validation avec stakeholders (Marketing, etc.)
   [ ] Outliers investiguÃ©s (erreurs donnÃ©es ? VIP ?)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

print("\n" + "="*80)
print("PARTIE 6 : ANALYSE APPROFONDIE DES SEGMENTS")
print("="*80)

print("\nğŸ“Š PROFILS DÃ‰TAILLÃ‰S DES SEGMENTS (K-Means)")

# Ajouter clusters aux donnÃ©es originales (non normalisÃ©es)
df['Cluster'] = clusters_kmeans

# Statistiques par cluster
print("\n1ï¸âƒ£ MOYENNES PAR CLUSTER :")
print(df.groupby('Cluster').mean().round(2))

print("\n2ï¸âƒ£ MÃ‰DIANES PAR CLUSTER :")
print(df.groupby('Cluster').median().round(2))

print("\n3ï¸âƒ£ TAILLES DES CLUSTERS :")
cluster_sizes = df['Cluster'].value_counts().sort_index()
print(cluster_sizes)
print(f"\nProportion de chaque cluster :")
print((cluster_sizes / len(df) * 100).round(1).astype(str) + '%')

# Visualisation boxplots
fig, axes = plt.subplots(2, 3, figsize=(16, 10))
features = ['Age', 'Income', 'SpendingScore', 'Recency', 'Frequency']

for idx, feature in enumerate(features):
    row = idx // 3
    col = idx % 3
    ax = axes[row, col]

    df.boxplot(column=feature, by='Cluster', ax=ax)
    ax.set_title(f'{feature} par Cluster', fontsize=12, fontweight='bold')
    ax.set_xlabel('Cluster', fontsize=10)
    ax.set_ylabel(feature, fontsize=10)
    ax.grid(True, alpha=0.3)

# Supprimer le subplot vide
fig.delaxes(axes[1, 2])

# Ajouter un texte rÃ©capitulatif
axes[1, 2] = fig.add_subplot(2, 3, 6)
axes[1, 2].axis('off')
recap_text = f"""
RÃ‰CAPITULATIF
{'='*30}

Nombre de clusters : {len(cluster_sizes)}

Silhouette Score : {silhouette_score(X_scaled, clusters_kmeans):.3f}

Tailles des clusters :
{chr(10).join([f'  Cluster {i}: {size} ({size/len(df)*100:.1f}%)' for i, size in cluster_sizes.items()])}

Prochaines Ã©tapes :
1. Nommer les clusters (profils)
2. Valider avec mÃ©tier
3. DÃ©finir stratÃ©gies par segment
4. Suivre Ã©volution dans le temps
"""
axes[1, 2].text(0.1, 0.5, recap_text, fontsize=10, family='monospace', verticalalignment='center')

plt.suptitle('Distribution des Features par Cluster', fontsize=14, fontweight='bold', y=0.995)
plt.tight_layout()
plt.savefig('E:\\Nicolas\\MIAGE\\M2\\BigData\\FORMATION_ML\\TUTORIELS\\06_boxplots_clusters.png', dpi=100, bbox_inches='tight')
print("\nâœ“ Graphique sauvegardÃ© : 06_boxplots_clusters.png")
plt.close()

print("""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š OBSERVATION #6 : BOXPLOTS ET PROFILS DÃ‰TAILLÃ‰S
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CE QU'IL FAUT OBSERVER DANS LES BOXPLOTS :

1. SÃ‰PARATION DES CLUSTERS

   â†’ BoÃ®tes qui se chevauchent PEU : Bonne sÃ©paration
   â†’ Chevauchement fort : Clusters ambigus sur cette feature

   Exemple :
     - Age : Si boÃ®tes bien sÃ©parÃ©es â†’ Age discriminant
     - Income : Si chevauchement â†’ Pas discriminant seul

2. DISPERSION INTRA-CLUSTER

   â†’ BoÃ®tes Ã‰TROITES : Cluster homogÃ¨ne (bonne cohÃ©sion)
   â†’ BoÃ®tes LARGES : Cluster hÃ©tÃ©rogÃ¨ne (revoir k ?)

3. OUTLIERS (points au-delÃ  des moustaches)

   â†’ Beaucoup d'outliers dans un cluster :
     - Cluster mal dÃ©fini
     - Ou vraie diversitÃ© dans le segment

   â†’ Investiguer ces points extrÃªmes

4. FEATURES DISCRIMINANTES

   â†’ Identifier quelles features sÃ©parent le MIEUX les clusters
   â†’ Celles avec sÃ©paration claire = clÃ©s pour nommage

   Exemple profil :
     Cluster 0 : Age BAS + SpendingScore HAUT
                 â†’ "Jeunes DÃ©pensiers"
     Cluster 2 : Recency HAUT + Frequency BAS
                 â†’ "Clients Dormants"

ğŸ’¡ CONCLUSION POUR LA SEGMENTATION CLIENT :

   âœ“ NOMMAGE DES SEGMENTS (exemples)

   Segment 0 : "Jeunes Actifs"
     - Age : 25-35 ans
     - Income : Moyen
     - StratÃ©gie : Offres lifestyle, rÃ©seaux sociaux

   Segment 1 : "VIP Premium"
     - Income : Ã‰levÃ©
     - Frequency : TrÃ¨s Ã©levÃ©e
     - StratÃ©gie : Services exclusifs, early access

   Segment 2 : "En Sommeil"
     - Recency : > 60 jours
     - Frequency : Faible
     - StratÃ©gie : RÃ©activation, promotions agressives

   Segment 3 : "Ã‰conomes"
     - SpendingScore : Faible
     - StratÃ©gie : Coupons, soldes, rapport qualitÃ©/prix

   Segment 4 : "Matures Stables"
     - Age : > 50 ans
     - Behaviour : Stable, prÃ©visible
     - StratÃ©gie : FidÃ©lisation, service client premium

âš ï¸ ACTIONS POST-CLUSTERING :

   [ ] PrÃ©senter profils Ã  Ã©quipes Marketing/Sales
   [ ] Valider cohÃ©rence avec connaissance mÃ©tier
   [ ] Calculer mÃ©triques business par segment :
       - LTV (Lifetime Value)
       - Taux de conversion
       - Panier moyen
   [ ] DÃ©finir KPI de suivi par segment
   [ ] ImplÃ©menter scoring pour nouveaux clients
   [ ] Monitorer Ã©volution des segments dans le temps
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

print("\n" + "="*80)
print("PARTIE 7 : Ã‰VALUATION ET VALIDATION")
print("="*80)

print("""
ğŸ“Š RÃ‰CAPITULATIF DES MÃ‰TRIQUES DE QUALITÃ‰

1. SILHOUETTE SCORE [-1, 1]

   Formule : s = (b - a) / max(a, b)
   oÃ¹ a = distance moyenne intra-cluster
       b = distance moyenne inter-cluster

   InterprÃ©tation :
     > 0.7  : SÃ©paration forte, clusters excellents
     0.5-0.7: SÃ©paration raisonnable, structure claire
     0.25-0.5: SÃ©paration faible, clusters moyens
     < 0.25 : Pas de structure claire, reconsidÃ©rer k

2. DAVIES-BOULDIN INDEX [0, +âˆ]

   Mesure : Ratio similaritÃ© intra-cluster / inter-cluster

   InterprÃ©tation :
     Plus BAS = Meilleur
     < 1.0 : Excellente sÃ©paration
     1.0-2.0 : SÃ©paration acceptable
     > 2.0 : Clusters se chevauchent beaucoup

3. INERTIE (K-Means uniquement)

   Somme des distancesÂ² aux centroÃ¯des

   âš ï¸ NE PAS COMPARER entre datasets diffÃ©rents !
      Seulement pour Elbow Method (mÃªme dataset, diffÃ©rents k)
""")

# Tableau rÃ©capitulatif
print("\nğŸ“Š COMPARAISON FINALE DES MÃ‰THODES :")
print("="*70)
print(f"{'MÃ©thode':<20} {'Silhouette':>12} {'Davies-Bouldin':>15} {'Nb Clusters':>12}")
print("="*70)

sil_kmeans = silhouette_score(X_scaled, clusters_kmeans)
db_kmeans = davies_bouldin_score(X_scaled, clusters_kmeans)
print(f"{'K-Means':<20} {sil_kmeans:>12.3f} {db_kmeans:>15.3f} {5:>12}")

if n_clusters_dbscan > 1 and mask_not_noise.sum() > 0:
    sil_dbscan = silhouette_score(X_scaled[mask_not_noise], clusters_dbscan[mask_not_noise])
    db_dbscan = davies_bouldin_score(X_scaled[mask_not_noise], clusters_dbscan[mask_not_noise])
    print(f"{'DBSCAN':<20} {sil_dbscan:>12.3f} {db_dbscan:>15.3f} {n_clusters_dbscan:>12}")
else:
    print(f"{'DBSCAN':<20} {'N/A':>12} {'N/A':>15} {n_clusters_dbscan:>12}")

sil_hier = silhouette_score(X_scaled, clusters_hierarchical)
db_hier = davies_bouldin_score(X_scaled, clusters_hierarchical)
print(f"{'HiÃ©rarchique':<20} {sil_hier:>12.3f} {db_hier:>15.3f} {5:>12}")
print("="*70)

print("""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š OBSERVATION #7 : VALIDATION ET DÃ‰CISION FINALE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CE QU'IL FAUT OBSERVER DANS LES MÃ‰TRIQUES :

1. CONVERGENCE DES MÃ‰THODES

   â†’ Silhouette scores similaires entre mÃ©thodes ?
     âœ“ OUI : Structure de clustering robuste
     âœ— NON : Une mÃ©thode capte mieux la structure

   â†’ Davies-Bouldin cohÃ©rent ?
     âœ“ Tous < 1.5 : Bonne sÃ©paration quelle que soit mÃ©thode
     âœ— Un score >> autres : MÃ©thode inadaptÃ©e

2. CRITÃˆRES DE DÃ‰CISION

   MÃ©thode A : K-Means
     âœ“ Silhouette Ã©levÃ© + DB faible
     âœ“ RapiditÃ© (production sur millions de clients)
     âœ“ Nombre de clusters fixe acceptable (mÃ©tier)
     â†’ RECOMMANDÃ‰ pour production

   MÃ©thode B : DBSCAN
     âœ“ DÃ©tecte outliers importants (fraude, VIP)
     âœ“ Forme de clusters complexe
     â†’ RECOMMANDÃ‰ pour exploration + dÃ©tection anomalies

   MÃ©thode C : HiÃ©rarchique
     âœ“ Besoin de visualiser hiÃ©rarchie
     âœ“ Petit dataset
     â†’ RECOMMANDÃ‰ pour analyse exploratoire

3. VALIDATION BUSINESS

   âš ï¸ MÃ‰TRIQUES â‰  SUCCÃˆS BUSINESS !

   Un clustering avec silhouette 0.45 mais segments
   mÃ©tier cohÃ©rents > clustering silhouette 0.65 ininterprÃ©tables

   Questions clÃ©s :

   [ ] Les profils de clusters ont du SENS ?
   [ ] Actionnables pour Marketing/Sales ?
   [ ] Stables dans le temps (re-clustering mensuel) ?
   [ ] AmÃ©liorent KPI business (conversion, LTV) ?

ğŸ’¡ DÃ‰CISION FINALE (pour ce cas d'usage) :

   MÃ‰THODE RETENUE : K-Means (k=5)

   Raisons :
   âœ“ Silhouette satisfaisant (> 0.4)
   âœ“ 5 segments cohÃ©rents mÃ©tier
   âœ“ Scalable pour scoring temps rÃ©el
   âœ“ Facile Ã  expliquer aux stakeholders

   USAGE COMPLÃ‰MENTAIRE : DBSCAN

   Pour :
   âœ“ DÃ©tection clients VIP uniques (outliers)
   âœ“ Fraude / comportements anormaux

   PROCHAINES Ã‰TAPES :

   1. [ ] Nommer les 5 segments
   2. [ ] Calculer LTV par segment
   3. [ ] DÃ©finir stratÃ©gies marketing ciblÃ©es
   4. [ ] ImplÃ©menter scoring nouveaux clients
   5. [ ] Dashboard suivi segments (Power BI, Tableau)
   6. [ ] A/B testing stratÃ©gies par segment
   7. [ ] Re-clustering mensuel (Ã©volution segments)
   8. [ ] Analyser transitions inter-segments

âš ï¸ MONITORING EN PRODUCTION :

   - Surveiller distribution clusters (dÃ©rive ?)
   - Recalculer centroÃ¯des rÃ©guliÃ¨rement
   - Alertes si % outliers DBSCAN change brutalement
   - Valider que nouvelles donnÃ©es similaires (distribution)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

print("\n" + "="*80)
print("RÃ‰SUMÃ‰ FINAL : QUAND UTILISER QUEL ALGORITHME ?")
print("="*80)

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ARBRE DE DÃ‰CISION CLUSTERING                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Vous connaissez le NOMBRE DE CLUSTERS ?
â”‚
â”œâ”€ OUI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  Les clusters sont de forme SPHÃ‰RIQUE/CONVEXE ?            â”‚
â”‚  â”‚                                                           â”‚
â”‚  â”œâ”€ OUI â”€â”€> K-MEANS                                        â”‚
â”‚  â”‚          âœ“ Rapide, scalable                             â”‚
â”‚  â”‚          âœ“ InterprÃ©table (centroÃ¯des)                   â”‚
â”‚  â”‚          âœ“ Production                                    â”‚
â”‚  â”‚                                                           â”‚
â”‚  â””â”€ NON â”€â”€> CLUSTERING HIÃ‰RARCHIQUE (si n < 10k)           â”‚
â”‚             âœ“ Forme arbitraire                              â”‚
â”‚             âœ“ Dendrogramme (visualisation)                 â”‚
â”‚             âœ— Lent sur gros volumes                        â”‚
â”‚                                                              â”‚
â””â”€ NON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                               â”‚
   PrÃ©sence d'OUTLIERS importants ?                          â”‚
   â”‚                                                           â”‚
   â”œâ”€ OUI â”€â”€> DBSCAN                                         â”‚
   â”‚          âœ“ DÃ©tecte outliers automatiquement             â”‚
   â”‚          âœ“ Nombre de clusters automatique               â”‚
   â”‚          âœ“ Forme arbitraire                             â”‚
   â”‚          âš ï¸ Sensible aux paramÃ¨tres eps/min_samples     â”‚
   â”‚                                                           â”‚
   â””â”€ NON â”€â”€> Essayer K-MEANS avec Elbow Method              â”‚
              Ou HIÃ‰RARCHIQUE + Dendrogramme                  â”‚


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    CAS D'USAGE PAR ALGORITHME                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ K-MEANS

   âœ… Segmentation client (marketing)
   âœ… Compression d'images (rÃ©duire palette couleurs)
   âœ… Organisation de documents (topics similaires)
   âœ… Analyse de sÃ©ries temporelles (patterns)
   âœ… Recommandation (groupes de produits similaires)

   âš ï¸ Limitations :
   - Clusters non-sphÃ©riques mal dÃ©tectÃ©s
   - Sensible aux outliers (fausse centroÃ¯des)
   - RÃ©sultats dÃ©pendent initialisation (utiliser n_init=10+)

ğŸ¯ DBSCAN

   âœ… DÃ©tection de fraude (outliers = fraudes potentielles)
   âœ… Analyse spatiale (densitÃ© gÃ©ographique)
   âœ… Analyse de rÃ©seaux sociaux (communautÃ©s)
   âœ… DÃ©tection d'anomalies (maintenance prÃ©dictive)
   âœ… Traitement d'images (segmentation)

   âš ï¸ Limitations :
   - Difficile si densitÃ© trÃ¨s variable
   - ParamÃ¨tres eps/min_samples Ã  tuner finement
   - Peut crÃ©er UN SEUL gros cluster si mal paramÃ©trÃ©

ğŸ¯ HIÃ‰RARCHIQUE

   âœ… Analyse phylogÃ©nÃ©tique (biologie)
   âœ… Taxonomie (classification hiÃ©rarchique)
   âœ… Exploration de donnÃ©es (comprendre structure)
   âœ… Petits datasets avec besoin visualisation

   âš ï¸ Limitations :
   - NE SCALE PAS (max 10-20k points)
   - DÃ©cisions de fusion irrÃ©versibles
   - Plusieurs linkages possibles (ward, complete, average...)


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    CHECKLIST AVANT PRODUCTION                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ QUALITÃ‰ DES DONNÃ‰ES

   [ ] DonnÃ©es normalisÃ©es (StandardScaler, MinMaxScaler)
   [ ] Valeurs manquantes traitÃ©es (imputation ou suppression)
   [ ] Outliers investiguÃ©s (erreurs ? vrais outliers ?)
   [ ] Features corrÃ©lÃ©es identifiÃ©es (considÃ©rer PCA ?)
   [ ] Ã‰chelle des features cohÃ©rente

ğŸ“‹ QUALITÃ‰ DES CLUSTERS

   [ ] Silhouette score > 0.4 (idÃ©alement > 0.5)
   [ ] Davies-Bouldin index < 1.5
   [ ] Clusters Ã©quilibrÃ©s en taille (sauf si intentionnel)
   [ ] Profils de clusters INTERPRÃ‰TABLES
   [ ] Validation avec Ã©quipes mÃ©tier

ğŸ“‹ ROBUSTESSE

   [ ] Tester stabilitÃ© (re-run avec random_state diffÃ©rent)
   [ ] Cross-validation si possible (inertie, silhouette)
   [ ] Tester sur Ã©chantillon hold-out (gÃ©nÃ©ralisation)
   [ ] Comparer plusieurs algorithmes
   [ ] Documenter choix d'hyperparamÃ¨tres

ğŸ“‹ PRODUCTION

   [ ] Pipeline de preprocessing sauvegardÃ© (scaler, PCA)
   [ ] ModÃ¨le sauvegardÃ© (joblib, pickle)
   [ ] Fonction de scoring nouveaux points implÃ©mentÃ©e
   [ ] Monitoring distribution clusters en production
   [ ] Plan de re-clustering rÃ©gulier (mensuel, trimestriel)
   [ ] Dashboard visualisation segments
   [ ] Documentation pour utilisateurs mÃ©tier


ğŸ“ FIN DU TUTORIEL CLUSTERING !

   Vous savez maintenant :
   âœ“ Choisir l'algorithme adaptÃ© au problÃ¨me
   âœ“ Trouver le nombre optimal de clusters
   âœ“ Ã‰valuer la qualitÃ© des clusters
   âœ“ InterprÃ©ter et nommer les segments
   âœ“ Valider avec mÃ©triques ET connaissance mÃ©tier
   âœ“ DÃ©ployer en production avec monitoring
""")

print("\n" + "="*80)
print("SAUVEGARDE DU MODÃˆLE")
print("="*80)

import joblib

# Sauvegarder le modÃ¨le et le scaler
joblib.dump(kmeans_final, 'E:\\Nicolas\\MIAGE\\M2\\BigData\\FORMATION_ML\\TUTORIELS\\kmeans_model.pkl')
joblib.dump(scaler, 'E:\\Nicolas\\MIAGE\\M2\\BigData\\FORMATION_ML\\TUTORIELS\\scaler_clustering.pkl')
joblib.dump(pca, 'E:\\Nicolas\\MIAGE\\M2\\BigData\\FORMATION_ML\\TUTORIELS\\pca_clustering.pkl')

print("\nâœ“ ModÃ¨le sauvegardÃ© : kmeans_model.pkl")
print("âœ“ Scaler sauvegardÃ© : scaler_clustering.pkl")
print("âœ“ PCA sauvegardÃ© : pca_clustering.pkl")

print("""
ğŸ“ UTILISATION EN PRODUCTION :

import joblib
import numpy as np

# Charger modÃ¨le et transformers
scaler = joblib.load('scaler_clustering.pkl')
kmeans = joblib.load('kmeans_model.pkl')
pca = joblib.load('pca_clustering.pkl')

# Nouveau client
nouveau_client = np.array([[30, 50, 70, 20, 15]])  # Age, Income, SpendingScore, Recency, Frequency

# Preprocessing
client_scaled = scaler.transform(nouveau_client)

# PrÃ©diction
segment = kmeans.predict(client_scaled)[0]
print(f"Client assignÃ© au segment : {segment}")

# Distance aux centroÃ¯des (confiance)
distances = kmeans.transform(client_scaled)[0]
print(f"Distance au centroÃ¯de assignÃ© : {distances[segment]:.2f}")
print(f"Distance au centroÃ¯de le plus proche suivant : {np.sort(distances)[1]:.2f}")

# Si distance trÃ¨s Ã©levÃ©e â†’ client atypique, investiguer
if distances[segment] > 2.0:
    print("âš ï¸ Client atypique pour son segment !")
""")

print("\n" + "="*80)
print("ğŸ‰ TUTORIEL TERMINÃ‰ AVEC SUCCÃˆS !")
print("="*80)
