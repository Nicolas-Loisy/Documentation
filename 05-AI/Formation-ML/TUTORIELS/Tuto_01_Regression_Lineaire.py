"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TUTORIEL COMPLET : RÃ‰GRESSION LINÃ‰AIRE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ce tutoriel couvre :
1. Comprendre la rÃ©gression linÃ©aire
2. PrÃ©parer les donnÃ©es
3. EntraÃ®ner le modÃ¨le
4. Optimiser les hyperparamÃ¨tres
5. Valider le modÃ¨le
6. Tester et interprÃ©ter les rÃ©sultats
7. Comparaison Ridge et Lasso

Chaque Ã©tape est expliquÃ©e en dÃ©tail avec les paramÃ¨tres et fonctions.
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_boston, make_regression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Configuration des graphiques
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

print("="*80)
print("TUTORIEL : RÃ‰GRESSION LINÃ‰AIRE")
print("="*80)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 1 : COMPRENDRE LA RÃ‰GRESSION LINÃ‰AIRE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 1 : THÃ‰ORIE")
print("="*80)

print("""
ğŸ¯ OBJECTIF
-----------
La rÃ©gression linÃ©aire vise Ã  modÃ©liser la relation entre :
- Une variable dÃ©pendante Y (cible)
- Une ou plusieurs variables indÃ©pendantes X (features)

ğŸ“ FORMULE MATHÃ‰MATIQUE
-----------------------
RÃ©gression linÃ©aire simple (1 feature) :
    Y = Î²â‚€ + Î²â‚Â·X + Îµ

RÃ©gression linÃ©aire multiple (n features) :
    Y = Î²â‚€ + Î²â‚Â·Xâ‚ + Î²â‚‚Â·Xâ‚‚ + ... + Î²â‚™Â·Xâ‚™ + Îµ

oÃ¹ :
- Y : variable Ã  prÃ©dire
- X : features (variables explicatives)
- Î²â‚€ : intercept (ordonnÃ©e Ã  l'origine)
- Î²â‚, Î²â‚‚, ..., Î²â‚™ : coefficients (pentes)
- Îµ : erreur rÃ©siduelle

ğŸ¯ OBJECTIF D'OPTIMISATION
---------------------------
Minimiser la somme des erreurs au carrÃ© (MSE) :

    MSE = (1/n) Î£(yáµ¢ - Å·áµ¢)Â²

oÃ¹ :
- yáµ¢ : valeur rÃ©elle
- Å·áµ¢ : valeur prÃ©dite
- n : nombre d'observations

ğŸ“Š HYPOTHÃˆSES DE LA RÃ‰GRESSION LINÃ‰AIRE
----------------------------------------
1. LinÃ©aritÃ© : relation linÃ©aire entre X et Y
2. IndÃ©pendance : observations indÃ©pendantes
3. HomoscÃ©dasticitÃ© : variance constante des rÃ©sidus
4. NormalitÃ© : rÃ©sidus suivent une distribution normale
5. Absence de multicolinÃ©aritÃ© : features peu corrÃ©lÃ©es entre elles
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 2 : GÃ‰NÃ‰RATION ET PRÃ‰PARATION DES DONNÃ‰ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 2 : PRÃ‰PARATION DES DONNÃ‰ES")
print("="*80)

# 2.1 GÃ©nÃ©rer des donnÃ©es synthÃ©tiques
print("\nğŸ“Š GÃ©nÃ©ration de donnÃ©es synthÃ©tiques...\n")

# make_regression : GÃ©nÃ¨re un problÃ¨me de rÃ©gression
# - n_samples : nombre d'Ã©chantillons
# - n_features : nombre de features
# - n_informative : nombre de features utiles
# - noise : Ã©cart-type du bruit gaussien
# - random_state : graine pour reproductibilitÃ©
X, y = make_regression(
    n_samples=500,
    n_features=5,
    n_informative=3,
    noise=10.0,
    random_state=42
)

print(f"âœ“ DonnÃ©es gÃ©nÃ©rÃ©es : {X.shape[0]} Ã©chantillons Ã— {X.shape[1]} features")
print(f"âœ“ Target : {y.shape[0]} valeurs")

# CrÃ©er un DataFrame pour faciliter la visualisation
feature_names = [f'Feature_{i+1}' for i in range(X.shape[1])]
df = pd.DataFrame(X, columns=feature_names)
df['Target'] = y

print("\nğŸ“ˆ AperÃ§u des donnÃ©es :")
print(df.head())

print("\nğŸ“‰ Statistiques descriptives :")
print(df.describe())

# 2.2 Visualisation des donnÃ©es
print("\nğŸ“Š Visualisation des distributions...")

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.ravel()

# Histogrammes des features
for i, col in enumerate(feature_names):
    axes[i].hist(df[col], bins=30, edgecolor='black', alpha=0.7)
    axes[i].set_title(f'Distribution de {col}')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('FrÃ©quence')

# Histogramme de la target
axes[5].hist(df['Target'], bins=30, edgecolor='black', alpha=0.7, color='orange')
axes[5].set_title('Distribution de Target')
axes[5].set_xlabel('Target')
axes[5].set_ylabel('FrÃ©quence')

plt.tight_layout()
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/regression_distributions.png')
plt.show()

print("âœ“ Graphique sauvegardÃ© : regression_distributions.png")

# 2.3 Matrice de corrÃ©lation
print("\nğŸ”— Analyse des corrÃ©lations...")

correlation_matrix = df.corr()
print("\nCorrÃ©lations avec la Target :")
print(correlation_matrix['Target'].sort_values(ascending=False))

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',
            square=True, linewidths=0.5, center=0)
plt.title('Matrice de CorrÃ©lation')
plt.tight_layout()
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/regression_correlation.png')
plt.show()

print("âœ“ Graphique sauvegardÃ© : regression_correlation.png")

# 2.4 Division des donnÃ©es
print("\nâœ‚ï¸  Division des donnÃ©es...")

# train_test_split : Divise les donnÃ©es en train et test
# - test_size : proportion du test set (0.2 = 20%)
# - random_state : graine pour reproductibilitÃ©
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

print(f"âœ“ Train set : {len(X_train)} Ã©chantillons ({len(X_train)/len(X)*100:.1f}%)")
print(f"âœ“ Val set   : {len(X_val)} Ã©chantillons ({len(X_val)/len(X)*100:.1f}%)")
print(f"âœ“ Test set  : {len(X_test)} Ã©chantillons ({len(X_test)/len(X)*100:.1f}%)")

# 2.5 Normalisation
print("\nâš–ï¸  Normalisation des donnÃ©es...")

# StandardScaler : Standardise les features (moyenne=0, Ã©cart-type=1)
# Formule : z = (x - Î¼) / Ïƒ
# - fit() : calcule moyenne et Ã©cart-type sur train
# - transform() : applique la transformation
# âš ï¸ IMPORTANT : fit uniquement sur train, transform sur train/val/test
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

print(f"âœ“ Moyenne des features (train) : {X_train_scaled.mean(axis=0)}")
print(f"âœ“ Ã‰cart-type des features (train) : {X_train_scaled.std(axis=0)}")
print("âœ“ DonnÃ©es normalisÃ©es")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 3 : MODÃˆLE DE BASE (BASELINE)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 3 : BASELINE")
print("="*80)

print("""
ğŸ“Œ POURQUOI UNE BASELINE ?
--------------------------
Avant de construire un modÃ¨le complexe, Ã©tablir une baseline permet de :
- Avoir un point de rÃ©fÃ©rence
- Valider que le modÃ¨le apporte de la valeur
- Identifier rapidement les problÃ¨mes

Baseline simple : PrÃ©dire la moyenne de Y
""")

# Baseline : prÃ©dire toujours la moyenne
y_train_mean = y_train.mean()
y_val_pred_baseline = np.full(len(y_val), y_train_mean)

mse_baseline = mean_squared_error(y_val, y_val_pred_baseline)
rmse_baseline = np.sqrt(mse_baseline)
mae_baseline = mean_absolute_error(y_val, y_val_pred_baseline)
r2_baseline = r2_score(y_val, y_val_pred_baseline)

print(f"\nğŸ“Š BASELINE (prÃ©dire moyenne = {y_train_mean:.2f})")
print(f"  MSE  : {mse_baseline:.2f}")
print(f"  RMSE : {rmse_baseline:.2f}")
print(f"  MAE  : {mae_baseline:.2f}")
print(f"  RÂ²   : {r2_baseline:.4f}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 4 : RÃ‰GRESSION LINÃ‰AIRE SIMPLE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 4 : RÃ‰GRESSION LINÃ‰AIRE")
print("="*80)

print("""
ğŸ¤– LinearRegression de Scikit-Learn
------------------------------------
ImplÃ©mente la rÃ©gression linÃ©aire par moindres carrÃ©s ordinaires (OLS).

PARAMÃˆTRES PRINCIPAUX :
- fit_intercept (bool, default=True) : Calculer l'intercept ?
- normalize (bool, default=False) : Normaliser avant fit ?
- n_jobs (int, default=None) : Nombre de CPU (-1 = tous)

MÃ‰THODE :
- fit(X, y) : EntraÃ®ne le modÃ¨le
- predict(X) : Fait des prÃ©dictions
- score(X, y) : Retourne le RÂ² score

ATTRIBUTS APRÃˆS FIT :
- coef_ : Coefficients de la rÃ©gression
- intercept_ : OrdonnÃ©e Ã  l'origine
""")

# 4.1 CrÃ©er et entraÃ®ner le modÃ¨le
print("\nğŸš€ EntraÃ®nement du modÃ¨le...\n")

# CrÃ©er l'instance du modÃ¨le
# fit_intercept=True : calculer l'intercept (Î²â‚€)
model_lr = LinearRegression(fit_intercept=True)

# fit() : EntraÃ®ne le modÃ¨le sur les donnÃ©es d'entraÃ®nement
# Calcule les coefficients qui minimisent MSE
model_lr.fit(X_train_scaled, y_train)

print("âœ“ ModÃ¨le entraÃ®nÃ©")

# 4.2 Examiner les paramÃ¨tres du modÃ¨le
print("\nğŸ“ ParamÃ¨tres du modÃ¨le :")
print(f"  Intercept (Î²â‚€) : {model_lr.intercept_:.4f}")
print(f"\n  Coefficients (Î²â‚, ..., Î²â‚™) :")
for i, coef in enumerate(model_lr.coef_):
    print(f"    Feature_{i+1} : {coef:.4f}")

# 4.3 PrÃ©dictions
print("\nğŸ”® PrÃ©dictions...\n")

# predict() : GÃ©nÃ¨re des prÃ©dictions
# Å· = Î²â‚€ + Î²â‚Â·Xâ‚ + Î²â‚‚Â·Xâ‚‚ + ... + Î²â‚™Â·Xâ‚™
y_train_pred = model_lr.predict(X_train_scaled)
y_val_pred = model_lr.predict(X_val_scaled)
y_test_pred = model_lr.predict(X_test_scaled)

print("âœ“ PrÃ©dictions gÃ©nÃ©rÃ©es")

# 4.4 Ã‰valuation
print("\nğŸ“Š Ã‰VALUATION\n")

# MÃ©triques sur train
train_mse = mean_squared_error(y_train, y_train_pred)
train_rmse = np.sqrt(train_mse)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

# MÃ©triques sur validation
val_mse = mean_squared_error(y_val, y_val_pred)
val_rmse = np.sqrt(val_mse)
val_mae = mean_absolute_error(y_val, y_val_pred)
val_r2 = r2_score(y_val, y_val_pred)

print("Train Set :")
print(f"  MSE  : {train_mse:.2f}")
print(f"  RMSE : {train_rmse:.2f}")
print(f"  MAE  : {train_mae:.2f}")
print(f"  RÂ²   : {train_r2:.4f}")

print("\nValidation Set :")
print(f"  MSE  : {val_mse:.2f}")
print(f"  RMSE : {val_rmse:.2f}")
print(f"  MAE  : {val_mae:.2f}")
print(f"  RÂ²   : {val_r2:.4f}")

print("\nComparaison avec Baseline :")
print(f"  AmÃ©lioration RÂ² : {val_r2 - r2_baseline:.4f}")
print(f"  AmÃ©lioration RMSE : {rmse_baseline - val_rmse:.2f}")

# 4.5 Visualisation des prÃ©dictions
print("\nğŸ“Š Visualisation des prÃ©dictions...")

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Scatter plot : PrÃ©dictions vs RÃ©alitÃ©
axes[0].scatter(y_val, y_val_pred, alpha=0.6, edgecolors='k')
axes[0].plot([y_val.min(), y_val.max()],
             [y_val.min(), y_val.max()],
             'r--', lw=2, label='PrÃ©diction parfaite')
axes[0].set_xlabel('Valeurs RÃ©elles')
axes[0].set_ylabel('PrÃ©dictions')
axes[0].set_title(f'PrÃ©dictions vs RÃ©alitÃ© (RÂ² = {val_r2:.4f})')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Histogramme des rÃ©sidus
residuals = y_val - y_val_pred
axes[1].hist(residuals, bins=30, edgecolor='black', alpha=0.7)
axes[1].axvline(0, color='r', linestyle='--', linewidth=2)
axes[1].set_xlabel('RÃ©sidus')
axes[1].set_ylabel('FrÃ©quence')
axes[1].set_title(f'Distribution des RÃ©sidus (MAE = {val_mae:.2f})')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/regression_predictions.png')
plt.show()

print("âœ“ Graphique sauvegardÃ© : regression_predictions.png")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 5 : VALIDATION CROISÃ‰E
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 5 : VALIDATION CROISÃ‰E (CROSS-VALIDATION)")
print("="*80)

print("""
ğŸ”„ CROSS-VALIDATION
-------------------
Technique pour Ã©valuer la robustesse d'un modÃ¨le :
- Divise les donnÃ©es en K folds
- EntraÃ®ne K fois (chaque fois avec 1 fold pour validation)
- Moyenne les scores pour avoir une estimation plus fiable

AVANTAGES :
âœ“ Utilise toutes les donnÃ©es pour entraÃ®nement et validation
âœ“ RÃ©duit la variance de l'estimation
âœ“ DÃ©tecte l'overfitting

K typique : 5 ou 10
""")

# cross_val_score : Effectue la cross-validation
# - estimator : modÃ¨le Ã  Ã©valuer
# - X, y : donnÃ©es
# - cv : nombre de folds
# - scoring : mÃ©trique ('r2', 'neg_mean_squared_error', etc.)
# - n_jobs : parallÃ©lisation
cv_scores_r2 = cross_val_score(
    model_lr, X_train_scaled, y_train,
    cv=5,
    scoring='r2',
    n_jobs=-1
)

cv_scores_mse = cross_val_score(
    model_lr, X_train_scaled, y_train,
    cv=5,
    scoring='neg_mean_squared_error',  # nÃ©gatif car sklearn minimise
    n_jobs=-1
)

cv_scores_mse = -cv_scores_mse  # Remettre en positif

print(f"\nğŸ“Š RÃ‰SULTATS CROSS-VALIDATION (5 folds)\n")
print(f"RÂ² Scores : {cv_scores_r2}")
print(f"  Moyenne : {cv_scores_r2.mean():.4f} (Â± {cv_scores_r2.std():.4f})")

print(f"\nMSE Scores : {cv_scores_mse}")
print(f"  Moyenne : {cv_scores_mse.mean():.2f} (Â± {cv_scores_mse.std():.2f})")

# Visualisation
plt.figure(figsize=(10, 5))
plt.bar(range(1, 6), cv_scores_r2, alpha=0.7, edgecolor='black')
plt.axhline(cv_scores_r2.mean(), color='r', linestyle='--',
            label=f'Moyenne = {cv_scores_r2.mean():.4f}')
plt.xlabel('Fold')
plt.ylabel('RÂ² Score')
plt.title('Cross-Validation : RÂ² par Fold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/regression_cv.png')
plt.show()

print("âœ“ Graphique sauvegardÃ© : regression_cv.png")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 6 : RÃ‰GULARISATION (RIDGE ET LASSO)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 6 : RÃ‰GULARISATION - RIDGE ET LASSO")
print("="*80)

print("""
ğŸ›¡ï¸  POURQUOI LA RÃ‰GULARISATION ?
--------------------------------
ProblÃ¨mes de la rÃ©gression linÃ©aire classique :
- Overfitting si beaucoup de features
- MulticollinÃ©aritÃ© (features corrÃ©lÃ©es)
- Coefficients instables

SOLUTION : Ajouter un terme de pÃ©nalitÃ©

1ï¸âƒ£  RIDGE REGRESSION (L2)
-------------------------
Minimise : MSE + Î±Â·Î£(Î²áµ¢Â²)

PARAMÃˆTRES :
- alpha : Force de rÃ©gularisation
  * alpha = 0 : rÃ©gression linÃ©aire classique
  * alpha petit : peu de rÃ©gularisation
  * alpha grand : forte rÃ©gularisation

AVANTAGES :
âœ“ RÃ©duit les coefficients
âœ“ GÃ¨re la multicollinÃ©aritÃ©
âœ“ Garde toutes les features

INCONVÃ‰NIENTS :
âœ— Ne fait pas de sÃ©lection de features

2ï¸âƒ£  LASSO REGRESSION (L1)
--------------------------
Minimise : MSE + Î±Â·Î£|Î²áµ¢|

AVANTAGES :
âœ“ Met certains coefficients Ã  0 â†’ SÃ©lection de features
âœ“ ModÃ¨le plus simple et interprÃ©table

INCONVÃ‰NIENTS :
âœ— Si features corrÃ©lÃ©es, sÃ©lectionne arbitrairement

3ï¸âƒ£  ELASTICNET (L1 + L2)
-------------------------
Combine Ridge et Lasso
Minimise : MSE + Î±Â·(Î»Â·Î£|Î²áµ¢| + (1-Î»)Â·Î£(Î²áµ¢Â²))

PARAMÃˆTRES :
- alpha : Force de rÃ©gularisation
- l1_ratio : Mix L1/L2 (0=Ridge, 1=Lasso, 0.5=Ã©quilibre)
""")

# 6.1 RIDGE REGRESSION
print("\n" + "-"*80)
print("6.1 RIDGE REGRESSION (L2)")
print("-"*80)

# Tester diffÃ©rents alpha
alphas = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
ridge_results = []

for alpha in alphas:
    # Ridge : RÃ©gression linÃ©aire avec pÃ©nalitÃ© L2
    # - alpha : Force de rÃ©gularisation
    # - fit_intercept : Calculer l'intercept
    # - solver : Algorithme ('auto', 'svd', 'cholesky', 'lsqr', 'sag')
    model_ridge = Ridge(alpha=alpha, fit_intercept=True)
    model_ridge.fit(X_train_scaled, y_train)

    y_val_pred_ridge = model_ridge.predict(X_val_scaled)
    r2_ridge = r2_score(y_val, y_val_pred_ridge)
    mse_ridge = mean_squared_error(y_val, y_val_pred_ridge)

    ridge_results.append({
        'alpha': alpha,
        'r2': r2_ridge,
        'mse': mse_ridge,
        'model': model_ridge
    })

    print(f"Alpha = {alpha:6.3f} | RÂ² = {r2_ridge:.4f} | MSE = {mse_ridge:.2f}")

# Meilleur alpha
best_ridge = max(ridge_results, key=lambda x: x['r2'])
print(f"\nâœ“ Meilleur alpha Ridge : {best_ridge['alpha']}")
print(f"  RÂ² : {best_ridge['r2']:.4f}")

# 6.2 LASSO REGRESSION
print("\n" + "-"*80)
print("6.2 LASSO REGRESSION (L1)")
print("-"*80)

lasso_results = []

for alpha in alphas:
    # Lasso : RÃ©gression linÃ©aire avec pÃ©nalitÃ© L1
    # - alpha : Force de rÃ©gularisation
    # - max_iter : Nombre max d'itÃ©rations
    # - tol : TolÃ©rance pour l'arrÃªt
    model_lasso = Lasso(alpha=alpha, fit_intercept=True, max_iter=10000)
    model_lasso.fit(X_train_scaled, y_train)

    y_val_pred_lasso = model_lasso.predict(X_val_scaled)
    r2_lasso = r2_score(y_val, y_val_pred_lasso)
    mse_lasso = mean_squared_error(y_val, y_val_pred_lasso)

    # Compter features sÃ©lectionnÃ©es (coef != 0)
    n_features_selected = np.sum(model_lasso.coef_ != 0)

    lasso_results.append({
        'alpha': alpha,
        'r2': r2_lasso,
        'mse': mse_lasso,
        'n_features': n_features_selected,
        'model': model_lasso
    })

    print(f"Alpha = {alpha:6.3f} | RÂ² = {r2_lasso:.4f} | Features = {n_features_selected}/{X.shape[1]}")

# Meilleur alpha
best_lasso = max(lasso_results, key=lambda x: x['r2'])
print(f"\nâœ“ Meilleur alpha Lasso : {best_lasso['alpha']}")
print(f"  RÂ² : {best_lasso['r2']:.4f}")
print(f"  Features sÃ©lectionnÃ©es : {best_lasso['n_features']}/{X.shape[1]}")

# 6.3 Comparaison des coefficients
print("\nğŸ“Š Comparaison des coefficients...")

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Linear Regression
axes[0].bar(range(len(model_lr.coef_)), model_lr.coef_, alpha=0.7, edgecolor='black')
axes[0].set_xlabel('Feature Index')
axes[0].set_ylabel('Coefficient')
axes[0].set_title('Linear Regression')
axes[0].grid(True, alpha=0.3)

# Ridge
axes[1].bar(range(len(best_ridge['model'].coef_)), best_ridge['model'].coef_,
            alpha=0.7, edgecolor='black', color='orange')
axes[1].set_xlabel('Feature Index')
axes[1].set_ylabel('Coefficient')
axes[1].set_title(f'Ridge (Î±={best_ridge["alpha"]})')
axes[1].grid(True, alpha=0.3)

# Lasso
axes[2].bar(range(len(best_lasso['model'].coef_)), best_lasso['model'].coef_,
            alpha=0.7, edgecolor='black', color='green')
axes[2].set_xlabel('Feature Index')
axes[2].set_ylabel('Coefficient')
axes[2].set_title(f'Lasso (Î±={best_lasso["alpha"]})')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/regression_coefficients.png')
plt.show()

print("âœ“ Graphique sauvegardÃ© : regression_coefficients.png")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 7 : Ã‰VALUATION FINALE SUR TEST SET
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 7 : Ã‰VALUATION FINALE SUR TEST SET")
print("="*80)

print("""
ğŸ¯ Ã‰VALUATION FINALE
--------------------
Le test set n'a JAMAIS Ã©tÃ© vu par le modÃ¨le pendant l'entraÃ®nement.
Il sert Ã  Ã©valuer la performance rÃ©elle en production.

âš ï¸ IMPORTANT : Ã‰valuer une seule fois sur le test set !
Si on l'utilise plusieurs fois, il devient un validation set.
""")

# Ã‰valuer tous les modÃ¨les sur le test set
models = {
    'Linear Regression': model_lr,
    'Ridge': best_ridge['model'],
    'Lasso': best_lasso['model']
}

print("\nğŸ“Š RÃ‰SULTATS SUR TEST SET\n")
print("-" * 60)
print(f"{'ModÃ¨le':<20} {'RÂ²':<10} {'RMSE':<10} {'MAE':<10}")
print("-" * 60)

test_results = {}

for name, model in models.items():
    y_test_pred = model.predict(X_test_scaled)

    test_r2 = r2_score(y_test, y_test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    test_mae = mean_absolute_error(y_test, y_test_pred)

    test_results[name] = {
        'r2': test_r2,
        'rmse': test_rmse,
        'mae': test_mae
    }

    print(f"{name:<20} {test_r2:<10.4f} {test_rmse:<10.2f} {test_mae:<10.2f}")

print("-" * 60)

# Meilleur modÃ¨le
best_model_name = max(test_results, key=lambda k: test_results[k]['r2'])
print(f"\nğŸ† Meilleur modÃ¨le : {best_model_name}")
print(f"   RÂ² = {test_results[best_model_name]['r2']:.4f}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 8 : INTERPRÃ‰TATION ET DIAGNOSTICS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 8 : INTERPRÃ‰TATION")
print("="*80)

best_model = models[best_model_name]
y_test_pred = best_model.predict(X_test_scaled)

# 8.1 Analyse des rÃ©sidus
print("\nğŸ“Š Analyse des rÃ©sidus...\n")

residuals_test = y_test - y_test_pred

print(f"Statistiques des rÃ©sidus :")
print(f"  Moyenne : {residuals_test.mean():.4f} (devrait Ãªtre proche de 0)")
print(f"  Ã‰cart-type : {residuals_test.std():.2f}")
print(f"  Min : {residuals_test.min():.2f}")
print(f"  Max : {residuals_test.max():.2f}")

# Visualisation complÃ¨te
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. PrÃ©dictions vs RÃ©alitÃ©
axes[0, 0].scatter(y_test, y_test_pred, alpha=0.6, edgecolors='k')
axes[0, 0].plot([y_test.min(), y_test.max()],
                [y_test.min(), y_test.max()],
                'r--', lw=2)
axes[0, 0].set_xlabel('Valeurs RÃ©elles')
axes[0, 0].set_ylabel('PrÃ©dictions')
axes[0, 0].set_title('PrÃ©dictions vs RÃ©alitÃ©')
axes[0, 0].grid(True, alpha=0.3)

# 2. RÃ©sidus vs PrÃ©dictions
axes[0, 1].scatter(y_test_pred, residuals_test, alpha=0.6, edgecolors='k')
axes[0, 1].axhline(0, color='r', linestyle='--', lw=2)
axes[0, 1].set_xlabel('PrÃ©dictions')
axes[0, 1].set_ylabel('RÃ©sidus')
axes[0, 1].set_title('RÃ©sidus vs PrÃ©dictions')
axes[0, 1].grid(True, alpha=0.3)

# 3. Distribution des rÃ©sidus
axes[1, 0].hist(residuals_test, bins=30, edgecolor='black', alpha=0.7)
axes[1, 0].axvline(0, color='r', linestyle='--', lw=2)
axes[1, 0].set_xlabel('RÃ©sidus')
axes[1, 0].set_ylabel('FrÃ©quence')
axes[1, 0].set_title('Distribution des RÃ©sidus')
axes[1, 0].grid(True, alpha=0.3)

# 4. Q-Q plot
from scipy import stats
stats.probplot(residuals_test, dist="norm", plot=axes[1, 1])
axes[1, 1].set_title('Q-Q Plot (NormalitÃ© des rÃ©sidus)')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/regression_diagnostics.png')
plt.show()

print("âœ“ Graphique sauvegardÃ© : regression_diagnostics.png")

# 8.2 Importance des features
print("\nğŸ“Š Importance des features (coefficients)...\n")

if hasattr(best_model, 'coef_'):
    feature_importance = pd.DataFrame({
        'Feature': feature_names,
        'Coefficient': best_model.coef_,
        'Abs_Coefficient': np.abs(best_model.coef_)
    })
    feature_importance = feature_importance.sort_values('Abs_Coefficient', ascending=False)

    print(feature_importance)

    plt.figure(figsize=(10, 6))
    plt.barh(feature_importance['Feature'], feature_importance['Coefficient'])
    plt.xlabel('Coefficient')
    plt.title(f'Importance des Features - {best_model_name}')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/regression_feature_importance.png')
    plt.show()

    print("âœ“ Graphique sauvegardÃ© : regression_feature_importance.png")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 9 : SAUVEGARDE DU MODÃˆLE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 9 : SAUVEGARDE")
print("="*80)

import joblib

# Sauvegarder le modÃ¨le et le scaler
joblib.dump(best_model, 'e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/best_regression_model.pkl')
joblib.dump(scaler, 'e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/scaler.pkl')

print("âœ“ ModÃ¨le sauvegardÃ© : best_regression_model.pkl")
print("âœ“ Scaler sauvegardÃ© : scaler.pkl")

print("""
ğŸ“¦ UTILISATION DU MODÃˆLE SAUVEGARDÃ‰
-----------------------------------
Pour charger et utiliser le modÃ¨le :

```python
import joblib
import numpy as np

# Charger
model = joblib.load('best_regression_model.pkl')
scaler = joblib.load('scaler.pkl')

# PrÃ©dire sur nouvelles donnÃ©es
X_new = np.array([[...]])  # Vos nouvelles donnÃ©es
X_new_scaled = scaler.transform(X_new)
predictions = model.predict(X_new_scaled)
```
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RÃ‰SUMÃ‰ ET CONCLUSIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("ğŸ‰ RÃ‰SUMÃ‰ ET CONCLUSIONS")
print("="*80)

print(f"""
ğŸ“Š RÃ‰SULTATS FINAUX
-------------------
Meilleur modÃ¨le : {best_model_name}
  RÂ² Score : {test_results[best_model_name]['r2']:.4f}
  RMSE : {test_results[best_model_name]['rmse']:.2f}
  MAE : {test_results[best_model_name]['mae']:.2f}

ğŸ“ˆ PERFORMANCE
--------------
- RÂ² proche de 1 : Excellent modÃ¨le
- RÂ² > 0.7 : Bon modÃ¨le
- RÂ² > 0.5 : ModÃ¨le acceptable
- RÂ² < 0.5 : ModÃ¨le faible

ğŸ¯ INTERPRÃ‰TATION DU RÂ²
-----------------------
Le modÃ¨le explique {test_results[best_model_name]['r2']*100:.1f}% de la variance de la variable cible.

âœ… CHECKLIST RÃ‰GRESSION LINÃ‰AIRE
---------------------------------
âœ“ DonnÃ©es prÃ©parÃ©es et normalisÃ©es
âœ“ Baseline Ã©tablie
âœ“ ModÃ¨le entraÃ®nÃ©
âœ“ Cross-validation effectuÃ©e
âœ“ RÃ©gularisation testÃ©e (Ridge/Lasso)
âœ“ Ã‰valuation sur test set
âœ“ RÃ©sidus analysÃ©s
âœ“ ModÃ¨le sauvegardÃ©

ğŸš€ PROCHAINES Ã‰TAPES
--------------------
1. Tester sur de nouvelles donnÃ©es rÃ©elles
2. Feature engineering avancÃ©
3. Essayer d'autres modÃ¨les (Random Forest, XGBoost)
4. Optimiser les hyperparamÃ¨tres davantage
5. DÃ©ployer en production

ğŸ’¡ CONSEILS
-----------
- Toujours vÃ©rifier les hypothÃ¨ses de la rÃ©gression linÃ©aire
- Analyser les rÃ©sidus pour diagnostiquer les problÃ¨mes
- La normalisation est cruciale pour Ridge/Lasso
- Cross-validation donne une meilleure estimation de la performance
- Ne jamais sur-optimiser sur le test set !
""")

print("="*80)
print("âœ¨ TUTORIEL TERMINÃ‰ AVEC SUCCÃˆS ! âœ¨")
print("="*80)
