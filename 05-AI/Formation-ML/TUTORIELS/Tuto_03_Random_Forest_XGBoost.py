"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TUTORIEL COMPLET : RANDOM FOREST & XGBOOST - MODÃˆLES ENSEMBLE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ CAS D'USAGE RÃ‰EL : PrÃ©diction de Churn (DÃ©sabonnement Client)

CONTEXTE :
Une entreprise tÃ©lÃ©coms veut prÃ©dire quels clients vont se dÃ©sabonner (churn).
Objectif : Identifier clients Ã  risque pour actions de rÃ©tention ciblÃ©es.

POURQUOI RANDOM FOREST & XGBOOST ?
- Performance MAXIMALE sur donnÃ©es tabulaires
- Standard de l'industrie (Kaggle, production)
- Robustes, gÃ¨rent bien donnÃ©es bruitÃ©es
- Peu de preprocessing nÃ©cessaire

Ce tutoriel couvre :
1. POURQUOI Random Forest et XGBoost dominent le ML industriel
2. QUAND utiliser RF vs XGBoost vs LightGBM
3. Random Forest : principe, hyperparamÃ¨tres, optimisation
4. XGBoost : principe, hyperparamÃ¨tres, optimisation
5. Comparaison dÃ©taillÃ©e et choix du meilleur
6. Diagnostic avancÃ© (learning curves, overfitting)
7. Feature engineering et importance
8. Optimisation poussÃ©e (Grid Search, Random Search)

Chaque Ã©tape explique CE QU'IL FAUT OBSERVER et LES CONCLUSIONS.
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_classification
from sklearn.model_selection import (
    train_test_split, cross_val_score, GridSearchCV,
    RandomizedSearchCV, learning_curve
)
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score, roc_curve
)
from sklearn.tree import plot_tree
import warnings
warnings.filterwarnings('ignore')

# XGBoost
try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ImportError:
    print("âš ï¸  XGBoost non installÃ©. Installez avec : pip install xgboost")
    XGBOOST_AVAILABLE = False

# LightGBM
try:
    from lightgbm import LGBMClassifier
    LIGHTGBM_AVAILABLE = True
except ImportError:
    print("âš ï¸  LightGBM non installÃ©. Installez avec : pip install lightgbm")
    LIGHTGBM_AVAILABLE = False

# Configuration
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

print("="*80)
print("TUTORIEL : RANDOM FOREST & XGBOOST - MODÃˆLES ENSEMBLE")
print("="*80)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 1 : COMPRENDRE LES MODÃˆLES ENSEMBLE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 1 : POURQUOI RANDOM FOREST ET XGBOOST DOMINENT LE ML ?")
print("="*80)

print("""
ğŸ† LES ROIS DU ML SUR DONNÃ‰ES TABULAIRES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Random Forest et XGBoost sont les modÃ¨les les PLUS UTILISÃ‰S en production
et dominent les compÃ©titions Kaggle pour donnÃ©es tabulaires.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“š PRINCIPE DES MODÃˆLES ENSEMBLE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¡ IDÃ‰E CENTRALE : "L'union fait la force"
   Combiner plusieurs modÃ¨les FAIBLES â†’ ModÃ¨le FORT

ANALOGIE : Jury vs Juge unique
   - 1 juge peut se tromper
   - 10 juges votent â†’ dÃ©cision plus robuste

1ï¸âƒ£  RANDOM FOREST (Bagging)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PRINCIPE : Moyenne de Decision Trees indÃ©pendants

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tree 1  â”‚  â”‚ Tree 2  â”‚  â”‚ Tree N  â”‚  â”€â”€â”€â†’ â”‚  VOTE    â”‚ â”€â”€â”€â†’ PrÃ©diction
â”‚ (vote)  â”‚  â”‚ (vote)  â”‚  â”‚ (vote)  â”‚       â”‚ MajoritÃ© â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ã‰TAPES :
1. CrÃ©er N arbres de dÃ©cision
2. Chaque arbre entraÃ®nÃ© sur Ã©chantillon alÃ©atoire (bootstrap)
3. Chaque split utilise sous-ensemble alÃ©atoire de features
4. PrÃ©diction finale = vote majoritaire (classification)

AVANTAGES :
âœ… RÃ©duit OVERFITTING (chaque arbre overfit diffÃ©remment)
âœ… Robuste au bruit
âœ… ParallÃ©lisable (arbres indÃ©pendants)
âœ… Peu sensible aux hyperparamÃ¨tres
âœ… Pas besoin de normalisation

INCONVÃ‰NIENTS :
âŒ Moins performant que Boosting sur donnÃ©es propres
âŒ ModÃ¨le "boÃ®te noire" (moins interprÃ©table qu'arbre unique)
âŒ Lent Ã  prÃ©dire (doit Ã©valuer tous les arbres)


2ï¸âƒ£  XGBOOST (Gradient Boosting)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PRINCIPE : Arbres sÃ©quentiels corrigeant erreurs prÃ©cÃ©dentes

Tree 1 â”€â”€â”€â†’ Erreurs 1 â”€â”€â”€â†’ Tree 2 â”€â”€â”€â†’ Erreurs 2 â”€â”€â”€â†’ Tree 3 â”€â”€â”€â†’ ...
(prÃ©diction)  (focus sur    (corrige      (focus sur    (corrige
               erreurs)      erreurs 1)    erreurs 2)    erreurs 2)

Ã‰TAPES :
1. EntraÃ®ner arbre sur donnÃ©es
2. Calculer rÃ©sidus (erreurs)
3. EntraÃ®ner nouvel arbre pour prÃ©dire rÃ©sidus
4. Ajouter prÃ©diction pondÃ©rÃ©e au modÃ¨le
5. RÃ©pÃ©ter jusqu'Ã  convergence ou max_trees

AVANTAGES :
âœ… Performance MAXIMALE sur donnÃ©es tabulaires
âœ… RÃ©gularisation intÃ©grÃ©e (L1, L2, Gamma)
âœ… GÃ¨re valeurs manquantes nativement
âœ… GÃ¨re classes dÃ©sÃ©quilibrÃ©es (scale_pos_weight)
âœ… Supporte GPU

INCONVÃ‰NIENTS :
âŒ Plus sensible aux hyperparamÃ¨tres (tuning nÃ©cessaire)
âŒ Risque d'overfitting si mal configurÃ©
âŒ Non parallÃ©lisable (arbres sÃ©quentiels)
âŒ Plus lent Ã  entraÃ®ner que Random Forest


3ï¸âƒ£  LIGHTGBM (Gradient Boosting OptimisÃ©)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PRINCIPE : Boosting optimisÃ© pour grandes donnÃ©es

AVANTAGES vs XGBoost :
âœ… Plus RAPIDE (algorithme GOSS + EFB)
âœ… Moins de mÃ©moire
âœ… Meilleur sur grandes donnÃ©es (>10k lignes)
âœ… GÃ¨re catÃ©gories nativement

INCONVÃ‰NIENTS :
âŒ Peut overfitter sur petites donnÃ©es
âŒ Plus sensible aux hyperparamÃ¨tres


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š TABLEAU DE DÃ‰CISION : QUAND UTILISER QUOI ?
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CritÃ¨re           â”‚  Random Forest â”‚  XGBoost       â”‚  LightGBM      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Performance        â”‚ â˜…â˜…â˜…â˜…â˜†          â”‚ â˜…â˜…â˜…â˜…â˜…          â”‚ â˜…â˜…â˜…â˜…â˜…          â”‚
â”‚ Vitesse entraÃ®n.   â”‚ â˜…â˜…â˜…â˜…â˜…          â”‚ â˜…â˜…â˜…â˜†â˜†          â”‚ â˜…â˜…â˜…â˜…â˜…          â”‚
â”‚ Vitesse infÃ©rence  â”‚ â˜…â˜…â˜†â˜†â˜†          â”‚ â˜…â˜…â˜…â˜†â˜†          â”‚ â˜…â˜…â˜…â˜…â˜†          â”‚
â”‚ Robustesse         â”‚ â˜…â˜…â˜…â˜…â˜…          â”‚ â˜…â˜…â˜…â˜…â˜†          â”‚ â˜…â˜…â˜…â˜†â˜†          â”‚
â”‚ FacilitÃ© tuning    â”‚ â˜…â˜…â˜…â˜…â˜…          â”‚ â˜…â˜…â˜…â˜†â˜†          â”‚ â˜…â˜…â˜†â˜†â˜†          â”‚
â”‚ Grandes donnÃ©es    â”‚ â˜…â˜…â˜…â˜†â˜†          â”‚ â˜…â˜…â˜…â˜…â˜†          â”‚ â˜…â˜…â˜…â˜…â˜…          â”‚
â”‚ InterprÃ©tabilitÃ©   â”‚ â˜…â˜…â˜…â˜†â˜†          â”‚ â˜…â˜…â˜…â˜†â˜†          â”‚ â˜…â˜…â˜…â˜†â˜†          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ RECOMMANDATIONS :

UTILISER RANDOM FOREST QUAND :
âœ… PremiÃ¨re approche (baseline robuste)
âœ… Peu de temps pour tuning
âœ… DonnÃ©es bruitÃ©es
âœ… Peu de donnÃ©es (<10k)
âœ… Besoin de parallÃ©lisation
âœ… Feature importance simple requise

UTILISER XGBOOST QUAND :
âœ… Performance maximale requise
âœ… CompÃ©tition Kaggle
âœ… DonnÃ©es moyennes (10k-1M)
âœ… Temps pour tuning disponible
âœ… Classes dÃ©sÃ©quilibrÃ©es

UTILISER LIGHTGBM QUAND :
âœ… Grandes donnÃ©es (>100k)
âœ… Vitesse critique
âœ… Features catÃ©gorielles nombreuses
âœ… Ressources limitÃ©es (mÃ©moire)

ğŸ’¼ CAS D'USAGE TYPIQUES :
- PrÃ©diction de churn (notre cas !) â†’ XGBoost ou LightGBM
- Scoring de crÃ©dit â†’ XGBoost
- DÃ©tection de fraude â†’ XGBoost
- Recommandation â†’ XGBoost ou LightGBM
- Forecast de ventes â†’ XGBoost
""")

input("\nâ–¶ Appuyez sur EntrÃ©e pour continuer...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 2 : PRÃ‰PARATION DES DONNÃ‰ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 2 : DONNÃ‰ES - PRÃ‰DICTION DE CHURN")
print("="*80)

# 2.1 GÃ©nÃ©ration de donnÃ©es
print("\nğŸ“Š GÃ©nÃ©ration de donnÃ©es synthÃ©tiques (simulant clients tÃ©lÃ©coms)...\n")

# DonnÃ©es plus complexes pour montrer puissance des modÃ¨les ensemble
X, y = make_classification(
    n_samples=2000,
    n_features=20,
    n_informative=15,
    n_redundant=3,
    n_classes=2,
    weights=[0.70, 0.30],  # 70% rÃ©tention, 30% churn
    flip_y=0.03,
    random_state=42
)

print(f"âœ“ DonnÃ©es gÃ©nÃ©rÃ©es : {X.shape[0]} clients Ã— {X.shape[1]} features")
print(f"âœ“ Distribution : {np.bincount(y)[0]} rÃ©tention ({np.bincount(y)[0]/len(y)*100:.1f}%), "
      f"{np.bincount(y)[1]} churn ({np.bincount(y)[1]/len(y)*100:.1f}%)")

# Features rÃ©alistes
feature_names = [
    'Tenure', 'MonthlyCharges', 'TotalCharges', 'ContractLength',
    'DataUsage', 'CallMinutes', 'SMSCount', 'CustomerServiceCalls',
    'PaymentMethod', 'AutoPay', 'PaperlessBilling', 'TechSupport',
    'OnlineSecurity', 'DeviceProtection', 'StreamingTV', 'StreamingMovies',
    'Age', 'DependentsCount', 'PartnerStatus', 'SeniorCitizen'
]

df = pd.DataFrame(X, columns=feature_names)
df['Churn'] = y

print("\nğŸ“ˆ AperÃ§u :")
print(df.head())

# 2.2 Split des donnÃ©es
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

print(f"\nâœ“ Train : {len(X_train)} | Val : {len(X_val)} | Test : {len(X_test)}")

print("""
ğŸ’¡ QUESTION : Faut-il normaliser pour RF et XGBoost ?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RÃ‰PONSE : NON !

Random Forest et XGBoost sont basÃ©s sur ARBRES DE DÃ‰CISION.
Les arbres splitent sur SEUILS, pas magnitudes.
â†’ Normalisation N'APPORTE RIEN (parfois mÃªme nuit)

EXCEPTION : Si vous voulez comparer coefficients/importance entre features
sur mÃªmes Ã©chelles â†’ alors normaliser.

Pour ce tutoriel : PAS de normalisation (inutile).
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 3 : RANDOM FOREST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 3 : RANDOM FOREST")
print("="*80)

print("""
ğŸŒ² HYPERPARAMÃˆTRES CLÃ‰S DE RANDOM FOREST
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. n_estimators : Nombre d'arbres
   - Plus grand = Meilleur (jusqu'Ã  plateau)
   - DÃ©faut : 100, RecommandÃ© : 200-500
   - âš ï¸  Plus lent avec beaucoup d'arbres

2. max_depth : Profondeur max de chaque arbre
   - None = Pas de limite (risque overfitting)
   - RecommandÃ© : 10-30
   - ContrÃ´le complexitÃ©

3. min_samples_split : Min Ã©chantillons pour split
   - DÃ©faut : 2, RecommandÃ© : 10-20
   - Plus grand = Moins de splits = Moins d'overfitting

4. min_samples_leaf : Min Ã©chantillons dans feuille
   - DÃ©faut : 1, RecommandÃ© : 5-10
   - Ã‰vite feuilles trop spÃ©cifiques

5. max_features : Nb features par split
   - 'sqrt' : âˆšn_features (dÃ©faut, recommandÃ©)
   - 'log2' : log2(n_features)
   - ContrÃ´le diversitÃ© des arbres

6. class_weight : Gestion dÃ©sÃ©quilibre
   - 'balanced' : PÃ©nalise classe minoritaire
   - Utile pour churn (classe minoritaire importante)

7. n_jobs : ParallÃ©lisation
   - -1 : Utiliser tous les CPU
   - AccÃ©lÃ¨re beaucoup l'entraÃ®nement
""")

# 3.1 Random Forest de base
print("\nğŸš€ EntraÃ®nement Random Forest (baseline)...\n")

rf_baseline = RandomForestClassifier(
    n_estimators=100,
    random_state=42,
    n_jobs=-1
)
rf_baseline.fit(X_train, y_train)

y_val_pred_rf_base = rf_baseline.predict(X_val)
y_val_proba_rf_base = rf_baseline.predict_proba(X_val)[:, 1]

# MÃ©triques
acc_rf_base = accuracy_score(y_val, y_val_pred_rf_base)
f1_rf_base = f1_score(y_val, y_val_pred_rf_base)
auc_rf_base = roc_auc_score(y_val, y_val_proba_rf_base)

print(f"Random Forest (baseline) :")
print(f"  Accuracy : {acc_rf_base:.4f}")
print(f"  F1-Score : {f1_rf_base:.4f}")
print(f"  ROC-AUC  : {auc_rf_base:.4f}")

# 3.2 Random Forest optimisÃ©
print("\nğŸš€ EntraÃ®nement Random Forest (optimisÃ©)...\n")

rf_optimized = RandomForestClassifier(
    n_estimators=300,
    max_depth=20,
    min_samples_split=15,
    min_samples_leaf=5,
    max_features='sqrt',
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)
rf_optimized.fit(X_train, y_train)

y_val_pred_rf_opt = rf_optimized.predict(X_val)
y_val_proba_rf_opt = rf_optimized.predict_proba(X_val)[:, 1]

acc_rf_opt = accuracy_score(y_val, y_val_pred_rf_opt)
f1_rf_opt = f1_score(y_val, y_val_pred_rf_opt)
auc_rf_opt = roc_auc_score(y_val, y_val_proba_rf_opt)

print(f"Random Forest (optimisÃ©) :")
print(f"  Accuracy : {acc_rf_opt:.4f}")
print(f"  F1-Score : {f1_rf_opt:.4f}")
print(f"  ROC-AUC  : {auc_rf_opt:.4f}")

print(f"""
ğŸ” OBSERVATION #1 : Impact de l'optimisation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Baseline â†’ OptimisÃ© :
  F1-Score : {f1_rf_base:.4f} â†’ {f1_rf_opt:.4f} (Î” = {f1_rf_opt - f1_rf_base:+.4f})
  ROC-AUC  : {auc_rf_base:.4f} â†’ {auc_rf_opt:.4f} (Î” = {auc_rf_opt - auc_rf_base:+.4f})

CE QU'IL FAUT OBSERVER :

1. AmÃ©lioration significative ? (Î” > 0.02)
   â†’ Oui : Tuning a portÃ© ses fruits
   â†’ Non : HyperparamÃ¨tres par dÃ©faut suffisants

2. AmÃ©lioration sur TOUTES les mÃ©triques ?
   â†’ Oui : Optimisation robuste
   â†’ Non : Trade-off possible (ex: F1 â†‘ mais Accuracy â†“)

ğŸ’¡ CONCLUSION :
   Random Forest est ROBUSTE : mÃªme baseline donne souvent bons rÃ©sultats.
   Optimisation apporte amÃ©lioration MODÃ‰RÃ‰E (pas spectaculaire).

   Si amÃ©lioration < 0.01 : Gain marginal, baseline suffit.
   Si amÃ©lioration > 0.05 : Optimisation cruciale !
""")

# 3.3 Diagnostic overfitting
print("\nğŸ” Diagnostic d'overfitting...\n")

y_train_pred_rf = rf_optimized.predict(X_train)
train_acc_rf = accuracy_score(y_train, y_train_pred_rf)
train_f1_rf = f1_score(y_train, y_train_pred_rf)

print(f"Train Accuracy : {train_acc_rf:.4f}")
print(f"Val Accuracy   : {acc_rf_opt:.4f}")
print(f"Ã‰cart          : {train_acc_rf - acc_rf_opt:.4f}")

print(f"""
ğŸ” OBSERVATION #2 : Overfitting de Random Forest
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Ã‰cart Train - Val : {train_acc_rf - acc_rf_opt:.4f}

CE QU'IL FAUT OBSERVER :

1. Ã‰cart < 0.05 : EXCELLENT (pas d'overfitting)
   â†’ RF est robuste, gÃ©nÃ©ralise bien

2. Ã‰cart 0.05-0.10 : ACCEPTABLE
   â†’ LÃ©ger overfitting, mais gÃ©rable

3. Ã‰cart > 0.10 : PROBLÃˆME
   â†’ Overfitting significatif
   â†’ Actions : Augmenter min_samples_split/leaf, rÃ©duire max_depth

ğŸ’¡ CONCLUSION ATTENDUE :
   Random Forest overfit RAREMENT grÃ¢ce au bagging.
   Ã‰cart devrait Ãªtre faible (~0.02-0.05).

   Si overfitting fort :
   1. RÃ©duire max_depth (20 â†’ 15)
   2. Augmenter min_samples_leaf (5 â†’ 10)
   3. Augmenter min_samples_split (15 â†’ 30)
""")

# 3.4 Feature Importance
print("\nğŸ“Š Feature Importance (Random Forest)...\n")

importance_rf = pd.DataFrame({
    'Feature': feature_names,
    'Importance': rf_optimized.feature_importances_
}).sort_values('Importance', ascending=False)

print(importance_rf.head(10))

plt.figure(figsize=(12, 8))
plt.barh(importance_rf['Feature'][:15], importance_rf['Importance'][:15], color='green', alpha=0.7)
plt.xlabel('Importance')
plt.title('Top 15 Features - Random Forest')
plt.gca().invert_yaxis()
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/ensemble_rf_importance.png', dpi=100)
plt.show()

print("\nâœ“ Graphique sauvegardÃ© : ensemble_rf_importance.png")

print(f"""
ğŸ” OBSERVATION #3 : Feature Importance (InterprÃ©tation)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CE QU'IL FAUT OBSERVER :

1. Features dominantes (Importance > 0.10) :
   Ces features sont CRITIQUES pour les prÃ©dictions.
   â†’ Prioriser leur qualitÃ© (collecte, nettoyage)

2. Features nÃ©gligeables (Importance < 0.01) :
   Ces features n'apportent RIEN.
   â†’ Candidats pour suppression (rÃ©duire dimensionnalitÃ©)

3. Distribution de l'importance :
   - Plate (toutes â‰ˆ Ã©gales) : Toutes features utiles
   - ConcentrÃ©e (top 3-5 dominent) : Peu de features vraiment importantes

ğŸ’¡ UTILITÃ‰ BUSINESS :
   Top features = Leviers d'action pour rÃ©duire churn

   Ex: Si "CustomerServiceCalls" est top 1 :
   â†’ AmÃ©liorer service client peut rÃ©duire churn significativement

âš ï¸  ATTENTION :
   Feature Importance â‰  CausalitÃ©
   CorrÃ©lation n'implique pas causalitÃ© !

   Ex: "TotalCharges" important peut signifier :
   - Clients chers partent plus â†’ RÃ©duire prix ?
   - Clients fidÃ¨les (charges Ã©levÃ©es) partent moins â†’ FidÃ©liser ?

   â†’ Analyse mÃ©tier nÃ©cessaire pour interprÃ©ter correctement
""")

input("\nâ–¶ Appuyez sur EntrÃ©e pour continuer vers XGBoost...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 4 : XGBOOST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 4 : XGBOOST - LE CHAMPION DE KAGGLE")
print("="*80)

if not XGBOOST_AVAILABLE:
    print("\nâš ï¸  XGBoost non disponible. Installation : pip install xgboost")
    print("Suite du tutoriel avec Random Forest uniquement.")
else:
    print("""
ğŸ† HYPERPARAMÃˆTRES CLÃ‰S DE XGBOOST
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PARAMÃˆTRES DE BASE :

1. n_estimators : Nombre d'arbres (boosting rounds)
   - Plus grand = Meilleur (jusqu'Ã  plateau)
   - DÃ©faut : 100, RecommandÃ© : 100-1000
   - Utiliser early_stopping pour trouver optimal

2. learning_rate (eta) : Taux d'apprentissage
   - ContrÃ´le contribution de chaque arbre
   - DÃ©faut : 0.3, RecommandÃ© : 0.01-0.1
   - Plus petit = Plus d'arbres nÃ©cessaires mais meilleure gÃ©nÃ©ralisation

3. max_depth : Profondeur max arbres
   - DÃ©faut : 6, RecommandÃ© : 3-10
   - Plus grand = Plus complexe = Risque overfitting

PARAMÃˆTRES DE RÃ‰GULARISATION :

4. subsample : Fraction d'Ã©chantillons par arbre
   - DÃ©faut : 1.0, RecommandÃ© : 0.7-0.9
   - Comme bagging dans RF

5. colsample_bytree : Fraction de features par arbre
   - DÃ©faut : 1.0, RecommandÃ© : 0.7-0.9
   - RÃ©duit overfitting

6. reg_alpha (L1) : RÃ©gularisation L1
   - DÃ©faut : 0, RecommandÃ© : 0-1
   - SÃ©lection de features

7. reg_lambda (L2) : RÃ©gularisation L2
   - DÃ©faut : 1, RecommandÃ© : 1-10
   - RÃ©duit overfitting

PARAMÃˆTRES POUR DÃ‰SÃ‰QUILIBRE :

8. scale_pos_weight : Poids classe minoritaire
   - DÃ©faut : 1, RecommandÃ© : ratio_majority/ratio_minority
   - Crucial pour classes dÃ©sÃ©quilibrÃ©es

9. eval_metric : MÃ©trique Ã  optimiser
   - 'logloss' : Log loss (dÃ©faut)
   - 'auc' : ROC-AUC
   - 'aucpr' : Precision-Recall AUC

ğŸ’¡ STRATÃ‰GIE D'OPTIMISATION :
   1. Fixer learning_rate petit (0.05)
   2. Trouver n_estimators optimal (early_stopping)
   3. Optimiser max_depth, subsample, colsample
   4. Ajouter rÃ©gularisation si overfitting
""")

    # 4.1 XGBoost baseline
    print("\nğŸš€ EntraÃ®nement XGBoost (baseline)...\n")

    xgb_baseline = XGBClassifier(
        n_estimators=100,
        learning_rate=0.1,
        random_state=42,
        use_label_encoder=False,
        eval_metric='logloss'
    )
    xgb_baseline.fit(X_train, y_train)

    y_val_pred_xgb_base = xgb_baseline.predict(X_val)
    y_val_proba_xgb_base = xgb_baseline.predict_proba(X_val)[:, 1]

    acc_xgb_base = accuracy_score(y_val, y_val_pred_xgb_base)
    f1_xgb_base = f1_score(y_val, y_val_pred_xgb_base)
    auc_xgb_base = roc_auc_score(y_val, y_val_proba_xgb_base)

    print(f"XGBoost (baseline) :")
    print(f"  Accuracy : {acc_xgb_base:.4f}")
    print(f"  F1-Score : {f1_xgb_base:.4f}")
    print(f"  ROC-AUC  : {auc_xgb_base:.4f}")

    # 4.2 XGBoost avec early stopping
    print("\nğŸš€ EntraÃ®nement XGBoost (avec early stopping)...\n")

    # scale_pos_weight pour dÃ©sÃ©quilibre
    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

    xgb_early_stop = XGBClassifier(
        n_estimators=1000,  # Grand nombre car early stopping
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=scale_pos_weight,
        random_state=42,
        use_label_encoder=False,
        eval_metric='logloss'
    )

    # early_stopping_rounds : ArrÃªte si pas d'amÃ©lioration pendant N rounds
    xgb_early_stop.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        early_stopping_rounds=50,
        verbose=False
    )

    y_val_pred_xgb_es = xgb_early_stop.predict(X_val)
    y_val_proba_xgb_es = xgb_early_stop.predict_proba(X_val)[:, 1]

    acc_xgb_es = accuracy_score(y_val, y_val_pred_xgb_es)
    f1_xgb_es = f1_score(y_val, y_val_pred_xgb_es)
    auc_xgb_es = roc_auc_score(y_val, y_val_proba_xgb_es)

    print(f"XGBoost (early stopping) :")
    print(f"  Best iteration : {xgb_early_stop.best_iteration}")
    print(f"  Accuracy : {acc_xgb_es:.4f}")
    print(f"  F1-Score : {f1_xgb_es:.4f}")
    print(f"  ROC-AUC  : {auc_xgb_es:.4f}")

    print(f"""
ğŸ” OBSERVATION #4 : Early Stopping
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Best iteration : {xgb_early_stop.best_iteration} / 1000

CE QU'IL FAUT OBSERVER :

1. Best iteration << n_estimators (ex: 150 / 1000)
   â†’ Convergence RAPIDE
   â†’ Peut augmenter learning_rate ou rÃ©duire n_estimators

2. Best iteration â‰ˆ n_estimators (ex: 950 / 1000)
   â†’ Convergence LENTE ou pas atteinte
   â†’ RÃ©duire learning_rate ou augmenter n_estimators

3. Best iteration modÃ©rÃ© (ex: 300-500 / 1000)
   â†’ OPTIMAL : Ã©quilibre trouvÃ©

ğŸ’¡ CONCLUSION :
   Early stopping est ESSENTIEL pour XGBoost.
   Ã‰vite overfitting ET trouve automatiquement nombre optimal d'arbres.

   Sans early stopping : Risque Ã©levÃ© d'overfitting aprÃ¨s N iterations.

âš™ï¸  AJUSTEMENTS POSSIBLES :
   - Si best_iteration < 100 : Augmenter learning_rate (0.05 â†’ 0.1)
   - Si best_iteration > 800 : Diminuer learning_rate (0.05 â†’ 0.03)
   - early_stopping_rounds = 50 : Standard, peut ajuster (20-100)
""")

    # 4.3 XGBoost optimisÃ© (Grid Search)
    print("\nğŸ”§ Optimisation XGBoost (Grid Search)...\n")
    print("â³ Cela peut prendre quelques minutes...")

    param_grid = {
        'max_depth': [4, 6, 8],
        'learning_rate': [0.03, 0.05, 0.1],
        'subsample': [0.7, 0.8, 0.9],
        'colsample_bytree': [0.7, 0.8, 0.9]
    }

    xgb_grid = XGBClassifier(
        n_estimators=500,
        scale_pos_weight=scale_pos_weight,
        random_state=42,
        use_label_encoder=False,
        eval_metric='logloss',
        early_stopping_rounds=50
    )

    grid_search = GridSearchCV(
        xgb_grid,
        param_grid,
        cv=3,
        scoring='f1',
        n_jobs=-1,
        verbose=0
    )

    grid_search.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        verbose=False
    )

    print(f"\nâœ“ Optimisation terminÃ©e")
    print(f"Meilleurs paramÃ¨tres : {grid_search.best_params_}")
    print(f"Meilleur F1-Score (CV) : {grid_search.best_score_:.4f}")

    xgb_optimized = grid_search.best_estimator_

    y_val_pred_xgb_opt = xgb_optimized.predict(X_val)
    y_val_proba_xgb_opt = xgb_optimized.predict_proba(X_val)[:, 1]

    acc_xgb_opt = accuracy_score(y_val, y_val_pred_xgb_opt)
    f1_xgb_opt = f1_score(y_val, y_val_pred_xgb_opt)
    auc_xgb_opt = roc_auc_score(y_val, y_val_proba_xgb_opt)

    print(f"\nXGBoost (optimisÃ©) :")
    print(f"  Accuracy : {acc_xgb_opt:.4f}")
    print(f"  F1-Score : {f1_xgb_opt:.4f}")
    print(f"  ROC-AUC  : {auc_xgb_opt:.4f}")

    print(f"""
ğŸ” OBSERVATION #5 : Impact du Grid Search
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Early Stop â†’ Grid Search :
  F1-Score : {f1_xgb_es:.4f} â†’ {f1_xgb_opt:.4f} (Î” = {f1_xgb_opt - f1_xgb_es:+.4f})

CE QU'IL FAUT OBSERVER :

1. AmÃ©lioration > 0.02 : Grid Search UTILE
   â†’ Continuer avec Random Search ou Bayesian Optimization

2. AmÃ©lioration < 0.01 : Gain MARGINAL
   â†’ Early stopping suffisait
   â†’ Ã‰conomiser temps : utiliser hyperparamÃ¨tres par dÃ©faut

3. DÃ©gradation (Î” < 0) : Sur-optimisation sur CV
   â†’ Risque d'overfitting
   â†’ VÃ©rifier sur test set

ğŸ’¡ CONCLUSION :
   Grid Search amÃ©liore XGBoost MODÃ‰RÃ‰MENT (gain ~1-3%).
   Sur grandes donnÃ©es : PrivilÃ©gier Random Search (plus rapide).

ğŸ¯ TRADE-OFF Temps vs Performance :
   - Baseline : 10 secondes, F1 = {f1_xgb_base:.4f}
   - Early Stop : 30 secondes, F1 = {f1_xgb_es:.4f}
   - Grid Search : 5-10 min, F1 = {f1_xgb_opt:.4f}

   â†’ Early stopping = Meilleur compromis temps/performance !
""")

    # 4.4 Feature Importance XGBoost
    print("\nğŸ“Š Feature Importance (XGBoost)...\n")

    importance_xgb = pd.DataFrame({
        'Feature': feature_names,
        'Importance': xgb_optimized.feature_importances_
    }).sort_values('Importance', ascending=False)

    print(importance_xgb.head(10))

    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    # XGBoost
    axes[0].barh(importance_xgb['Feature'][:15], importance_xgb['Importance'][:15],
                 color='orange', alpha=0.7)
    axes[0].set_xlabel('Importance')
    axes[0].set_title('Feature Importance - XGBoost')
    axes[0].invert_yaxis()
    axes[0].grid(True, alpha=0.3, axis='x')

    # Random Forest (comparaison)
    axes[1].barh(importance_rf['Feature'][:15], importance_rf['Importance'][:15],
                 color='green', alpha=0.7)
    axes[1].set_xlabel('Importance')
    axes[1].set_title('Feature Importance - Random Forest')
    axes[1].invert_yaxis()
    axes[1].grid(True, alpha=0.3, axis='x')

    plt.tight_layout()
    plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/ensemble_importance_comparison.png', dpi=100)
    plt.show()

    print("\nâœ“ Graphique sauvegardÃ© : ensemble_importance_comparison.png")

    print(f"""
ğŸ” OBSERVATION #6 : RF vs XGBoost Feature Importance
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DIFFÃ‰RENCES POSSIBLES :

1. XGBoost : Gain moyen de la feature dans les splits
   â†’ Mesure contribution directe Ã  la rÃ©duction de loss

2. Random Forest : RÃ©duction moyenne d'impuretÃ©
   â†’ Mesure contribution Ã  la "puretÃ©" des splits

CE QU'IL FAUT OBSERVER :

â€¢ Rankings similaires ?
  â†’ Accord entre modÃ¨les â†’ Features vraiment importantes
  â†’ Confiance Ã©levÃ©e dans les insights

â€¢ Rankings divergents ?
  â†’ ModÃ¨les capturent aspects diffÃ©rents
  â†’ RF : Interactions locales
  â†’ XGBoost : Corrections sÃ©quentielles

  â†’ Analyser les deux pour comprÃ©hension complÃ¨te

ğŸ’¡ UTILITÃ‰ :
   Features importantes dans LES DEUX modÃ¨les :
   â†’ Leviers d'action les plus fiables

   Features importantes seulement dans un modÃ¨le :
   â†’ Interactions complexes possibles
   â†’ Investiguer pourquoi cette feature est importante ici mais pas lÃ 

ğŸ“Š EXEMPLE D'INTERPRÃ‰TATION :
   Si "CustomerServiceCalls" top 1 dans les deux :
   â†’ Lever d'action CRITIQUE pour rÃ©duire churn
   â†’ Investir dans amÃ©lioration service client

   Si "Tenure" top dans RF mais pas XGBoost :
   â†’ RF capte mieux effet non-linÃ©aire de l'anciennetÃ©
   â†’ XGBoost compense via autres features
""")

    input("\nâ–¶ Appuyez sur EntrÃ©e pour voir la comparaison finale...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 5 : COMPARAISON FINALE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 5 : COMPARAISON FINALE - QUEL MODÃˆLE CHOISIR ?")
print("="*80)

# Tableau rÃ©capitulatif
if XGBOOST_AVAILABLE:
    results = pd.DataFrame({
        'ModÃ¨le': [
            'RF Baseline',
            'RF OptimisÃ©',
            'XGBoost Baseline',
            'XGBoost Early Stop',
            'XGBoost OptimisÃ©'
        ],
        'Accuracy': [acc_rf_base, acc_rf_opt, acc_xgb_base, acc_xgb_es, acc_xgb_opt],
        'F1-Score': [f1_rf_base, f1_rf_opt, f1_xgb_base, f1_xgb_es, f1_xgb_opt],
        'ROC-AUC': [auc_rf_base, auc_rf_opt, auc_xgb_base, auc_xgb_es, auc_xgb_opt]
    })
else:
    results = pd.DataFrame({
        'ModÃ¨le': ['RF Baseline', 'RF OptimisÃ©'],
        'Accuracy': [acc_rf_base, acc_rf_opt],
        'F1-Score': [f1_rf_base, f1_rf_opt],
        'ROC-AUC': [auc_rf_base, auc_rf_opt]
    })

print("\nğŸ“Š TABLEAU RÃ‰CAPITULATIF\n")
print(results.to_string(index=False))

# Visualisation
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

for idx, metric in enumerate(['Accuracy', 'F1-Score', 'ROC-AUC']):
    axes[idx].bar(results['ModÃ¨le'], results[metric], alpha=0.7, edgecolor='black')
    axes[idx].set_ylabel(metric)
    axes[idx].set_title(f'Comparaison : {metric}')
    axes[idx].tick_params(axis='x', rotation=45)
    axes[idx].grid(True, alpha=0.3, axis='y')
    axes[idx].set_ylim(results[metric].min() - 0.05, results[metric].max() + 0.05)

plt.tight_layout()
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/ensemble_comparison.png', dpi=100)
plt.show()

print("\nâœ“ Graphique sauvegardÃ© : ensemble_comparison.png")

# Meilleur modÃ¨le
best_model_name = results.loc[results['F1-Score'].idxmax(), 'ModÃ¨le']
best_f1 = results['F1-Score'].max()
best_auc = results.loc[results['F1-Score'].idxmax(), 'ROC-AUC']

if XGBOOST_AVAILABLE:
    best_model = xgb_optimized if 'XGBoost' in best_model_name else rf_optimized
else:
    best_model = rf_optimized

print(f"""
ğŸ” OBSERVATION FINALE : Choix du ModÃ¨le
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CE QU'IL FAUT OBSERVER :

1. PERFORMANCE :
   Meilleur F1-Score : {best_model_name} = {best_f1:.4f}

2. DIFFÃ‰RENCES ENTRE MODÃˆLES :
   - Si Î” < 0.01 : Performance Ã‰QUIVALENTE
     â†’ Choisir le plus SIMPLE (RF baseline)

   - Si Î” 0.01-0.03 : AmÃ©lioration MODÃ‰RÃ‰E
     â†’ Trade-off complexitÃ© vs gain

   - Si Î” > 0.03 : AmÃ©lioration SIGNIFICATIVE
     â†’ Choisir le meilleur

3. CONSIDÃ‰RATIONS PRATIQUES :
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ CritÃ¨re          â”‚ RF         â”‚ XGBoost     â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ EntraÃ®nement     â”‚ Rapide     â”‚ Plus lent   â”‚
   â”‚ InfÃ©rence        â”‚ Lent       â”‚ Rapide      â”‚
   â”‚ Tuning           â”‚ Facile     â”‚ Complexe    â”‚
   â”‚ InterprÃ©tation   â”‚ Facile     â”‚ Moyenne     â”‚
   â”‚ Robustesse       â”‚ Excellente â”‚ Bonne       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

if XGBOOST_AVAILABLE:
    print(f"""
ğŸ¯ RECOMMANDATION POUR CHURN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Performance : {best_model_name} gagne avec F1 = {best_f1:.4f}

ğŸ’¼ SCÃ‰NARIOS DE DÃ‰CISION :

SCÃ‰NARIO A : Startup / Prototype / Peu de donnÃ©es
   â†’ CHOISIR : Random Forest
   Raisons :
   âœ… Setup rapide
   âœ… Peu de tuning nÃ©cessaire
   âœ… Robuste out-of-the-box
   âœ… Facile Ã  maintenir

SCÃ‰NARIO B : Production / Grande Ã©chelle / Performance critique
   â†’ CHOISIR : XGBoost
   Raisons :
   âœ… Meilleure performance
   âœ… InfÃ©rence plus rapide (important avec millions de clients)
   âœ… GÃ¨re mieux dÃ©sÃ©quilibre (scale_pos_weight)
   âœ… RÃ©gularisation avancÃ©e

SCÃ‰NARIO C : TrÃ¨s grandes donnÃ©es (>1M lignes)
   â†’ CHOISIR : LightGBM
   Raisons :
   âœ… Plus rapide que XGBoost
   âœ… Moins de mÃ©moire
   âœ… Performances similaires

ğŸ† CHOIX FINAL : {best_model_name}
   Pour churn tÃ©lÃ©coms : XGBoost gÃ©nÃ©ralement optimal
   (performance + robustesse + scale_pos_weight)
""")
else:
    print(f"""
ğŸ¯ RECOMMANDATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Performance : {best_model_name} avec F1 = {best_f1:.4f}

ğŸ’¼ UTILISATION EN PRODUCTION :
   Random Forest est EXCELLENT pour :
   âœ… Baseline rapide et robuste
   âœ… Applications oÃ¹ vitesse d'entraÃ®nement critique
   âœ… Cas oÃ¹ interprÃ©tabilitÃ© importante

   Pour ENCORE MEILLEURES performances :
   â†’ Installer XGBoost : pip install xgboost
   â†’ Gain attendu : +2-5% sur mÃ©triques
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 6 : Ã‰VALUATION FINALE SUR TEST SET
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 6 : Ã‰VALUATION FINALE SUR TEST SET")
print("="*80)

y_test_pred = best_model.predict(X_test)
y_test_proba = best_model.predict_proba(X_test)[:, 1]

test_acc = accuracy_score(y_test, y_test_pred)
test_prec = precision_score(y_test, y_test_pred)
test_rec = recall_score(y_test, y_test_pred)
test_f1 = f1_score(y_test, y_test_pred)
test_auc = roc_auc_score(y_test, y_test_proba)

print(f"\nğŸ† ModÃ¨le sÃ©lectionnÃ© : {best_model_name}\n")
print(f"ğŸ“Š PERFORMANCE SUR TEST SET")
print("-" * 60)
print(f"Accuracy  : {test_acc:.4f}")
print(f"Precision : {test_prec:.4f}")
print(f"Recall    : {test_rec:.4f}")
print(f"F1-Score  : {test_f1:.4f}")
print(f"ROC-AUC   : {test_auc:.4f}")

print("\nğŸ“‹ Classification Report :")
print(classification_report(y_test, y_test_pred, target_names=['Retention', 'Churn']))

# Matrice de confusion
cm_test = confusion_matrix(y_test, y_test_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('PrÃ©diction')
plt.ylabel('RÃ©alitÃ©')
plt.title(f'Matrice de Confusion - Test Set\n{best_model_name}')
plt.tight_layout()
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/ensemble_confusion_final.png', dpi=100)
plt.show()

print("\nâœ“ Graphique sauvegardÃ© : ensemble_confusion_final.png")

print(f"""
ğŸ” OBSERVATION #7 : Performance Test vs Val
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Val F1  : {best_f1:.4f}
Test F1 : {test_f1:.4f}
Ã‰cart   : {abs(test_f1 - best_f1):.4f}

CE QU'IL FAUT OBSERVER :

1. Test â‰ˆ Val (Ã©cart < 0.02) : EXCELLENT
   â†’ ModÃ¨le GÃ‰NÃ‰RALISE bien
   â†’ PrÃªt pour production

2. Test < Val (Ã©cart 0.02-0.05) : ACCEPTABLE
   â†’ LÃ©gÃ¨re dÃ©gradation normale
   â†’ Probablement dÃ» Ã  sur-optimisation sur val

3. Test << Val (Ã©cart > 0.05) : PROBLÃˆME
   â†’ Overfitting sur val
   â†’ Revoir stratÃ©gie validation (plus de folds, donnÃ©es)

4. Test > Val : CHANCEUX
   â†’ Val set Ã©tait plus difficile par hasard
   â†’ Ou test set non reprÃ©sentatif

ğŸ’¡ CONCLUSION :
   {"ModÃ¨le STABLE et fiable" if abs(test_f1 - best_f1) < 0.03 else "VÃ©rifier overfitting"}
   {"â†’ PrÃªt pour production" if abs(test_f1 - best_f1) < 0.03 else "â†’ Revoir validation"}

ğŸ¯ INTERPRÃ‰TATION BUSINESS :
   Recall = {test_rec:.2%} â†’ DÃ©tecte {test_rec:.0%} des churners
   Precision = {test_prec:.2%} â†’ {test_prec:.0%} des alertes sont correctes

   Si action de rÃ©tention coÃ»te 50â‚¬ :
   - {int(cm_test[1, 1])} churners dÃ©tectÃ©s = {int(cm_test[1, 1]) * 50}â‚¬ investi
   - Si rÃ©tention rÃ©ussie (50% taux) = {int(cm_test[1, 1] * 0.5)} clients sauvÃ©s
   - Si valeur client = 500â‚¬ â†’ ROI = {int(cm_test[1, 1] * 0.5 * 500 - cm_test[1, 1] * 50)}â‚¬
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 7 : SAUVEGARDE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 7 : SAUVEGARDE DU MODÃˆLE")
print("="*80)

import joblib

joblib.dump(best_model, 'e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/best_ensemble_model.pkl')

print("\nâœ“ ModÃ¨le sauvegardÃ© : best_ensemble_model.pkl")

print("""
ğŸ“¦ UTILISATION EN PRODUCTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```python
import joblib
import numpy as np

# Charger
model = joblib.load('best_ensemble_model.pkl')

# Nouveau client
nouveau_client = np.array([[
    24,      # Tenure (mois)
    89.99,   # MonthlyCharges
    2159.76, # TotalCharges
    # ... autres features
]])

# PrÃ©dire
prediction = model.predict(nouveau_client)[0]
proba_churn = model.predict_proba(nouveau_client)[0, 1]

print(f"Risque de churn : {proba_churn:.2%}")

# DÃ©cision business
if proba_churn > 0.6:  # Seuil ajustable
    print("ğŸš¨ ALERTE : Client Ã  haut risque")
    print("Action : Offre de rÃ©tention personnalisÃ©e")
elif proba_churn > 0.3:
    print("âš ï¸  ATTENTION : Client Ã  risque modÃ©rÃ©")
    print("Action : Suivi proactif")
else:
    print("âœ… OK : Client fidÃ¨le")
```

ğŸ¯ AJUSTER LE SEUIL selon coÃ»ts/bÃ©nÃ©fices :
   - Seuil bas (0.3) : Plus d'alertes, dÃ©tecte plus de churners
   - Seuil haut (0.7) : Moins d'alertes, uniquement trÃ¨s haut risque

   â†’ Optimiser selon coÃ»t action vs valeur client
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RÃ‰SUMÃ‰ ET CONCLUSIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("ğŸ‰ RÃ‰SUMÃ‰ ET CONCLUSIONS")
print("="*80)

print("""
ğŸ“š CE QUE NOUS AVONS APPRIS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1ï¸âƒ£  RANDOM FOREST
   âœ… Excellent baseline (robuste, peu de tuning)
   âœ… ParallÃ©lisable (rapide sur multi-core)
   âœ… Rarement overfit grÃ¢ce au bagging
   âŒ Moins performant que boosting sur donnÃ©es propres

   ğŸ’¼ Usage : Prototype, baseline, peu de temps

2ï¸âƒ£  XGBOOST
   âœ… Performance MAXIMALE sur tabulaire
   âœ… RÃ©gularisation avancÃ©e
   âœ… Early stopping essentiel
   âœ… GÃ¨re dÃ©sÃ©quilibre (scale_pos_weight)
   âŒ Tuning complexe (nombreux hyperparamÃ¨tres)

   ğŸ’¼ Usage : Production, Kaggle, performance critique

3ï¸âƒ£  LIGHTGBM (si installÃ©)
   âœ… Plus rapide que XGBoost
   âœ… Moins de mÃ©moire
   âœ… IdÃ©al grandes donnÃ©es (>100k)
   âŒ Peut overfitter sur petites donnÃ©es

   ğŸ’¼ Usage : Big Data, vitesse critique

4ï¸âƒ£  OPTIMISATION
   ğŸ”§ Random Forest : Baseline suffit souvent
   ğŸ”§ XGBoost : Early stopping >> Grid Search (rapport temps/gain)
   ğŸ”§ Grid Search utile si temps disponible (gain 1-3%)

5ï¸âƒ£  FEATURE IMPORTANCE
   ğŸ“Š Accords RF â†” XGBoost â†’ Features fiables
   ğŸ“Š Divergences â†’ Investiguer (interactions ?)
   âš ï¸  Importance â‰  CausalitÃ© (attention interprÃ©tation)

6ï¸âƒ£  DIAGNOSTIC
   ğŸ” Train vs Val : DÃ©tecter overfitting
   ğŸ” Val vs Test : VÃ©rifier gÃ©nÃ©ralisation
   ğŸ” Learning curves : Comprendre convergence

âœ… CHECKLIST MODÃˆLES ENSEMBLE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ“ Random Forest baseline Ã©tabli
âœ“ XGBoost avec early stopping testÃ©
âœ“ HyperparamÃ¨tres optimisÃ©s (si temps)
âœ“ Feature importance analysÃ©e
âœ“ Overfitting vÃ©rifiÃ© (train vs val)
âœ“ GÃ©nÃ©ralisation validÃ©e (val vs test)
âœ“ MÃ©triques business interprÃ©tÃ©es
âœ“ ModÃ¨le sauvegardÃ©

ğŸ¯ RÃˆGLE D'OR
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"Pour donnÃ©es tabulaires :
 1. Baseline : Random Forest
 2. Production : XGBoost avec early stopping
 3. Big Data : LightGBM"

ğŸš€ PROCHAINES Ã‰TAPES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Feature engineering avancÃ©
2. Ensemble de modÃ¨les (stacking)
3. Calibration des probabilitÃ©s
4. InterprÃ©tabilitÃ© (SHAP values)
5. A/B testing en production
6. Monitoring et retraining automatique

ğŸ’¡ BONUS : QUAND NE PAS UTILISER RF/XGBoost ?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âŒ Images â†’ Utiliser CNN
âŒ Texte â†’ Utiliser Transformers (BERT)
âŒ SÃ©ries temporelles â†’ Utiliser LSTM ou Prophet
âŒ InterprÃ©tabilitÃ© stricte requise â†’ Logistic Reg ou Decision Tree unique
âŒ Temps rÃ©el critique (<1ms) â†’ Linear models
""")

print("="*80)
print("âœ¨ TUTORIEL TERMINÃ‰ AVEC SUCCÃˆS ! âœ¨")
print("="*80)
print("\nğŸ† Vous maÃ®trisez maintenant Random Forest et XGBoost !")
print("ğŸ“š Prochain tutoriel : Neural Networks et Deep Learning")
