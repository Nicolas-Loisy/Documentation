"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TUTORIEL COMPLET : CNN (CONVOLUTIONAL NEURAL NETWORKS)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ CAS D'USAGE RÃ‰EL : Classification d'Images (MNIST et CIFAR-10)

CONTEXTE :
SystÃ¨me de reconnaissance automatique d'images pour :
- Reconnaissance de chiffres manuscrits (MNIST)
- Classification d'objets (CIFAR-10)

POURQUOI CNN ?
- STANDARD pour traitement d'images
- DÃ©tecte patterns locaux (edges, textures, objets)
- Invariance aux translations
- Moins de paramÃ¨tres que Dense Networks

Ce tutoriel couvre :
1. POURQUOI CNN vs Dense Networks pour images
2. Architecture CNN : Convolution, Pooling, Dense
3. Fonctionnement de la Convolution (expliquÃ© simplement)
4. Construire un CNN from scratch
5. Architectures cÃ©lÃ¨bres (LeNet, AlexNet, VGG, ResNet)
6. Data Augmentation (crucial pour images)
7. Transfer Learning (utiliser modÃ¨les prÃ©-entraÃ®nÃ©s)
8. Visualisation des features apprises
9. Optimisation et diagnostic

Chaque Ã©tape explique CE QU'IL FAUT OBSERVER et LES CONCLUSIONS.
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# TensorFlow / Keras
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, callbacks, applications
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.datasets import mnist, cifar10

from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# Configuration
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)
tf.random.set_seed(42)
np.random.seed(42)

print("="*80)
print("TUTORIEL : CNN (CONVOLUTIONAL NEURAL NETWORKS)")
print("="*80)
print(f"TensorFlow version : {tf.__version__}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 1 : COMPRENDRE LES CNN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 1 : POURQUOI CNN POUR LES IMAGES ?")
print("="*80)

print("""
ğŸ–¼ï¸  LE PROBLÃˆME DES DENSE NETWORKS SUR IMAGES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

EXEMPLE : Image 28Ã—28 pixels (MNIST)
   Dense Network : 28 Ã— 28 = 784 inputs

   Si 1Ã¨re hidden layer = 128 neurons :
   â†’ ParamÃ¨tres = 784 Ã— 128 = 100,352 !

   Pour image 224Ã—224 RGB (ImageNet) :
   â†’ Inputs = 224 Ã— 224 Ã— 3 = 150,528
   â†’ Avec 512 neurons : 150,528 Ã— 512 = 77M paramÃ¨tres !

âŒ PROBLÃˆMES :
   1. Trop de paramÃ¨tres â†’ Overfitting
   2. Pas de notion de LOCALITÃ‰ (pixels voisins reliÃ©s)
   3. Pas d'INVARIANCE (mÃªme objet dÃ©placÃ© = input diffÃ©rent)
   4. Pas de HIÃ‰RARCHIE (low-level â†’ high-level features)


âœ… SOLUTION : CONVOLUTIONAL NEURAL NETWORKS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PRINCIPE : DÃ©tecter patterns LOCAUX via filtres partagÃ©s

1. CONVOLUTION : Filtres glissants dÃ©tectent patterns
   â†’ Edges, textures, formes, objets

2. POOLING : RÃ©duction dimensionnelle
   â†’ Robustesse aux translations

3. HIÃ‰RARCHIE : Layers progressives
   â†’ Layer 1 : Edges (lignes, courbes)
   â†’ Layer 2 : Textures (grilles, motifs)
   â†’ Layer 3 : Parties (yeux, roues)
   â†’ Layer 4 : Objets complets (chat, voiture)


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š CNN vs DENSE NETWORKS : COMPARAISON
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CritÃ¨re         â”‚  Dense NN      â”‚  CNN                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ParamÃ¨tres       â”‚ Ã‰NORME         â”‚ RÃ©duit (poids partagÃ©s)â”‚
â”‚ Images           â”‚ â˜…â˜†â˜†â˜†â˜†          â”‚ â˜…â˜…â˜…â˜…â˜…                  â”‚
â”‚ LocalitÃ©         â”‚ âŒ IgnorÃ©e     â”‚ âœ… CapturÃ©e            â”‚
â”‚ Invariance       â”‚ âŒ Aucune      â”‚ âœ… Translation         â”‚
â”‚ HiÃ©rarchie       â”‚ âŒ Plate       â”‚ âœ… HiÃ©rarchique        â”‚
â”‚ Tabulaire        â”‚ â˜…â˜…â˜…â˜…â˜†          â”‚ â˜…â˜†â˜†â˜†â˜†                  â”‚
â”‚ Performance      â”‚ â˜…â˜…â˜†â˜†â˜†          â”‚ â˜…â˜…â˜…â˜…â˜…                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


ğŸ—ï¸  ARCHITECTURE CNN TYPIQUE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Input Image (28Ã—28Ã—1)
   â†“
[Conv 3Ã—3, 32 filtres] â†’ Feature Maps (26Ã—26Ã—32)
   â†“
[ReLU Activation]
   â†“
[MaxPooling 2Ã—2] â†’ (13Ã—13Ã—32)
   â†“
[Conv 3Ã—3, 64 filtres] â†’ (11Ã—11Ã—64)
   â†“
[ReLU]
   â†“
[MaxPooling 2Ã—2] â†’ (5Ã—5Ã—64)
   â†“
[Flatten] â†’ Vector (1600)
   â†“
[Dense 128]
   â†“
[Dense 10] â†’ Classes


ğŸ¯ QUAND UTILISER CNN ?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ… UTILISER CNN QUAND :
   - DonnÃ©es = IMAGES
   - Patterns locaux importants
   - Invariance spatiale nÃ©cessaire
   - Classification, dÃ©tection, segmentation

âŒ NE PAS UTILISER CNN QUAND :
   - DonnÃ©es tabulaires (colonnes indÃ©pendantes)
   - Pas de structure spatiale
   - Ordre des features arbitraire

ğŸ’¼ APPLICATIONS :
   - Classification d'images
   - DÃ©tection d'objets (YOLO, R-CNN)
   - Segmentation sÃ©mantique
   - Reconnaissance faciale
   - MÃ©dical (radiographies, IRM)
   - VÃ©hicules autonomes
""")

input("\nâ–¶ Appuyez sur EntrÃ©e pour continuer...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 2 : COMPRENDRE LA CONVOLUTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 2 : FONCTIONNEMENT DE LA CONVOLUTION")
print("="*80)

print("""
ğŸ” QU'EST-CE QU'UNE CONVOLUTION ?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PRINCIPE : Filtre (kernel) glisse sur l'image et calcule produit scalaire

EXEMPLE SIMPLE : DÃ©tection de bord vertical

Image 5Ã—5 :            Filtre 3Ã—3 :          RÃ©sultat :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0 0 1 1 0 â”‚        â”‚ -1  0  1â”‚         Output = Î£(Image Ã— Filtre)
â”‚ 0 0 1 1 0 â”‚    Ã—   â”‚ -1  0  1â”‚    â†’   Valeur Ã©levÃ©e si edge dÃ©tectÃ©
â”‚ 0 0 1 1 0 â”‚        â”‚ -1  0  1â”‚
â”‚ 0 0 1 1 0 â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ 0 0 1 1 0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


PARAMÃˆTRES CLÃ‰S :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. TAILLE DU FILTRE (kernel_size)
   - 1Ã—1 : Mixe channels, pas de spatial
   - 3Ã—3 : STANDARD (bon compromis)
   - 5Ã—5 : Plus large, coÃ»teux
   - 7Ã—7 : TrÃ¨s large, rare

   ğŸ’¡ Recommandation : 3Ã—3 (99% des cas)


2. NOMBRE DE FILTRES (filters)
   - Plus de filtres = Plus de patterns dÃ©tectÃ©s
   - Layer 1 : 32-64 filtres
   - Layer 2 : 64-128 filtres
   - Layer 3+ : 128-512 filtres

   ğŸ’¡ Double gÃ©nÃ©ralement Ã  chaque layer


3. STRIDE (pas de glissement)
   - stride=1 : Glisse d'1 pixel (dÃ©faut)
   - stride=2 : Glisse de 2 pixels â†’ RÃ©duit taille 2Ã—

   ğŸ’¡ stride=1 presque toujours, pooling pour rÃ©duire


4. PADDING (remplissage)
   - valid : Pas de padding â†’ Output plus petit
   - same : Padding â†’ Output mÃªme taille que input

   ğŸ’¡ 'same' permet de contrÃ´ler taille output


CALCUL DE LA TAILLE OUTPUT :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Output size = (Input - Kernel + 2Ã—Padding) / Stride + 1

Exemple : Input 28Ã—28, Kernel 3Ã—3, Padding 0, Stride 1
   Output = (28 - 3 + 0) / 1 + 1 = 26Ã—26

Avec padding='same' et stride=1 :
   Output = Input size (28Ã—28)


ğŸŒŠ POOLING (RÃ‰DUCTION)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PRINCIPE : RÃ©duit dimensionnalitÃ© en prenant max ou moyenne

MaxPooling 2Ã—2 :
   â”Œâ”€â”€â”€â”€â”€â”
   â”‚ 1 3 â”‚ â†’ max = 6
   â”‚ 2 6 â”‚
   â””â”€â”€â”€â”€â”€â”˜

AVANTAGES :
âœ… RÃ©duit calculs (2Ã— moins de pixels)
âœ… Invariance aux petites translations
âœ… Augmente champ rÃ©ceptif

TYPES :
- MaxPooling : Prend maximum (STANDARD)
- AveragePooling : Prend moyenne (rare)

ğŸ’¡ Recommandation : MaxPooling 2Ã—2


ğŸ“Š NOMBRE DE PARAMÃˆTRES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Conv2D(32 filtres, 3Ã—3, input_channels=1) :
   ParamÃ¨tres = (3 Ã— 3 Ã— 1 + 1) Ã— 32 = 320

   Formule : (kernel_h Ã— kernel_w Ã— input_channels + 1) Ã— nb_filtres

Dense(128, input=784) :
   ParamÃ¨tres = (784 + 1) Ã— 128 = 100,480

â†’ Conv a 300Ã— MOINS de paramÃ¨tres !
""")

# DÃ©monstration visuelle de la convolution
print("\nğŸ“Š DÃ©monstration : DÃ©tection de bords...\n")

# CrÃ©er image simple avec un bord
test_image = np.zeros((10, 10))
test_image[:, 5:] = 1  # Bord vertical au milieu

# Filtre de dÃ©tection de bord vertical
edge_filter = np.array([
    [-1, 0, 1],
    [-1, 0, 1],
    [-1, 0, 1]
])

# Appliquer convolution manuellement (simplifiÃ©)
from scipy.signal import correlate2d
filtered_image = correlate2d(test_image, edge_filter, mode='valid')

fig, axes = plt.subplots(1, 3, figsize=(15, 4))

axes[0].imshow(test_image, cmap='gray')
axes[0].set_title('Image Originale\n(Bord vertical au centre)')
axes[0].axis('off')

axes[1].imshow(edge_filter, cmap='gray', vmin=-1, vmax=1)
axes[1].set_title('Filtre de DÃ©tection\nde Bord Vertical')
axes[1].axis('off')

axes[2].imshow(filtered_image, cmap='gray')
axes[2].set_title('RÃ©sultat de la Convolution\n(Bord dÃ©tectÃ© = valeurs Ã©levÃ©es)')
axes[2].axis('off')

plt.tight_layout()
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/cnn_convolution_demo.png', dpi=100)
plt.show()

print("âœ“ Graphique sauvegardÃ© : cnn_convolution_demo.png")

print("""
ğŸ” OBSERVATION #1 : Convolution en Action
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CE QU'IL FAUT OBSERVER :

1. IMAGE ORIGINALE :
   Bord net entre noir (0) et blanc (1)

2. FILTRE :
   Valeurs nÃ©gatives Ã  gauche, positives Ã  droite
   â†’ DÃ©tecte transitions sombre â†’ clair

3. RÃ‰SULTAT :
   Valeurs Ã©levÃ©es AU NIVEAU DU BORD
   Valeurs faibles ailleurs

ğŸ’¡ CONCLUSION :
   Le filtre a APPRIS Ã  dÃ©tecter ce pattern spÃ©cifique.
   CNN apprend automatiquement ces filtres pendant entraÃ®nement !

   Layer 1 : Apprend bords simples
   Layer 2 : Combine bords â†’ Textures
   Layer 3 : Combine textures â†’ Formes
   Layer 4 : Combine formes â†’ Objets
""")

input("\nâ–¶ Appuyez sur EntrÃ©e pour continuer...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 3 : DONNÃ‰ES - MNIST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 3 : CHARGEMENT DES DONNÃ‰ES - MNIST")
print("="*80)

print("\nğŸ“Š Chargement du dataset MNIST...\n")

# Charger MNIST
(X_train, y_train), (X_test, y_test) = mnist.load_data()

print(f"âœ“ DonnÃ©es chargÃ©es")
print(f"  Train : {X_train.shape[0]} images")
print(f"  Test : {X_test.shape[0]} images")
print(f"  Shape : {X_train.shape[1:]} (28Ã—28 pixels, grayscale)")

# Visualiser quelques exemples
fig, axes = plt.subplots(2, 5, figsize=(12, 5))
for i, ax in enumerate(axes.flat):
    ax.imshow(X_train[i], cmap='gray')
    ax.set_title(f'Label: {y_train[i]}')
    ax.axis('off')
plt.suptitle('Exemples MNIST', fontsize=14)
plt.tight_layout()
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/cnn_mnist_samples.png', dpi=100)
plt.show()

print("\nâœ“ Graphique sauvegardÃ© : cnn_mnist_samples.png")

# PrÃ©paration
print("\nâš™ï¸  PrÃ©paration des donnÃ©es...\n")

# Normaliser pixels [0, 255] â†’ [0, 1]
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Reshape pour ajouter dimension channel
# (60000, 28, 28) â†’ (60000, 28, 28, 1)
X_train = X_train.reshape(-1, 28, 28, 1)
X_test = X_test.reshape(-1, 28, 28, 1)

# Split train/val
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.1, random_state=42, stratify=y_train
)

print(f"âœ“ PrÃ©paration terminÃ©e")
print(f"  Train : {X_train.shape}")
print(f"  Val : {X_val.shape}")
print(f"  Test : {X_test.shape}")

print("""
ğŸ’¡ PRÃ‰PARATION DES IMAGES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. NORMALISATION [0, 1] :
   Pixels entre 0-255 â†’ Diviser par 255
   âœ… Crucial pour convergence des NN

2. RESHAPE POUR CHANNEL :
   CNN attend format : (batch, height, width, channels)
   - Grayscale : channels = 1
   - RGB : channels = 3

3. LABELS :
   MNIST : 10 classes (chiffres 0-9)
   â†’ Pas besoin one-hot encoding si sparse_categorical_crossentropy
   â†’ Besoin one-hot si categorical_crossentropy
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 4 : CNN SIMPLE (BASELINE)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 4 : CNN SIMPLE (BASELINE)")
print("="*80)

print("\nğŸ—ï¸  Construction du CNN simple...\n")

def create_simple_cnn():
    """
    CNN simple : 2 Conv + 2 Dense
    """
    model = models.Sequential([
        # Conv Block 1
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),
        layers.MaxPooling2D((2, 2), name='pool1'),

        # Conv Block 2
        layers.Conv2D(64, (3, 3), activation='relu', name='conv2'),
        layers.MaxPooling2D((2, 2), name='pool2'),

        # Dense
        layers.Flatten(name='flatten'),
        layers.Dense(128, activation='relu', name='dense1'),
        layers.Dense(10, activation='softmax', name='output')
    ])

    return model

model_simple = create_simple_cnn()

model_simple.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',  # Labels entiers (0-9)
    metrics=['accuracy']
)

print(model_simple.summary())

print(f"""
ğŸ” OBSERVATION #2 : Architecture CNN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CE QU'IL FAUT OBSERVER DANS LE SUMMARY :

1. PROGRESSION DES SHAPES :
   Input : (28, 28, 1)
   Conv1 : (26, 26, 32) â†’ 32 feature maps
   Pool1 : (13, 13, 32) â†’ DivisÃ© par 2
   Conv2 : (11, 11, 64) â†’ 64 feature maps
   Pool2 : (5, 5, 64)
   Flatten : (1600) â†’ 5Ã—5Ã—64 = 1600

2. NOMBRE DE PARAMÃˆTRES :
   Total : {model_simple.count_params():,} paramÃ¨tres

   Conv1 : (3Ã—3Ã—1 + 1) Ã— 32 = 320
   Conv2 : (3Ã—3Ã—32 + 1) Ã— 64 = 18,496
   Dense1 : (1600 + 1) Ã— 128 = 204,928
   Output : (128 + 1) Ã— 10 = 1,290

   â†’ Conv layers : Peu de paramÃ¨tres (~20k)
   â†’ Dense layers : Beaucoup de paramÃ¨tres (~200k)

3. INTERPRÃ‰TATION :
   - Conv capture patterns avec peu de paramÃ¨tres
   - Mais Dense aprÃ¨s Flatten reste coÃ»teux
   - Solution moderne : Global Average Pooling au lieu de Flatten

ğŸ’¡ RÃˆGLE EMPIRIQUE :
   - Augmenter nb filtres en descendant
   - RÃ©duire taille spatiale en descendant
   - Structure pyramidale : Large & peu profond â†’ Ã‰troit & profond
""")

# EntraÃ®ner
print("\nğŸš€ EntraÃ®nement du CNN simple...\n")

early_stop = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

history_simple = model_simple.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=20,
    batch_size=128,
    callbacks=[early_stop],
    verbose=0
)

print(f"âœ“ EntraÃ®nement terminÃ© (epoch {len(history_simple.history['loss'])})")

# Ã‰valuer
train_acc = history_simple.history['accuracy'][-1]
val_acc = history_simple.history['val_accuracy'][-1]
test_loss, test_acc = model_simple.evaluate(X_test, y_test, verbose=0)

print(f"\nğŸ“Š Performances :")
print(f"  Train Accuracy : {train_acc:.4f}")
print(f"  Val Accuracy : {val_acc:.4f}")
print(f"  Test Accuracy : {test_acc:.4f}")

# Courbes
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].plot(history_simple.history['loss'], label='Train Loss', linewidth=2)
axes[0].plot(history_simple.history['val_loss'], label='Val Loss', linewidth=2)
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].set_title('CNN Simple - Loss')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

axes[1].plot(history_simple.history['accuracy'], label='Train Acc', linewidth=2)
axes[1].plot(history_simple.history['val_accuracy'], label='Val Acc', linewidth=2)
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Accuracy')
axes[1].set_title('CNN Simple - Accuracy')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/cnn_simple_training.png', dpi=100)
plt.show()

print("\nâœ“ Graphique sauvegardÃ© : cnn_simple_training.png")

print(f"""
ğŸ” OBSERVATION #3 : Performance CNN Simple
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Test Accuracy : {test_acc:.4f} ({test_acc*100:.2f}%)

CE QU'IL FAUT OBSERVER :

1. PERFORMANCE :
   - >95% : Bon modÃ¨le
   - >97% : TrÃ¨s bon
   - >99% : Excellent (MNIST est "facile")

   Notre modÃ¨le : {test_acc*100:.2f}% â†’ {"Excellent" if test_acc > 0.99 else "TrÃ¨s bon" if test_acc > 0.97 else "Bon"}

2. OVERFITTING :
   Ã‰cart Train - Val : {train_acc - val_acc:.4f}
   â†’ {"Pas d'overfitting" if train_acc - val_acc < 0.02 else "LÃ©ger overfitting"}

3. CONVERGENCE :
   ArrÃªt epoch {len(history_simple.history['loss'])} / 20
   â†’ {"Convergence rapide" if len(history_simple.history['loss']) < 15 else "Convergence lente"}

ğŸ’¡ CONCLUSION :
   CNN simple atteint dÃ©jÃ  excellente performance sur MNIST.
   MNIST est dataset "facile" (images propres, centrÃ©es).

   Pour amÃ©liorer :
   1. Data Augmentation
   2. Architecture plus profonde
   3. Dropout / Batch Normalization
   4. Augmenter nombre de filtres
""")

input("\nâ–¶ Appuyez sur EntrÃ©e pour continuer...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 5 : DATA AUGMENTATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 5 : DATA AUGMENTATION")
print("="*80)

print("""
ğŸ”„ POURQUOI DATA AUGMENTATION ?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PROBLÃˆME :
   Dataset limitÃ© â†’ Overfitting
   ModÃ¨le voit toujours mÃªmes images â†’ MÃ©morise

SOLUTION :
   CrÃ©er variations artificielles des images pendant entraÃ®nement
   â†’ ModÃ¨le voit images lÃ©gÃ¨rement diffÃ©rentes Ã  chaque epoch
   â†’ GÃ©nÃ©ralise mieux

TRANSFORMATIONS COURANTES :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. ROTATION (rotation_range=10Â°)
   Tourne image jusqu'Ã  Â±10Â°
   Usage : Objets pas toujours droits

2. TRANSLATION (width_shift, height_shift=0.1)
   DÃ©cale image horizontalement/verticalement
   Usage : Objet pas toujours centrÃ©

3. ZOOM (zoom_range=0.1)
   Zoom in/out jusqu'Ã  10%
   Usage : Objet Ã  diffÃ©rentes distances

4. FLIP (horizontal_flip=True)
   Miroir horizontal
   Usage : Objets symÃ©triques (chat, voiture)
   âš ï¸ Pas pour texte ou chiffres !

5. SHEAR (shear_range=0.1)
   Cisaillement (dÃ©formation)
   Usage : Perspectives diffÃ©rentes

6. BRIGHTNESS (brightness_range=[0.8, 1.2])
   Ajuste luminositÃ©
   Usage : Conditions d'Ã©clairage variables

âš ï¸  ATTENTION :
   Transformations doivent Ãªtre RÃ‰ALISTES
   Ne pas augmenter si transformations changent classe !
   Ex : Flip horizontal d'un '6' â†’ ressemble Ã  '9'

ğŸ¯ IMPACT ATTENDU :
   - RÃ©duit overfitting (Ã©cart train-val â†“)
   - AmÃ©liore gÃ©nÃ©ralisation (test acc â†‘)
   - Ã‰quivalent Ã  avoir plus de donnÃ©es
""")

print("\nğŸ”„ CrÃ©ation du gÃ©nÃ©rateur d'augmentation...\n")

# Data Augmentation pour MNIST (lÃ©ger car chiffres)
datagen = ImageDataGenerator(
    rotation_range=10,  # Rotation Â±10Â°
    width_shift_range=0.1,  # Translation horizontale 10%
    height_shift_range=0.1,  # Translation verticale 10%
    zoom_range=0.1,  # Zoom Â±10%
    # Pas de flip pour chiffres !
)

datagen.fit(X_train)

# Visualiser augmentation
print("ğŸ“Š Exemples d'augmentation...\n")

fig, axes = plt.subplots(3, 6, figsize=(15, 8))

# Image originale
original_image = X_train[0:1]

for i, ax in enumerate(axes.flat):
    if i == 0:
        ax.imshow(original_image[0, :, :, 0], cmap='gray')
        ax.set_title('Original')
    else:
        # GÃ©nÃ©rer image augmentÃ©e
        augmented = next(datagen.flow(original_image, batch_size=1))
        ax.imshow(augmented[0, :, :, 0], cmap='gray')
        ax.set_title(f'AugmentÃ© {i}')
    ax.axis('off')

plt.suptitle('Data Augmentation - Variations d\'une mÃªme image', fontsize=14)
plt.tight_layout()
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/cnn_data_augmentation.png', dpi=100)
plt.show()

print("âœ“ Graphique sauvegardÃ© : cnn_data_augmentation.png")

print("""
ğŸ” OBSERVATION #4 : Variations GÃ©nÃ©rÃ©es
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CE QU'IL FAUT OBSERVER :

1. VARIATIONS LÃ‰GÃˆRES :
   Chiffre toujours reconnaissable
   â†’ Transformations RÃ‰ALISTES

2. DIVERSITÃ‰ :
   Chaque image lÃ©gÃ¨rement diffÃ©rente
   â†’ ModÃ¨le ne voit jamais exactement la mÃªme

3. CLASSE PRÃ‰SERVÃ‰E :
   Toujours le mÃªme chiffre
   â†’ Transformations ne changent pas le label

ğŸ’¡ CONCLUSION :
   Data Augmentation = RÃ©gularisation puissante
   Ã‰quivalent Ã  avoir 10-100Ã— plus de donnÃ©es !

âš ï¸  SI TRANSFORMATIONS TROP FORTES :
   - Chiffre illisible â†’ Peut nuire performance
   - RÃ¨gle : Humain doit encore reconnaÃ®tre l'objet
""")

# EntraÃ®ner avec augmentation
print("\nğŸš€ EntraÃ®nement avec Data Augmentation...\n")

# Nouveau modÃ¨le
model_augmented = create_simple_cnn()
model_augmented.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# EntraÃ®ner avec gÃ©nÃ©rateur
history_augmented = model_augmented.fit(
    datagen.flow(X_train, y_train, batch_size=128),
    validation_data=(X_val, y_val),
    epochs=20,
    steps_per_epoch=len(X_train) // 128,
    callbacks=[early_stop],
    verbose=0
)

print(f"âœ“ EntraÃ®nement terminÃ© (epoch {len(history_augmented.history['loss'])})")

# Ã‰valuer
test_loss_aug, test_acc_aug = model_augmented.evaluate(X_test, y_test, verbose=0)

print(f"\nğŸ“Š Comparaison :")
print(f"  Sans augmentation : {test_acc:.4f}")
print(f"  Avec augmentation : {test_acc_aug:.4f}")
print(f"  AmÃ©lioration : {(test_acc_aug - test_acc)*100:+.2f}%")

print(f"""
ğŸ” OBSERVATION #5 : Impact de l'Augmentation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CE QU'IL FAUT OBSERVER :

1. TEST ACCURACY :
   Sans : {test_acc:.4f}
   Avec : {test_acc_aug:.4f}

   {"âœ… AmÃ©lioration significative" if test_acc_aug > test_acc + 0.005 else "âš ï¸ Peu d'amÃ©lioration"}

2. OVERFITTING :
   Avec augmentation, Ã©cart train-val devrait DIMINUER
   â†’ ModÃ¨le gÃ©nÃ©ralise mieux

3. CONVERGENCE :
   Peut Ãªtre PLUS LENTE (images plus variÃ©es)
   â†’ Normal, continuer plus longtemps si nÃ©cessaire

ğŸ’¡ INTERPRÃ‰TATION :

Si amÃ©lioration faible :
   - MNIST est dÃ©jÃ  "facile" (98%+ sans augmentation)
   - Augmentation plus utile sur datasets complexes (CIFAR, ImageNet)
   - Ou transformations pas assez fortes

Si amÃ©lioration significative :
   - Augmentation efficace
   - ModÃ¨le moins overfittÃ©
   - Continue avec cette approche !

ğŸ¯ RÃˆGLE D'OR :
   TOUJOURS utiliser Data Augmentation sur images en production !
   Sauf si dataset Ã‰NORME (>1M images) et trÃ¨s diversifiÃ©.
""")

input("\nâ–¶ Appuyez sur EntrÃ©e pour continuer...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 6 : CNN PROFOND ET ARCHITECTURES CÃ‰LÃˆBRES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 6 : ARCHITECTURES CNN CÃ‰LÃˆBRES")
print("="*80)

print("""
ğŸ›ï¸  Ã‰VOLUTION DES ARCHITECTURES CNN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1ï¸âƒ£  LeNet-5 (1998) - Yann LeCun
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Architecture : Conv â†’ Pool â†’ Conv â†’ Pool â†’ Dense â†’ Dense
Usage : MNIST, reconnaissance de chiffres
Innovation : PremiÃ¨re CNN efficace

2ï¸âƒ£  AlexNet (2012) - ImageNet Winner
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Architecture : 5 Conv + 3 Dense
Innovation :
- ReLU (au lieu de tanh)
- Dropout
- Data Augmentation
- GPU training

Impact : RÃ©volution Deep Learning !

3ï¸âƒ£  VGG (2014)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Architecture : Blocs de Conv 3Ã—3 rÃ©pÃ©tÃ©s
Innovation :
- PROFONDEUR (16-19 layers)
- Uniquement Conv 3Ã—3
- Architecture simple et uniforme

Limitation : Beaucoup de paramÃ¨tres (~140M)

4ï¸âƒ£  ResNet (2015) - He et al.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Architecture : Residual Blocks avec skip connections
Innovation :
- Connexions rÃ©siduelles : x + F(x)
- Permet rÃ©seaux TRÃˆS PROFONDS (50, 101, 152 layers)
- RÃ©sout "gradient vanishing"

Impact : Standard actuel !

5ï¸âƒ£  EfficientNet (2019)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Architecture : Scaling composÃ© (depth, width, resolution)
Innovation :
- Optimisation automatique architecture
- Meilleur compromis performance/efficacitÃ©

Usage : Production avec ressources limitÃ©es


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Architectureâ”‚ AnnÃ©e  â”‚ Layers     â”‚ Params   â”‚ Top-1   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LeNet       â”‚ 1998   â”‚ 5          â”‚ 60K      â”‚ -       â”‚
â”‚ AlexNet     â”‚ 2012   â”‚ 8          â”‚ 60M      â”‚ 63.3%   â”‚
â”‚ VGG-16      â”‚ 2014   â”‚ 16         â”‚ 138M     â”‚ 71.5%   â”‚
â”‚ ResNet-50   â”‚ 2015   â”‚ 50         â”‚ 25M      â”‚ 76.2%   â”‚
â”‚ EfficientNetâ”‚ 2019   â”‚ Variable   â”‚ 5-66M    â”‚ 84.4%   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Top-1 = Accuracy sur ImageNet


ğŸ¯ QUELLE ARCHITECTURE CHOISIR ?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SCRATCH (entraÃ®nement from scratch) :
   - Petites images (MNIST) â†’ CNN simple
   - Images moyennes (CIFAR) â†’ VGG-like ou ResNet
   - Grandes images â†’ ResNet ou EfficientNet

TRANSFER LEARNING (rÃ©utiliser modÃ¨le prÃ©-entraÃ®nÃ©) :
   - Dataset petit (<10k) â†’ ResNet50 prÃ©-entraÃ®nÃ©
   - Dataset moyen â†’ ResNet ou EfficientNet prÃ©-entraÃ®nÃ©
   - Production â†’ EfficientNet (efficace)

ğŸ’¡ RECOMMANDATION 2024 :
   - From scratch : Inspiration ResNet (residual blocks)
   - Transfer learning : EfficientNet ou ResNet50
   - Recherche : Vision Transformers (ViT)
""")

print("\nğŸ—ï¸  Construction d'un CNN inspirÃ© VGG...\n")

def create_vgg_like_cnn():
    """
    CNN inspirÃ© de VGG : Blocs de Conv rÃ©pÃ©tÃ©es
    """
    model = models.Sequential([
        # Block 1
        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),
        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),

        # Block 2
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),

        # Block 3
        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),

        # Dense
        layers.Flatten(),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(10, activation='softmax')
    ])

    return model

model_vgg = create_vgg_like_cnn()
model_vgg.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

print(model_vgg.summary())

print(f"\nğŸš€ EntraÃ®nement du CNN profond...\n")

history_vgg = model_vgg.fit(
    datagen.flow(X_train, y_train, batch_size=128),
    validation_data=(X_val, y_val),
    epochs=20,
    steps_per_epoch=len(X_train) // 128,
    callbacks=[early_stop],
    verbose=0
)

test_loss_vgg, test_acc_vgg = model_vgg.evaluate(X_test, y_test, verbose=0)

print(f"âœ“ EntraÃ®nement terminÃ©")
print(f"\nğŸ“Š Comparaison Finale :")
print(f"  CNN Simple : {test_acc:.4f}")
print(f"  + Augmentation : {test_acc_aug:.4f}")
print(f"  VGG-like : {test_acc_vgg:.4f}")

print(f"""
ğŸ” OBSERVATION #6 : Architecture Profonde vs Simple
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CE QU'IL FAUT OBSERVER :

1. PERFORMANCE :
   Simple â†’ VGG : {test_acc:.4f} â†’ {test_acc_vgg:.4f}
   AmÃ©lioration : {(test_acc_vgg - test_acc)*100:+.2f}%

2. PARAMÃˆTRES :
   Simple : {model_simple.count_params():,}
   VGG-like : {model_vgg.count_params():,}
   Ratio : {model_vgg.count_params() / model_simple.count_params():.1f}Ã—

3. TEMPS D'ENTRAÃNEMENT :
   Plus profond = Plus lent (mais plus puissant)

ğŸ’¡ INTERPRÃ‰TATION :

Sur MNIST (simple) :
   - Gain modeste car dataset "facile"
   - CNN simple dÃ©jÃ  >98%
   - VGG utile surtout si besoin 99%+

Sur datasets complexes (CIFAR, ImageNet) :
   - Gain MAJEUR avec architecture profonde
   - NÃ©cessitÃ© de ResNet/VGG pour bonne performance

ğŸ¯ RÃˆGLE DE DÃ‰CISION :
   Dataset simple (MNIST) â†’ CNN simple suffit
   Dataset complexe (CIFAR, ImageNet) â†’ Architecture profonde nÃ©cessaire

ğŸ’¡ OVERFITTING ?
   Si VGG overfit :
   - Augmenter Dropout (0.25 â†’ 0.4)
   - Plus de Data Augmentation
   - Batch Normalization
   - Plus de donnÃ©es
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 7 : VISUALISATION DES FEATURES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 7 : VISUALISATION DES FEATURES APPRISES")
print("="*80)

print("\nğŸ” Visualisation des activations...\n")

# Prendre une image de test
test_image = X_test[0:1]
true_label = y_test[0]

# CrÃ©er modÃ¨le pour extraire activations intermÃ©diaires
layer_outputs = [layer.output for layer in model_vgg.layers[:6]]  # Premiers 6 layers
activation_model = models.Model(inputs=model_vgg.input, outputs=layer_outputs)

# Obtenir activations
activations = activation_model.predict(test_image, verbose=0)

# Visualiser
layer_names = ['conv2d', 'conv2d_1', 'max_pooling2d', 'conv2d_2', 'conv2d_3', 'max_pooling2d_1']

fig, axes = plt.subplots(2, 3, figsize=(15, 10))

for idx, (ax, activation, name) in enumerate(zip(axes.flat, activations, layer_names)):
    if 'conv' in name:
        # Montrer premier filtre
        ax.imshow(activation[0, :, :, 0], cmap='viridis')
        ax.set_title(f'{name}\nFiltre 0')
    else:
        # Pooling
        ax.imshow(activation[0, :, :, 0], cmap='viridis')
        ax.set_title(f'{name}')
    ax.axis('off')

plt.suptitle(f'Activations pour chiffre {true_label}', fontsize=14)
plt.tight_layout()
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/cnn_activations.png', dpi=100)
plt.show()

print("âœ“ Graphique sauvegardÃ© : cnn_activations.png")

print("""
ğŸ” OBSERVATION #7 : Features Apprises
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CE QU'IL FAUT OBSERVER :

1. LAYER 1 (Conv) :
   DÃ©tecte bords, lignes, courbes simples
   â†’ Features BAS NIVEAU (low-level)

2. LAYER 2 (Conv) :
   Combine bords â†’ Textures, motifs
   â†’ Features INTERMÃ‰DIAIRES

3. APRÃˆS POOLING :
   RÃ©solution rÃ©duite mais information prÃ©servÃ©e
   â†’ Abstraction progressive

4. LAYERS PROFONDS :
   Features de plus en plus abstraites
   â†’ ReprÃ©sentations HAUT NIVEAU (high-level)

ğŸ’¡ INTERPRÃ‰TATION :
   CNN apprend HIÃ‰RARCHIE de features :
   Pixels â†’ Bords â†’ Textures â†’ Parties â†’ Objets

   C'est pourquoi CNN excelle sur images :
   Mimique systÃ¨me visuel humain !

ğŸ¯ DEBUGGING :
   Si activations nulles (toutes noires) :
   â†’ Neurones "morts" (dying ReLU)
   â†’ Solutions :
     - Utiliser LeakyReLU
     - RÃ©duire learning rate
     - VÃ©rifier normalisation input
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 8 : RÃ‰SUMÃ‰ ET CONCLUSIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("ğŸ‰ RÃ‰SUMÃ‰ ET CONCLUSIONS")
print("="*80)

print(f"""
ğŸ“š CE QUE NOUS AVONS APPRIS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1ï¸âƒ£  POURQUOI CNN POUR IMAGES
   âœ… DÃ©tecte patterns locaux (bords, textures)
   âœ… Poids partagÃ©s â†’ Moins de paramÃ¨tres
   âœ… Invariance aux translations
   âœ… HiÃ©rarchie de features (bas â†’ haut niveau)
   âŒ Pas pour donnÃ©es tabulaires

2ï¸âƒ£  COMPOSANTS CNN
   ğŸ”· Conv2D : DÃ©tection de patterns
      - Filters : 32 â†’ 64 â†’ 128 (doubler)
      - Kernel : 3Ã—3 (standard)
      - Padding : 'same' (garder taille)

   ğŸ”· MaxPooling : RÃ©duction dimensionnelle
      - Taille : 2Ã—2 (standard)
      - RÃ©duit overfitting
      - Invariance spatiale

   ğŸ”· Flatten : Conv â†’ Dense
      - Ou Global Average Pooling (moderne)

   ğŸ”· Dense : Classification finale

3ï¸âƒ£  DATA AUGMENTATION
   ğŸ”„ ESSENTIEL pour images !
   ğŸ”„ Transformations rÃ©alistes
   ğŸ”„ Ã‰quivalent Ã  10-100Ã— plus de donnÃ©es
   ğŸ”„ RÃ©duit overfitting significativement

4ï¸âƒ£  ARCHITECTURES
   ğŸ›ï¸  Simple : 2-3 Conv blocks
   ğŸ›ï¸  VGG : Conv rÃ©pÃ©tÃ©es + profondeur
   ğŸ›ï¸  ResNet : Skip connections (moderne)
   ğŸ›ï¸  EfficientNet : OptimisÃ© (production)

5ï¸âƒ£  RÃ‰SULTATS SUR MNIST
   ğŸ“Š CNN Simple : {test_acc*100:.2f}%
   ğŸ“Š + Augmentation : {test_acc_aug*100:.2f}%
   ğŸ“Š VGG-like : {test_acc_vgg*100:.2f}%

   â†’ CNN atteint facilement >98% sur MNIST


âœ… CHECKLIST CNN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ“ Images normalisÃ©es [0, 1]
âœ“ Shape correcte (batch, H, W, C)
âœ“ Architecture progressive (filtres â†‘, taille â†“)
âœ“ Data Augmentation configurÃ©e
âœ“ Dropout entre Dense layers
âœ“ Early Stopping
âœ“ Learning rate scheduling
âœ“ Visualisation des activations (debugging)


ğŸ¯ RÃˆGLES D'OR CNN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. "TOUJOURS normaliser images [0, 1]"
2. "Conv 3Ã—3 est le standard (99% des cas)"
3. "Doubler filtres, diviser taille par 2"
4. "Data Augmentation est NON-NÃ‰GOCIABLE"
5. "Start simple, go deep si underfitting"


ğŸš€ PROCHAINES Ã‰TAPES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Transfer Learning (rÃ©utiliser modÃ¨les prÃ©-entraÃ®nÃ©s)
2. CIFAR-10 (images couleur, plus complexe)
3. Object Detection (YOLO, Faster R-CNN)
4. Segmentation SÃ©mantique (U-Net)
5. GANs (Generative Adversarial Networks)
6. Vision Transformers (ViT)


ğŸ’¡ TRANSFER LEARNING (APERÃ‡U)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Au lieu d'entraÃ®ner from scratch :

```python
# Charger ResNet50 prÃ©-entraÃ®nÃ© sur ImageNet
base_model = applications.ResNet50(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)

# Geler les layers
base_model.trainable = False

# Ajouter classification personnalisÃ©e
model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation='softmax')
])
```

AVANTAGES :
âœ… Converge 10-100Ã— plus vite
âœ… Meilleure performance avec peu de donnÃ©es
âœ… Features gÃ©nÃ©riques dÃ©jÃ  apprises

USAGE :
- Dataset < 10k images : ESSENTIEL
- Dataset > 100k : Optionnel mais accÃ©lÃ¨re


ğŸ”§ HYPERPARAMÃˆTRES Ã€ TUNER (PAR PRIORITÃ‰)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Data Augmentation (rotation, shift, zoom)
2. Architecture (nb layers, filtres)
3. Dropout rate (0.25-0.5)
4. Learning rate (0.0001-0.01)
5. Batch size (32, 64, 128)
6. Optimizer (Adam, SGD+momentum)


ğŸ› DEBUGGING CNN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âŒ Accuracy stagne ~10% (random) :
   â†’ ModÃ¨le ne converge pas
   â†’ VÃ©rifier normalisation images [0, 1]
   â†’ VÃ©rifier labels corrects
   â†’ RÃ©duire learning rate

âŒ Loss = NaN :
   â†’ Learning rate trop grand
   â†’ Gradient exploding
   â†’ VÃ©rifier pas de valeurs infinies dans data

âŒ Overfitting fort :
   â†’ Plus de Data Augmentation
   â†’ Dropout (0.3-0.5)
   â†’ L2 regularization
   â†’ RÃ©duire nombre de filtres
   â†’ Plus de donnÃ©es

âŒ Underfitting :
   â†’ Architecture trop simple
   â†’ Augmenter filtres (32 â†’ 64 â†’ 128)
   â†’ Ajouter Conv layers
   â†’ RÃ©duire Dropout
   â†’ EntraÃ®ner plus longtemps


ğŸ“Š PERFORMANCES ATTENDUES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MNIST : 99%+ (facile)
Fashion-MNIST : 90-93%
CIFAR-10 : 85-95% (selon architecture)
ImageNet : 75-85% (state-of-the-art)


ğŸ’ª VOUS MAÃTRISEZ MAINTENANT :
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Principes des CNN
âœ… Architecture (Conv, Pooling, Dense)
âœ… Data Augmentation
âœ… EntraÃ®nement et optimisation
âœ… Visualisation des features
âœ… Debugging et amÃ©lioration

â†’ PrÃªt pour projets de vision par ordinateur !
""")

print("="*80)
print("âœ¨ TUTORIEL TERMINÃ‰ AVEC SUCCÃˆS ! âœ¨")
print("="*80)
print("\nğŸ¨ Vous maÃ®trisez maintenant les CNN !")
print("ğŸ“š Dernier tutoriel : Clustering (Apprentissage Non SupervisÃ©)")
