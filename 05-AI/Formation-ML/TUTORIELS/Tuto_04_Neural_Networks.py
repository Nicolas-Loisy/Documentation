"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TUTORIEL COMPLET : NEURAL NETWORKS (RÃ‰SEAUX DE NEURONES)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ CAS D'USAGE RÃ‰EL : PrÃ©diction de Prix Immobilier (RÃ©gression)

CONTEXTE :
Agence immobiliÃ¨re veut prÃ©dire prix de vente basÃ© sur caractÃ©ristiques.
DonnÃ©es complexes avec interactions non-linÃ©aires.

POURQUOI NEURAL NETWORKS ?
- Capture relations NON-LINÃ‰AIRES complexes
- Interactions automatiques entre features
- Performance excellente si assez de donnÃ©es
- Base du Deep Learning

Ce tutoriel couvre :
1. QUAND utiliser Neural Networks vs modÃ¨les classiques
2. Architecture : layers, neurons, activations
3. Forward propagation et Backpropagation (expliquÃ© simplement)
4. Optimiseurs (SGD, Adam, RMSprop) - Quand utiliser quoi ?
5. Learning rate et scheduling
6. RÃ©gularisation (Dropout, L1/L2, Batch Normalization)
7. Diagnostic overfitting/underfitting
8. Early stopping et callbacks
9. Comparaison avec modÃ¨les classiques

Chaque Ã©tape explique CE QU'IL FAUT OBSERVER et LES CONCLUSIONS.
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# TensorFlow / Keras
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, callbacks, regularizers, optimizers

import warnings
warnings.filterwarnings('ignore')

# Configuration
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)
tf.random.set_seed(42)

print("="*80)
print("TUTORIEL : NEURAL NETWORKS (RÃ‰SEAUX DE NEURONES)")
print("="*80)
print(f"TensorFlow version : {tf.__version__}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 1 : COMPRENDRE LES NEURAL NETWORKS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 1 : QUAND UTILISER NEURAL NETWORKS ?")
print("="*80)

print("""
ğŸ§  QU'EST-CE QU'UN NEURAL NETWORK ?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ModÃ¨le inspirÃ© du cerveau humain : rÃ©seau de neurones artificiels.

STRUCTURE :
   Input Layer â†’ Hidden Layers â†’ Output Layer

   [Feature 1]â”€â”
   [Feature 2]â”€â”¼â”€â†’ [Neuron 1]â”€â”
   [Feature 3]â”€â”¼â”€â†’ [Neuron 2]â”€â”¼â”€â†’ [Neuron]â”€â†’ [PrÃ©diction]
   [Feature N]â”€â”˜  [Neuron M]â”€â”˜

Chaque connexion a un POIDS (weight) ajustÃ© pendant entraÃ®nement.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š QUAND UTILISER NEURAL NETWORKS vs MODÃˆLES CLASSIQUES ?
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… UTILISER NEURAL NETWORKS QUAND :

1. BEAUCOUP DE DONNÃ‰ES (>10k Ã©chantillons)
   â†’ NN apprennent mieux avec plus de donnÃ©es
   â†’ ModÃ¨les classiques plafonnent plus vite

2. RELATIONS COMPLEXES NON-LINÃ‰AIRES
   â†’ Interactions multiples entre features
   â†’ Patterns difficiles Ã  capturer par arbres

3. DONNÃ‰ES HAUTE DIMENSIONNALITÃ‰
   â†’ Images (milliers de pixels)
   â†’ Texte (milliers de mots)
   â†’ SÃ©quences (sÃ©ries temporelles)

4. BESOIN DE FLEXIBILITÃ‰
   â†’ Architecture personnalisable
   â†’ Transfer learning possible

5. PERFORMANCE MAXIMALE REQUISE
   â†’ PrÃªt Ã  sacrifier interprÃ©tabilitÃ©
   â†’ Temps/ressources disponibles


âŒ NE PAS UTILISER NEURAL NETWORKS QUAND :

1. PEU DE DONNÃ‰ES (<1k Ã©chantillons)
   â†’ NN overfittent facilement
   â†’ Random Forest / XGBoost meilleurs

2. INTERPRÃ‰TABILITÃ‰ CRITIQUE
   â†’ NN = "boÃ®te noire"
   â†’ PrÃ©fÃ©rer Logistic Reg, Decision Tree

3. DONNÃ‰ES TABULAIRES SIMPLES
   â†’ Relations linÃ©aires ou simples
   â†’ XGBoost souvent Ã©quivalent et plus rapide

4. RESSOURCES LIMITÃ‰ES
   â†’ NN gourmands en calcul/mÃ©moire
   â†’ ModÃ¨les classiques plus lÃ©gers

5. DÃ‰PLOIEMENT TEMPS RÃ‰EL STRICT
   â†’ Latence critique (<1ms)
   â†’ Linear models plus rapides


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CritÃ¨re           â”‚  NN                â”‚  XGBoost/RF        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ DonnÃ©es requises   â”‚ >10k (idÃ©al >100k) â”‚ 1k-10k suffit      â”‚
â”‚ Tabulaires         â”‚ â˜…â˜…â˜…â˜†â˜†              â”‚ â˜…â˜…â˜…â˜…â˜…              â”‚
â”‚ Images/Texte       â”‚ â˜…â˜…â˜…â˜…â˜…              â”‚ â˜…â˜†â˜†â˜†â˜†              â”‚
â”‚ InterprÃ©tabilitÃ©   â”‚ â˜…â˜†â˜†â˜†â˜†              â”‚ â˜…â˜…â˜…â˜†â˜†              â”‚
â”‚ Vitesse entraÃ®n.   â”‚ â˜…â˜…â˜†â˜†â˜†              â”‚ â˜…â˜…â˜…â˜…â˜†              â”‚
â”‚ Vitesse infÃ©rence  â”‚ â˜…â˜…â˜…â˜†â˜†              â”‚ â˜…â˜…â˜…â˜…â˜†              â”‚
â”‚ Setup complexitÃ©   â”‚ â˜…â˜…â˜†â˜†â˜† (difficile)  â”‚ â˜…â˜…â˜…â˜…â˜† (facile)     â”‚
â”‚ FlexibilitÃ©        â”‚ â˜…â˜…â˜…â˜…â˜…              â”‚ â˜…â˜…â˜…â˜†â˜†              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ RECOMMANDATION POUR NOTRE CAS (PRIX IMMOBILIER) :

DonnÃ©es : ~2000 Ã©chantillons, 20 features â†’ LIMITE
Objectif : RÃ©gression, relations potentiellement complexes

STRATÃ‰GIE :
1. Baseline : Linear Regression
2. AmÃ©lioration : XGBoost
3. Exploration : Neural Network (pour comparaison)

ğŸ’¡ Dans la rÃ©alitÃ© : XGBoost probablement meilleur sur tabulaire.
   Mais NN excellent pour APPRENDRE les concepts !
""")

input("\nâ–¶ Appuyez sur EntrÃ©e pour continuer...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 2 : PRÃ‰PARATION DES DONNÃ‰ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 2 : DONNÃ‰ES - PRÃ‰DICTION DE PRIX IMMOBILIER")
print("="*80)

# 2.1 GÃ©nÃ©ration
print("\nğŸ“Š GÃ©nÃ©ration de donnÃ©es synthÃ©tiques...\n")

X, y = make_regression(
    n_samples=2000,
    n_features=20,
    n_informative=15,
    noise=20.0,
    random_state=42
)

# Ajouter interactions non-linÃ©aires
X[:, 0] = X[:, 0] ** 2  # Feature 0 au carrÃ©
X[:, 1] = np.log1p(np.abs(X[:, 1]))  # Log de feature 1
X[:, 2] = X[:, 2] * X[:, 3]  # Interaction entre 2 et 3

# Normaliser target (prix) pour faciliter entraÃ®nement
y = (y - y.mean()) / y.std()

print(f"âœ“ DonnÃ©es gÃ©nÃ©rÃ©es : {X.shape[0]} propriÃ©tÃ©s Ã— {X.shape[1]} features")

# Features rÃ©alistes
feature_names = [
    'Surface', 'Chambres', 'Surface_x_Chambres', 'Annee_Construction',
    'Distance_Centre', 'Etage', 'Balcon', 'Parking',
    'Etat', 'Chauffage', 'Isolation', 'Quartier_Score',
    'Commerces_Proximite', 'Transports', 'Ecoles', 'Verdure',
    'Bruit', 'Securite', 'Taxe_Fonciere', 'Charges'
]

df = pd.DataFrame(X, columns=feature_names)
df['Prix'] = y

print("\nğŸ“ˆ AperÃ§u :")
print(df.head())

# 2.2 Split
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

print(f"\nâœ“ Train : {len(X_train)} | Val : {len(X_val)} | Test : {len(X_test)}")

# 2.3 Normalisation (CRUCIALE pour NN)
print("\nâš–ï¸  Normalisation...\n")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

print("âœ“ DonnÃ©es normalisÃ©es")

print("""
ğŸ’¡ POURQUOI NORMALISER EST CRUCIAL POUR NEURAL NETWORKS ?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. GRADIENT DESCENT CONVERGE MIEUX
   Si features sur Ã©chelles diffÃ©rentes (ex: Surface=100 vs Etage=3)
   â†’ Gradients de magnitudes diffÃ©rentes
   â†’ Oscillations, convergence lente ou Ã©chec

2. POIDS INITIALISÃ‰S DE FAÃ‡ON UNIFORME
   Initialisation suppose inputs normalisÃ©s
   â†’ Sinon certains neurons dominÃ©s dÃ¨s le dÃ©part

3. ACTIVATIONS STABLES
   Inputs normalisÃ©s â†’ Outputs de layers intermÃ©diaires stables
   â†’ Ã‰vite "gradient vanishing" ou "exploding"

4. LEARNING RATE UNIQUE FONCTIONNE
   MÃªme learning rate pour toutes les features
   â†’ Sinon faut ajuster par feature (complexe)

âš ï¸  SANS NORMALISATION :
   - Convergence 10-100Ã— plus lente
   - Souvent ne converge jamais
   - Performance dÃ©gradÃ©e

âœ… AVEC NORMALISATION :
   - Convergence rapide et stable
   - Performance optimale
   - HyperparamÃ¨tres plus faciles Ã  tuner

ğŸ¯ RÃˆGLE : TOUJOURS normaliser pour Neural Networks !
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 3 : ARCHITECTURE DES NEURAL NETWORKS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 3 : ARCHITECTURE - CONSTRUIRE UN RÃ‰SEAU")
print("="*80)

print("""
ğŸ—ï¸  COMPOSANTS D'UN NEURAL NETWORK
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1ï¸âƒ£  LAYERS (COUCHES)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Types principaux :

â€¢ Dense (Fully Connected) : Tous neurons connectÃ©s
  â†’ Usage : Tabulaire, features gÃ©nÃ©rales

â€¢ Convolutional : DÃ©tecte patterns locaux
  â†’ Usage : Images, signaux

â€¢ Recurrent (LSTM, GRU) : MÃ©moire sÃ©quentielle
  â†’ Usage : SÃ©ries temporelles, texte

â€¢ Dropout : DÃ©sactive neurons alÃ©atoirement
  â†’ Usage : RÃ©gularisation


2ï¸âƒ£  ACTIVATION FUNCTIONS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Introduisent NON-LINÃ‰ARITÃ‰ (sinon rÃ©seau = rÃ©gression linÃ©aire !)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Activation â”‚ Usage           â”‚ CaractÃ©ristiques    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ReLU       â”‚ Hidden layers   â”‚ Rapide, efficace    â”‚
â”‚            â”‚ (par dÃ©faut)    â”‚ Peut "mourir"       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LeakyReLU  â”‚ Hidden layers   â”‚ Ã‰vite "dying ReLU"  â”‚
â”‚            â”‚ (alternative)   â”‚                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Sigmoid    â”‚ Output binaire  â”‚ Entre 0 et 1        â”‚
â”‚            â”‚                 â”‚ Vanishing gradient  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tanh       â”‚ Hidden layers   â”‚ Entre -1 et 1       â”‚
â”‚            â”‚                 â”‚ Meilleur que sigmoidâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Linear     â”‚ Output rÃ©gressionâ”‚ Pas d'activation   â”‚
â”‚ (None)     â”‚                 â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ RECOMMANDATIONS :
   Hidden layers : ReLU (ou LeakyReLU si problÃ¨me)
   Output rÃ©gression : Linear (None)
   Output classification binaire : Sigmoid
   Output multi-classe : Softmax


3ï¸âƒ£  NOMBRE DE LAYERS ET NEURONS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

RÃˆGLES EMPIRIQUES :

â€¢ Nombre de layers (profondeur) :
  - 1-2 hidden layers : ProblÃ¨mes simples
  - 3-5 layers : ProblÃ¨mes modÃ©rÃ©s (notre cas)
  - >5 layers : Deep Learning (images, texte)

â€¢ Nombre de neurons par layer :
  - Layer 1 : Plus grand (ex: 128, 256)
  - Layer 2 : Moyen (ex: 64, 128)
  - Layer 3+ : Plus petit (ex: 32, 64)

  Structure "entonnoir" : RÃ©duction progressive

â€¢ Taille totale du rÃ©seau :
  - Petit : <10k paramÃ¨tres â†’ Petites donnÃ©es
  - Moyen : 10k-100k â†’ DonnÃ©es moyennes
  - Grand : >100k â†’ Grandes donnÃ©es

âš ï¸  TROP DE NEURONS : Overfitting
âš ï¸  TROP PEU : Underfitting

ğŸ’¡ START SIMPLE, SCALE UP :
   Commencer petit, augmenter si underfitting.
""")

# 3.1 ModÃ¨le simple (baseline)
print("\nğŸ—ï¸  Construction du modÃ¨le SIMPLE (baseline)...\n")

def create_simple_model(input_dim):
    """
    ModÃ¨le simple : 1 hidden layer
    """
    model = models.Sequential([
        layers.Dense(64, activation='relu', input_dim=input_dim, name='hidden'),
        layers.Dense(1, activation='linear', name='output')
    ])

    return model

model_simple = create_simple_model(X_train_scaled.shape[1])

# Compiler : spÃ©cifie loss, optimizer, metrics
# - loss='mse' : Mean Squared Error (rÃ©gression)
# - optimizer='adam' : Algorithme d'optimisation
# - metrics=['mae'] : MÃ©triques Ã  suivre (pas optimisÃ©es, juste monitorÃ©es)
model_simple.compile(
    loss='mse',
    optimizer='adam',
    metrics=['mae']
)

# Summary : affiche architecture
print(model_simple.summary())

print(f"""
ğŸ” OBSERVATION #1 : Architecture du ModÃ¨le
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CE QU'IL FAUT OBSERVER DANS LE SUMMARY :

1. NOMBRE DE PARAMÃˆTRES (Total params) :
   Formula : (input_neurons + 1) Ã— output_neurons par layer

   Notre modÃ¨le : {model_simple.count_params()} paramÃ¨tres

   Hidden : ({X_train_scaled.shape[1]} inputs + 1 bias) Ã— 64 neurons = {(X_train_scaled.shape[1] + 1) * 64}
   Output : (64 + 1) Ã— 1 = {65}

2. INTERPRÃ‰TATION :
   - <10k params : ModÃ¨le SIMPLE â†’ Bon pour petites donnÃ©es
   - 10k-100k : ModÃ¨le MOYEN â†’ DonnÃ©es moyennes
   - >100k : ModÃ¨le COMPLEXE â†’ Beaucoup de donnÃ©es requises

   Notre cas : {model_simple.count_params()} params â†’ APPROPRIÃ‰ pour {len(X_train)} Ã©chantillons

3. RÃˆGLE EMPIRIQUE :
   Nombre params â‰ˆ Nombre Ã©chantillons / 10

   Notre ratio : {model_simple.count_params()} / {len(X_train)} = {model_simple.count_params() / len(X_train):.2f}
   â†’ {"Bon" if model_simple.count_params() / len(X_train) < 0.2 else "Risque overfitting !"}

ğŸ’¡ CONCLUSION :
   ModÃ¨le simple avec 1 layer suffit souvent.
   Augmenter complexitÃ© seulement si underfitting.
""")

# 3.2 ModÃ¨le profond
print("\nğŸ—ï¸  Construction du modÃ¨le PROFOND...\n")

def create_deep_model(input_dim):
    """
    ModÃ¨le profond : 3 hidden layers + Dropout
    """
    model = models.Sequential([
        # Layer 1 : Large
        layers.Dense(128, activation='relu', input_dim=input_dim, name='hidden1'),
        layers.Dropout(0.3, name='dropout1'),

        # Layer 2 : Moyen
        layers.Dense(64, activation='relu', name='hidden2'),
        layers.Dropout(0.2, name='dropout2'),

        # Layer 3 : Small
        layers.Dense(32, activation='relu', name='hidden3'),

        # Output
        layers.Dense(1, activation='linear', name='output')
    ])

    return model

model_deep = create_deep_model(X_train_scaled.shape[1])

model_deep.compile(
    loss='mse',
    optimizer='adam',
    metrics=['mae']
)

print(model_deep.summary())

print(f"""
ğŸ” OBSERVATION #2 : ModÃ¨le Simple vs Profond
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Simple : {model_simple.count_params()} paramÃ¨tres
Profond : {model_deep.count_params()} paramÃ¨tres

CE QU'IL FAUT OBSERVER :

1. CAPACITÃ‰ DU MODÃˆLE :
   Plus de paramÃ¨tres = Plus de capacitÃ© d'apprentissage
   â†’ Peut capturer patterns plus complexes
   â†’ Mais risque overfitting ++

2. DROPOUT LAYERS :
   DÃ©sactive alÃ©atoirement neurons pendant entraÃ®nement
   â†’ Force rÃ©seau Ã  apprendre features redondantes
   â†’ RÃ‰GULARISATION puissante contre overfitting

   Dropout rate :
   - 0.2-0.3 : LÃ©ger (nos hidden layers)
   - 0.5 : Fort (moins utilisÃ© maintenant)

3. STRUCTURE "ENTONNOIR" :
   128 â†’ 64 â†’ 32 â†’ 1

   POURQUOI ?
   - DÃ©but : Features brutes, haute dimensionnalitÃ©
   - Milieu : ReprÃ©sentations abstraites
   - Fin : DÃ©cision compressÃ©e

   Comme cerveau : compression progressive d'information

ğŸ’¡ CONCLUSION :
   ModÃ¨le profond plus puissant MAIS :
   - Risque overfitting si pas assez de donnÃ©es
   - Plus lent Ã  entraÃ®ner
   - Plus difficile Ã  optimiser

   â†’ Start simple, go deep si underfitting !
""")

input("\nâ–¶ Appuyez sur EntrÃ©e pour continuer...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 4 : ENTRAÃNEMENT ET OPTIMISEURS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 4 : ENTRAÃNEMENT - OPTIMISEURS ET LEARNING RATE")
print("="*80)

print("""
âš™ï¸  OPTIMISEURS : COMMENT LE RÃ‰SEAU APPREND
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PRINCIPE : Ajuster poids pour minimiser loss via GRADIENT DESCENT

1ï¸âƒ£  SGD (Stochastic Gradient Descent)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PRINCIPE : Î¸_{t+1} = Î¸_t - Î· Ã— âˆ‡Loss

âœ… AVANTAGES :
   - Simple, bien compris
   - Avec momentum : Ã©vite minima locaux

âŒ INCONVÃ‰NIENTS :
   - Learning rate fixe â†’ ProblÃ©matique
   - Convergence lente
   - Sensible au scaling des features

ğŸ’¼ USAGE : Rarement utilisÃ© seul maintenant


2ï¸âƒ£  ADAM (Adaptive Moment Estimation)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PRINCIPE : Learning rate ADAPTATIF par paramÃ¨tre

âœ… AVANTAGES :
   - Learning rate s'ajuste automatiquement
   - Convergence RAPIDE
   - Robuste, fonctionne out-of-the-box
   - PEU SENSIBLE au tuning

âŒ INCONVÃ‰NIENTS :
   - Parfois converge vers solution sous-optimale
   - MÃ©moire supplÃ©mentaire

ğŸ’¼ USAGE : DÃ‰FAUT RECOMMANDÃ‰ (90% des cas)


3ï¸âƒ£  RMSprop
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PRINCIPE : Adapte learning rate basÃ© sur moyenne des gradients rÃ©cents

âœ… AVANTAGES :
   - Bon pour RNN
   - GÃ¨re bien gradients bruyants

âŒ INCONVÃ‰NIENTS :
   - Moins performant qu'Adam gÃ©nÃ©ralement

ğŸ’¼ USAGE : RNN, sÃ©ries temporelles


4ï¸âƒ£  ADAGRAD, ADADELTA, etc.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Variantes avec adaptations spÃ©cifiques.
Rarement utilisÃ©es maintenant (Adam les surpasse).


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Optimizer  â”‚ Vitesse    â”‚ Robustesse â”‚ Usage              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SGD        â”‚ â˜…â˜…â˜†â˜†â˜†      â”‚ â˜…â˜…â˜†â˜†â˜†      â”‚ Rare               â”‚
â”‚ SGD+moment â”‚ â˜…â˜…â˜…â˜†â˜†      â”‚ â˜…â˜…â˜…â˜†â˜†      â”‚ Fine-tuning        â”‚
â”‚ Adam       â”‚ â˜…â˜…â˜…â˜…â˜…      â”‚ â˜…â˜…â˜…â˜…â˜…      â”‚ DÃ‰FAUT (90%)       â”‚
â”‚ RMSprop    â”‚ â˜…â˜…â˜…â˜…â˜†      â”‚ â˜…â˜…â˜…â˜†â˜†      â”‚ RNN                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ RECOMMANDATION :
   TOUJOURS commencer avec ADAM.
   Changer seulement si problÃ¨me spÃ©cifique.


ğŸ“Š LEARNING RATE (Î·)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ParamÃ¨tre le PLUS IMPORTANT !

Î· trop petit â†’ Convergence TRÃˆS lente
Î· trop grand â†’ Divergence ou oscillations
Î· optimal â†’ Convergence rapide et stable

VALEURS TYPIQUES :
- SGD : 0.01 - 0.1
- Adam : 0.0001 - 0.01 (dÃ©faut : 0.001)

ğŸ’¡ STRATÃ‰GIE :
   1. Adam avec dÃ©faut (0.001) â†’ Marche 80% du temps
   2. Si convergence lente : Augmenter (0.003, 0.01)
   3. Si divergence : Diminuer (0.0003, 0.0001)
""")

# 4.1 EntraÃ®ner modÃ¨le simple
print("\nğŸš€ EntraÃ®nement du modÃ¨le SIMPLE...\n")

# fit() : EntraÃ®ne le modÃ¨le
# - X_train, y_train : DonnÃ©es d'entraÃ®nement
# - validation_data : DonnÃ©es de validation (Ã©valuÃ© mais pas entraÃ®nÃ© dessus)
# - epochs : Nombre de passages complets sur les donnÃ©es
# - batch_size : Nombre d'Ã©chantillons par mise Ã  jour des poids
# - verbose : Affichage (0=silent, 1=progress bar, 2=one line per epoch)

history_simple = model_simple.fit(
    X_train_scaled, y_train,
    validation_data=(X_val_scaled, y_val),
    epochs=100,
    batch_size=32,
    verbose=0  # Silent pour ne pas polluer output
)

print("âœ“ EntraÃ®nement terminÃ©")

# 4.2 Visualiser courbes d'apprentissage
print("\nğŸ“Š Courbes d'apprentissage...\n")

def plot_training_history(history, title):
    """
    Visualise loss et mÃ©triques pendant entraÃ®nement
    """
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Loss
    axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)
    axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss (MSE)')
    axes[0].set_title(f'{title} - Loss')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # MAE
    axes[1].plot(history.history['mae'], label='Train MAE', linewidth=2)
    axes[1].plot(history.history['val_mae'], label='Val MAE', linewidth=2)
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('MAE')
    axes[1].set_title(f'{title} - MAE')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    return fig

fig = plot_training_history(history_simple, "ModÃ¨le Simple")
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/nn_training_simple.png', dpi=100)
plt.show()

print("âœ“ Graphique sauvegardÃ© : nn_training_simple.png")

print(f"""
ğŸ” OBSERVATION #3 : Courbes d'Apprentissage
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CE QU'IL FAUT OBSERVER :

1. CONVERGENCE :
   Loss diminue-t-elle rÃ©guliÃ¨rement ?
   âœ… Oui : Apprentissage normal
   âŒ Non (plateau prÃ©coce) : Underfitting ou learning rate trop petit
   âŒ Non (oscillations) : Learning rate trop grand ou batch size trop petit

2. OVERFITTING :
   Train Loss << Val Loss ?

   Ã‰cart actuel : {history_simple.history['loss'][-1]:.4f} vs {history_simple.history['val_loss'][-1]:.4f}

   âœ… Ã‰cart < 10% : Pas d'overfitting
   âš ï¸  Ã‰cart 10-30% : LÃ©ger overfitting
   âŒ Ã‰cart > 30% : Overfitting FORT

3. UNDERFITTING :
   Train Loss ET Val Loss Ã©levÃ©es ?
   â†’ ModÃ¨le pas assez puissant

   Actions :
   - Augmenter nombre de neurons
   - Ajouter layers
   - EntraÃ®ner plus longtemps

4. MOMENT D'ARRÃŠT :
   Val Loss ne diminue plus depuis plusieurs epochs ?
   â†’ EARLY STOPPING recommandÃ©

   Epoch optimal â‰ˆ {np.argmin(history_simple.history['val_loss']) + 1} / 100

ğŸ’¡ INTERPRÃ‰TATION DE NOS COURBES :
   - Convergence : {"âœ… Stable" if history_simple.history['loss'][-1] < history_simple.history['loss'][10] else "âŒ ProblÃ¨me"}
   - Overfitting : {"âœ… Pas d'overfitting" if (history_simple.history['val_loss'][-1] / history_simple.history['loss'][-1] - 1) < 0.3 else "âš ï¸ Overfitting dÃ©tectÃ©"}
   - Early stop : Aurait pu arrÃªter epoch {np.argmin(history_simple.history['val_loss']) + 1}

ğŸ¯ CONCLUSION :
   Courbes d'apprentissage = DIAGNOSTIC le plus important !
   Ã€ vÃ©rifier SYSTÃ‰MATIQUEMENT.
""")

# 4.3 ModÃ¨le avec Early Stopping
print("\nğŸš€ EntraÃ®nement avec EARLY STOPPING...\n")

# RecrÃ©er modÃ¨le (rÃ©initialiser poids)
model_early_stop = create_simple_model(X_train_scaled.shape[1])
model_early_stop.compile(loss='mse', optimizer='adam', metrics=['mae'])

# EarlyStopping : ArrÃªte si val_loss ne s'amÃ©liore plus
early_stop_callback = callbacks.EarlyStopping(
    monitor='val_loss',  # MÃ©trique Ã  surveiller
    patience=15,  # Attendre N epochs sans amÃ©lioration
    restore_best_weights=True,  # Restaurer meilleurs poids
    verbose=1
)

# ReduceLROnPlateau : RÃ©duit learning rate si plateau
reduce_lr_callback = callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,  # Diviser LR par 2
    patience=10,
    min_lr=1e-7,
    verbose=1
)

history_early_stop = model_early_stop.fit(
    X_train_scaled, y_train,
    validation_data=(X_val_scaled, y_val),
    epochs=200,  # Beaucoup car early stopping arrÃªtera
    batch_size=32,
    callbacks=[early_stop_callback, reduce_lr_callback],
    verbose=0
)

print(f"\nâœ“ EntraÃ®nement arrÃªtÃ© epoch {len(history_early_stop.history['loss'])}")

fig = plot_training_history(history_early_stop, "Avec Early Stopping")
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/nn_training_early_stop.png', dpi=100)
plt.show()

print("âœ“ Graphique sauvegardÃ© : nn_training_early_stop.png")

print(f"""
ğŸ” OBSERVATION #4 : Early Stopping
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ArrÃªtÃ© epoch {len(history_early_stop.history['loss'])} / 200

CE QU'IL FAUT OBSERVER :

1. EPOCH D'ARRÃŠT :
   - ArrÃªt prÃ©coce (<50) : Convergence rapide OU underfitting
   - ArrÃªt moyen (50-150) : OPTIMAL
   - ArrÃªt tardif (>150) : ModÃ¨le complexe ou learning rate trop petit

2. VAL LOSS FINALE :
   Sans early stop : {history_simple.history['val_loss'][-1]:.4f} (epoch 100)
   Avec early stop : {min(history_early_stop.history['val_loss']):.4f} (epoch {np.argmin(history_early_stop.history['val_loss']) + 1})

   AmÃ©lioration : {((history_simple.history['val_loss'][-1] - min(history_early_stop.history['val_loss'])) / history_simple.history['val_loss'][-1] * 100):.1f}%

3. REDUCE LR ON PLATEAU :
   Learning rate rÃ©duit si val_loss stagne
   â†’ Permet convergence fine
   â†’ Visible sur courbes (accÃ©lÃ©ration aprÃ¨s rÃ©duction)

ğŸ’¡ CONCLUSION :
   Early Stopping est ESSENTIEL :
   âœ… Ã‰vite overfitting automatiquement
   âœ… Ã‰conomise temps (arrÃªte quand inutile continuer)
   âœ… Trouve epoch optimal automatiquement

   TOUJOURS utiliser en production !

âš™ï¸  TUNING :
   - patience=15 : Standard pour petites donnÃ©es
   - patience=3-5 : Grandes donnÃ©es, entraÃ®nement long
   - patience=20-50 : TrÃ¨s petites donnÃ©es, convergence lente
""")

input("\nâ–¶ Appuyez sur EntrÃ©e pour continuer...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 5 : RÃ‰GULARISATION AVANCÃ‰E
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 5 : RÃ‰GULARISATION - COMBATTRE L'OVERFITTING")
print("="*80)

print("""
ğŸ›¡ï¸  TECHNIQUES DE RÃ‰GULARISATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1ï¸âƒ£  DROPOUT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DÃ©sactive alÃ©atoirement neurons pendant entraÃ®nement.

PRINCIPE :
   Layer avec 100 neurons, Dropout(0.3)
   â†’ 30 neurons dÃ©sactivÃ©s alÃ©atoirement Ã  chaque batch
   â†’ Force rÃ©seau Ã  ne pas dÃ©pendre d'un neuron spÃ©cifique

âœ… AVANTAGES :
   - TrÃ¨s efficace contre overfitting
   - Agit comme ensemble de rÃ©seaux

TAUX RECOMMANDÃ‰S :
   - 0.2-0.3 : LÃ©ger (hidden layers)
   - 0.5 : Fort (rarement utilisÃ© maintenant)
   - 0.1 : TrÃ¨s lÃ©ger

ğŸ’¼ USAGE : Entre chaque dense layer (sauf output)


2ï¸âƒ£  L1 / L2 REGULARIZATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PÃ©nalise poids trop grands.

L1 (Lasso) : loss + Î»Â·Î£|w|
   â†’ Met certains poids Ã  0
   â†’ SÃ©lection de features

L2 (Ridge) : loss + Î»Â·Î£wÂ²
   â†’ RÃ©duit tous les poids
   â†’ Ã‰vite poids extrÃªmes

Î» (paramÃ¨tre) :
   - 0.0001 : LÃ©ger
   - 0.001 : Moyen
   - 0.01 : Fort

ğŸ’¼ USAGE : Dans les Dense layers


3ï¸âƒ£  BATCH NORMALIZATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Normalise inputs de chaque layer.

PRINCIPE :
   Output du layer normalisÃ© (mean=0, std=1)
   â†’ AccÃ©lÃ¨re entraÃ®nement
   â†’ RÃ©gularisation (effet similaire Dropout lÃ©ger)

âœ… AVANTAGES :
   - EntraÃ®nement 2-3Ã— plus rapide
   - Permet learning rate plus Ã©levÃ©s
   - RÃ©duit sensibilitÃ© Ã  l'initialisation

âš ï¸  ATTENTION :
   Complexifie le modÃ¨le
   Pas toujours nÃ©cessaire sur petites donnÃ©es

ğŸ’¼ USAGE : AprÃ¨s Dense layer, avant activation


4ï¸âƒ£  EARLY STOPPING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ArrÃªte entraÃ®nement quand val_loss ne s'amÃ©liore plus.

âœ… AVANTAGES :
   - Simple et trÃ¨s efficace
   - Pas de calcul supplÃ©mentaire

ğŸ’¼ USAGE : TOUJOURS !


5ï¸âƒ£  DATA AUGMENTATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CrÃ©e variations artificielles des donnÃ©es.

Pour images : rotation, flip, zoom, etc.
Pour tabulaire : bruit, perturbations lÃ©gÃ¨res

ğŸ’¼ USAGE : Images surtout


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Technique           â”‚ EfficacitÃ© â”‚ CoÃ»t       â”‚ Usage        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dropout             â”‚ â˜…â˜…â˜…â˜…â˜…      â”‚ Faible     â”‚ TOUJOURS     â”‚
â”‚ Early Stopping      â”‚ â˜…â˜…â˜…â˜…â˜…      â”‚ Nul        â”‚ TOUJOURS     â”‚
â”‚ L2                  â”‚ â˜…â˜…â˜…â˜†â˜†      â”‚ Nul        â”‚ Si overfittingâ”‚
â”‚ Batch Norm          â”‚ â˜…â˜…â˜…â˜…â˜†      â”‚ Moyen      â”‚ Grands rÃ©seauxâ”‚
â”‚ Data Augmentation   â”‚ â˜…â˜…â˜…â˜…â˜…      â”‚ Moyen      â”‚ Images       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ STRATÃ‰GIE RECOMMANDÃ‰E :
   1. TOUJOURS : Early Stopping
   2. Si overfitting : Ajouter Dropout (0.2-0.3)
   3. Si encore overfit : Ajouter L2 (0.001)
   4. Si grands rÃ©seaux : Batch Normalization
""")

# 5.1 ModÃ¨le avec rÃ©gularisation complÃ¨te
print("\nğŸ—ï¸  Construction modÃ¨le avec RÃ‰GULARISATION COMPLÃˆTE...\n")

def create_regularized_model(input_dim):
    """
    ModÃ¨le avec toutes les rÃ©gularisations
    """
    model = models.Sequential([
        # Layer 1
        layers.Dense(
            128,
            activation='relu',
            kernel_regularizer=regularizers.l2(0.001),  # L2
            input_dim=input_dim,
            name='hidden1'
        ),
        layers.BatchNormalization(name='bn1'),
        layers.Dropout(0.3, name='dropout1'),

        # Layer 2
        layers.Dense(
            64,
            activation='relu',
            kernel_regularizer=regularizers.l2(0.001),
            name='hidden2'
        ),
        layers.BatchNormalization(name='bn2'),
        layers.Dropout(0.2, name='dropout2'),

        # Layer 3
        layers.Dense(
            32,
            activation='relu',
            name='hidden3'
        ),

        # Output
        layers.Dense(1, activation='linear', name='output')
    ])

    return model

model_regularized = create_regularized_model(X_train_scaled.shape[1])

model_regularized.compile(
    loss='mse',
    optimizer=optimizers.Adam(learning_rate=0.001),
    metrics=['mae']
)

print(model_regularized.summary())

# EntraÃ®ner
history_regularized = model_regularized.fit(
    X_train_scaled, y_train,
    validation_data=(X_val_scaled, y_val),
    epochs=200,
    batch_size=32,
    callbacks=[early_stop_callback, reduce_lr_callback],
    verbose=0
)

print(f"âœ“ EntraÃ®nement terminÃ© (epoch {len(history_regularized.history['loss'])})")

# Comparer toutes les versions
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Loss
axes[0].plot(history_simple.history['val_loss'], label='Simple', linewidth=2, alpha=0.7)
axes[0].plot(history_early_stop.history['val_loss'], label='Early Stop', linewidth=2, alpha=0.7)
axes[0].plot(history_regularized.history['val_loss'], label='RÃ©gularisÃ©', linewidth=2, alpha=0.7)
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Validation Loss')
axes[0].set_title('Comparaison : Validation Loss')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# MAE
axes[1].plot(history_simple.history['val_mae'], label='Simple', linewidth=2, alpha=0.7)
axes[1].plot(history_early_stop.history['val_mae'], label='Early Stop', linewidth=2, alpha=0.7)
axes[1].plot(history_regularized.history['val_mae'], label='RÃ©gularisÃ©', linewidth=2, alpha=0.7)
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Validation MAE')
axes[1].set_title('Comparaison : Validation MAE')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/nn_comparison.png', dpi=100)
plt.show()

print("\nâœ“ Graphique sauvegardÃ© : nn_comparison.png")

# MÃ©triques finales
models_comparison = {
    'Simple': (model_simple, history_simple),
    'Early Stop': (model_early_stop, history_early_stop),
    'RÃ©gularisÃ©': (model_regularized, history_regularized)
}

print(f"""
ğŸ” OBSERVATION #5 : Impact de la RÃ©gularisation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CE QU'IL FAUT OBSERVER :

1. VAL LOSS FINALE :
   Simple : {history_simple.history['val_loss'][-1]:.4f}
   Early Stop : {min(history_early_stop.history['val_loss']):.4f}
   RÃ©gularisÃ© : {min(history_regularized.history['val_loss']):.4f}

2. STABILITÃ‰ :
   Courbes rÃ©gularisÃ©es gÃ©nÃ©ralement plus LISSES
   â†’ Moins d'oscillations
   â†’ Convergence plus stable

3. OVERFITTING :
   Ã‰cart Train vs Val :
   - Simple peut avoir grand Ã©cart
   - RÃ©gularisÃ© devrait avoir Ã©cart RÃ‰DUIT

ğŸ’¡ INTERPRÃ‰TATION :
   Si RÃ©gularisÃ© >> Simple :
   â†’ RÃ©gularisation EFFICACE, nÃ©cessaire

   Si RÃ©gularisÃ© â‰ˆ Simple :
   â†’ RÃ©gularisation inutile, modÃ¨le simple suffisait
   â†’ Ou donnÃ©es suffisantes (pas besoin rÃ©gularisation)

   Si RÃ©gularisÃ© < Simple :
   â†’ Sur-rÃ©gularisation ! ModÃ¨le bloquÃ© (underfitting)
   â†’ RÃ©duire dropout ou L2

ğŸ¯ CONCLUSION :
   RÃ©gularisation nÃ©cessaire si et seulement si overfitting.
   Sur petites donnÃ©es : Crucial
   Sur grandes donnÃ©es : Souvent moins nÃ©cessaire

âš™ï¸  TUNING RÃ‰GULARISATION :
   Trop d'overfitting :
   1. Augmenter Dropout (0.2 â†’ 0.4)
   2. Augmenter L2 (0.001 â†’ 0.01)
   3. RÃ©duire taille du modÃ¨le

   Underfitting :
   1. RÃ©duire Dropout (0.4 â†’ 0.2)
   2. RÃ©duire L2 (0.01 â†’ 0.001)
   3. Augmenter taille du modÃ¨le
""")

input("\nâ–¶ Appuyez sur EntrÃ©e pour l'Ã©valuation finale...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 6 : Ã‰VALUATION FINALE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 6 : Ã‰VALUATION FINALE ET COMPARAISON")
print("="*80)

# Ã‰valuer tous les modÃ¨les sur test set
results = []

for name, (model, history) in models_comparison.items():
    y_pred = model.predict(X_test_scaled, verbose=0).flatten()

    test_mse = mean_squared_error(y_test, y_pred)
    test_rmse = np.sqrt(test_mse)
    test_mae = mean_absolute_error(y_test, y_pred)
    test_r2 = r2_score(y_test, y_pred)

    results.append({
        'ModÃ¨le': name,
        'MSE': test_mse,
        'RMSE': test_rmse,
        'MAE': test_mae,
        'RÂ²': test_r2,
        'Params': model.count_params()
    })

results_df = pd.DataFrame(results)

print("\nğŸ“Š RÃ‰SULTATS SUR TEST SET\n")
print(results_df.to_string(index=False))

# Meilleur modÃ¨le
best_model_name = results_df.loc[results_df['RÂ²'].idxmax(), 'ModÃ¨le']
best_model = models_comparison[best_model_name][0]

print(f"\nğŸ† Meilleur modÃ¨le : {best_model_name}")

# Visualisation prÃ©dictions
y_test_pred = best_model.predict(X_test_scaled, verbose=0).flatten()

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Scatter pred vs real
axes[0].scatter(y_test, y_test_pred, alpha=0.6, edgecolors='k')
axes[0].plot([y_test.min(), y_test.max()],
             [y_test.min(), y_test.max()],
             'r--', lw=2, label='PrÃ©diction parfaite')
axes[0].set_xlabel('Prix RÃ©el (normalisÃ©)')
axes[0].set_ylabel('Prix PrÃ©dit')
axes[0].set_title(f'PrÃ©dictions vs RÃ©alitÃ©\n{best_model_name} (RÂ² = {results_df.loc[results_df["ModÃ¨le"] == best_model_name, "RÂ²"].values[0]:.4f})')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# RÃ©sidus
residuals = y_test - y_test_pred
axes[1].hist(residuals, bins=30, edgecolor='black', alpha=0.7)
axes[1].axvline(0, color='r', linestyle='--', lw=2)
axes[1].set_xlabel('RÃ©sidus')
axes[1].set_ylabel('FrÃ©quence')
axes[1].set_title(f'Distribution des RÃ©sidus\n(MAE = {results_df.loc[results_df["ModÃ¨le"] == best_model_name, "MAE"].values[0]:.4f})')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/nn_final_results.png', dpi=100)
plt.show()

print("\nâœ“ Graphique sauvegardÃ© : nn_final_results.png")

print(f"""
ğŸ” OBSERVATION #6 : Performance Finale
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CE QU'IL FAUT OBSERVER :

1. RÂ² SCORE :
   RÂ² = {results_df.loc[results_df['ModÃ¨le'] == best_model_name, 'RÂ²'].values[0]:.4f}

   InterprÃ©tation :
   - RÂ² > 0.9 : EXCELLENT
   - RÂ² 0.7-0.9 : BON
   - RÂ² 0.5-0.7 : ACCEPTABLE
   - RÂ² < 0.5 : FAIBLE

2. SCATTER PLOT :
   Points proches de la ligne rouge = Bonnes prÃ©dictions
   Dispersion = Erreur de prÃ©diction

   Biais visible (systÃ©matiquement au-dessus/en-dessous) ?
   â†’ Non : ModÃ¨le non biaisÃ© âœ…
   â†’ Oui : ProblÃ¨me dans les donnÃ©es ou modÃ¨le

3. RÃ‰SIDUS :
   Distribution centrÃ©e sur 0 ? âœ… Pas de biais
   Forme gaussienne ? âœ… Erreurs alÃ©atoires
   AsymÃ©trique ou multi-modale ? âŒ ProblÃ¨me

4. COMPARAISON DES MODÃˆLES :
   AmÃ©lioration Simple â†’ RÃ©gularisÃ© :
   RÂ² : {results_df.loc[results_df['ModÃ¨le'] == 'Simple', 'RÂ²'].values[0]:.4f} â†’ {results_df.loc[results_df['ModÃ¨le'] == best_model_name, 'RÂ²'].values[0]:.4f}

   {"âœ… RÃ©gularisation utile" if results_df.loc[results_df['ModÃ¨le'] == best_model_name, 'RÂ²'].values[0] > results_df.loc[results_df['ModÃ¨le'] == 'Simple', 'RÂ²'].values[0] + 0.02 else "âš ï¸ RÃ©gularisation peu d'impact"}

ğŸ’¡ CONCLUSION :
   Neural Network performant sur ce problÃ¨me de rÃ©gression.

   Comparaison avec modÃ¨les classiques recommandÃ©e :
   - XGBoost probablement similaire ou meilleur sur tabulaire
   - Linear Regression si relations linÃ©aires

   NN justifiÃ© si :
   âœ… Relations trÃ¨s non-linÃ©aires
   âœ… Beaucoup de donnÃ©es (>10k)
   âœ… Besoin de flexibilitÃ© architecturale

ğŸ¯ EN PRODUCTION :
   Avantages NN :
   âœ… Flexible, personnalisable
   âœ… Transfer learning possible
   âœ… IntÃ©gration facile (TensorFlow Serving)

   InconvÃ©nients NN :
   âŒ "BoÃ®te noire"
   âŒ Plus lent que modÃ¨les classiques
   âŒ Plus de maintenance
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTIE 7 : SAUVEGARDE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("PARTIE 7 : SAUVEGARDE DU MODÃˆLE")
print("="*80)

# Sauvegarder modÃ¨le Keras
best_model.save('e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/best_nn_model.h5')

# Sauvegarder scaler
import joblib
joblib.dump(scaler, 'e:/Nicolas/MIAGE/M2/BigData/FORMATION_ML/TUTORIELS/nn_scaler.pkl')

print("\nâœ“ ModÃ¨le sauvegardÃ© : best_nn_model.h5")
print("âœ“ Scaler sauvegardÃ© : nn_scaler.pkl")

print("""
ğŸ“¦ UTILISATION EN PRODUCTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```python
import tensorflow as tf
import joblib
import numpy as np

# Charger
model = tf.keras.models.load_model('best_nn_model.h5')
scaler = joblib.load('nn_scaler.pkl')

# Nouvelle propriÃ©tÃ©
nouvelle_propriete = np.array([[
    120,   # Surface
    3,     # Chambres
    360,   # Surface Ã— Chambres
    2015,  # AnnÃ©e construction
    # ... autres features
]])

# Normaliser
nouvelle_propriete_scaled = scaler.transform(nouvelle_propriete)

# PrÃ©dire
prix_predit_normalized = model.predict(nouvelle_propriete_scaled)[0, 0]

# DÃ©normaliser (si target normalisÃ©e)
# prix_predit = prix_predit_normalized * target_std + target_mean

print(f"Prix prÃ©dit : {prix_predit_normalized:.2f}")
```

âš™ï¸  OPTIMISATION INFÃ‰RENCE :
   Pour production haute performance :
   - Convertir en TensorFlow Lite (mobile)
   - Utiliser TensorFlow Serving (serveur)
   - Quantization pour rÃ©duire taille
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RÃ‰SUMÃ‰ ET CONCLUSIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("ğŸ‰ RÃ‰SUMÃ‰ ET CONCLUSIONS")
print("="*80)

print("""
ğŸ“š CE QUE NOUS AVONS APPRIS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1ï¸âƒ£  QUAND UTILISER NEURAL NETWORKS
   âœ… Beaucoup de donnÃ©es (>10k)
   âœ… Relations complexes non-linÃ©aires
   âœ… Images, texte, sÃ©quences
   âœ… Besoin de flexibilitÃ©
   âŒ Petites donnÃ©es tabulaires â†’ XGBoost meilleur

2ï¸âƒ£  ARCHITECTURE
   ğŸ—ï¸  Start simple : 1-2 hidden layers
   ğŸ—ï¸  Structure entonnoir : 128 â†’ 64 â†’ 32
   ğŸ—ï¸  Activation : ReLU (hidden), Linear (output rÃ©gression)
   ğŸ—ï¸  Nombre params â‰ˆ Nb Ã©chantillons / 10

3ï¸âƒ£  OPTIMISATION
   âš™ï¸  Optimizer : ADAM par dÃ©faut (0.001)
   âš™ï¸  Batch size : 32 ou 64 (standard)
   âš™ï¸  Epochs : Beaucoup + Early Stopping
   âš™ï¸  Learning rate : 0.001 (dÃ©faut), ajuster si besoin

4ï¸âƒ£  RÃ‰GULARISATION
   ğŸ›¡ï¸  TOUJOURS : Early Stopping
   ğŸ›¡ï¸  Si overfitting : Dropout (0.2-0.3)
   ğŸ›¡ï¸  Si encore overfit : L2 (0.001)
   ğŸ›¡ï¸  Grands rÃ©seaux : Batch Normalization

5ï¸âƒ£  DIAGNOSTIC
   ğŸ“Š Courbes d'apprentissage : ESSENTIEL
   ğŸ“Š Train << Val : Overfitting
   ğŸ“Š Train et Val Ã©levÃ©s : Underfitting
   ğŸ“Š RÃ©sidus centrÃ©s sur 0 : Pas de biais

6ï¸âƒ£  COMPARAISON AVEC MODÃˆLES CLASSIQUES
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ CritÃ¨re        â”‚ NN       â”‚ XGBoost  â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Tabulaire      â”‚ â˜…â˜…â˜…â˜†â˜†    â”‚ â˜…â˜…â˜…â˜…â˜…    â”‚
   â”‚ Images/Texte   â”‚ â˜…â˜…â˜…â˜…â˜…    â”‚ â˜…â˜†â˜†â˜†â˜†    â”‚
   â”‚ Setup          â”‚ â˜…â˜…â˜†â˜†â˜†    â”‚ â˜…â˜…â˜…â˜…â˜†    â”‚
   â”‚ InterprÃ©tation â”‚ â˜…â˜†â˜†â˜†â˜†    â”‚ â˜…â˜…â˜…â˜†â˜†    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… CHECKLIST NEURAL NETWORKS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ“ DonnÃ©es normalisÃ©es (CRUCIAL)
âœ“ Architecture simple au dÃ©part
âœ“ Optimizer Adam (dÃ©faut)
âœ“ Early Stopping configurÃ©
âœ“ Dropout si overfitting
âœ“ Courbes d'apprentissage vÃ©rifiÃ©es
âœ“ Test set Ã©valuÃ© UNE FOIS
âœ“ ModÃ¨le sauvegardÃ© (.h5)

ğŸ¯ RÃˆGLES D'OR
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. "TOUJOURS normaliser les donnÃ©es"
2. "Start simple, scale up si underfitting"
3. "Early Stopping est non-nÃ©gociable"
4. "Courbes d'apprentissage = Diagnostic #1"
5. "RÃ©gulariser seulement si overfitting"

ğŸš€ PROCHAINES Ã‰TAPES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. CNN pour images (Tutoriel 5)
2. RNN pour sÃ©ries temporelles
3. Transfer Learning
4. Hyperparameter tuning avancÃ© (Keras Tuner)
5. InterprÃ©tabilitÃ© (SHAP, LIME)
6. DÃ©ploiement (TensorFlow Serving)

ğŸ’¡ BONUS : HYPERPARAMÃˆTRES Ã€ TUNER (PAR PRIORITÃ‰)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Learning rate (0.0001 - 0.01)
2. Architecture (nombre layers/neurons)
3. Dropout rate (0.1 - 0.5)
4. Batch size (16, 32, 64, 128)
5. Optimizer (Adam, SGD+momentum, RMSprop)
6. L2 regularization (0.0001 - 0.01)
7. Batch Normalization (oui/non)
8. Activation function (ReLU, LeakyReLU, tanh)

ğŸ”§ DEBUGGING TIPS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âŒ Loss ne diminue pas :
   â†’ Learning rate trop petit (augmenter)
   â†’ Architecture trop simple (ajouter layers)
   â†’ DonnÃ©es mal normalisÃ©es (vÃ©rifier scaler)

âŒ Loss explose (NaN) :
   â†’ Learning rate trop grand (diminuer)
   â†’ Gradient exploding (ajouter gradient clipping)
   â†’ DonnÃ©es mal normalisÃ©es

âŒ Overfitting fort :
   â†’ Dropout (0.3-0.5)
   â†’ L2 regularization
   â†’ RÃ©duire taille modÃ¨le
   â†’ Plus de donnÃ©es / Data augmentation

âŒ Underfitting :
   â†’ Augmenter taille modÃ¨le
   â†’ EntraÃ®ner plus longtemps
   â†’ RÃ©duire rÃ©gularisation
   â†’ VÃ©rifier que donnÃ©es informatives
""")

print("="*80)
print("âœ¨ TUTORIEL TERMINÃ‰ AVEC SUCCÃˆS ! âœ¨")
print("="*80)
print("\nğŸ§  Vous maÃ®trisez maintenant les Neural Networks !")
print("ğŸ“š Prochain tutoriel : CNN pour traitement d'images")
