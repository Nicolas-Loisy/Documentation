# Workflows ML : Construire, Optimiser, Valider et Tester

## ğŸ“‹ Table des MatiÃ¨res

1. [Workflow Complet d'un Projet ML](#workflow-complet-dun-projet-ml)
2. [Workflow de Construction d'un ModÃ¨le](#workflow-de-construction-dun-modÃ¨le)
3. [Workflow d'Optimisation](#workflow-doptimisation)
4. [Workflow de Validation](#workflow-de-validation)
5. [Workflow de Deep Learning](#workflow-de-deep-learning)
6. [Pipeline de Production](#pipeline-de-production)
7. [Diagrammes de DÃ©cision](#diagrammes-de-dÃ©cision)

---

## Workflow Complet d'un Projet ML

### Vue d'ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     WORKFLOW PROJET ML COMPLET                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. COMPRÃ‰HENSION DU PROBLÃˆME
   â”‚
   â”œâ”€ DÃ©finir la problÃ©matique mÃ©tier
   â”œâ”€ Identifier le type de problÃ¨me ML
   â”œâ”€ DÃ©finir les objectifs mesurables
   â””â”€ Identifier les contraintes
   â”‚
   â†“
2. COLLECTE ET EXPLORATION DES DONNÃ‰ES (EDA)
   â”‚
   â”œâ”€ Identifier les sources de donnÃ©es
   â”œâ”€ Collecter les donnÃ©es
   â”œâ”€ Analyser statistiques descriptives
   â”œâ”€ Visualiser les distributions
   â”œâ”€ Identifier valeurs manquantes, outliers
   â””â”€ Analyser les corrÃ©lations
   â”‚
   â†“
3. PRÃ‰PARATION DES DONNÃ‰ES
   â”‚
   â”œâ”€ Traiter valeurs manquantes
   â”œâ”€ GÃ©rer les outliers
   â”œâ”€ Feature Engineering
   â”œâ”€ Encoder variables catÃ©gorielles
   â”œâ”€ Normaliser/Standardiser
   â””â”€ Split Train/Val/Test
   â”‚
   â†“
4. MODÃ‰LISATION
   â”‚
   â”œâ”€ DÃ©finir baseline
   â”œâ”€ Tester plusieurs modÃ¨les
   â”œâ”€ SÃ©lectionner le meilleur
   â”œâ”€ Optimiser hyperparamÃ¨tres
   â””â”€ Valider avec cross-validation
   â”‚
   â†“
5. Ã‰VALUATION
   â”‚
   â”œâ”€ Ã‰valuer sur test set
   â”œâ”€ Calculer mÃ©triques
   â”œâ”€ Analyser les erreurs
   â””â”€ InterprÃ©ter le modÃ¨le
   â”‚
   â†“
6. DÃ‰PLOIEMENT
   â”‚
   â”œâ”€ SÃ©rialiser le modÃ¨le
   â”œâ”€ CrÃ©er API
   â”œâ”€ Dockeriser
   â”œâ”€ Monitoring
   â””â”€ Maintenance
```

---

## Workflow de Construction d'un ModÃ¨le

### Ã‰tapes DÃ©taillÃ©es

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# WORKFLOW DE CONSTRUCTION D'UN MODÃˆLE ML
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 1 : CHARGEMENT DES DONNÃ‰ES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("="*80)
print("Ã‰TAPE 1 : CHARGEMENT DES DONNÃ‰ES")
print("="*80)

df = pd.read_csv('data.csv')
print(f"âœ“ DonnÃ©es chargÃ©es : {df.shape[0]} lignes Ã— {df.shape[1]} colonnes")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 2 : EXPLORATION DES DONNÃ‰ES (EDA)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "="*80)
print("Ã‰TAPE 2 : EXPLORATION DES DONNÃ‰ES")
print("="*80)

# 2.1 AperÃ§u gÃ©nÃ©ral
print("\nğŸ“Š AperÃ§u des donnÃ©es :")
print(df.head())
print("\nğŸ“ˆ Informations gÃ©nÃ©rales :")
print(df.info())
print("\nğŸ“‰ Statistiques descriptives :")
print(df.describe())

# 2.2 Valeurs manquantes
print("\nâ“ Valeurs manquantes :")
missing = df.isnull().sum()
missing_pct = 100 * missing / len(df)
missing_table = pd.DataFrame({
    'Manquantes': missing,
    'Pourcentage': missing_pct
})
print(missing_table[missing_table['Manquantes'] > 0])

# 2.3 Distribution de la variable cible
print("\nğŸ¯ Distribution de la variable cible :")
print(df['target'].value_counts())

# 2.4 CorrÃ©lations
print("\nğŸ”— CorrÃ©lations avec la cible :")
correlations = df.corr()['target'].sort_values(ascending=False)
print(correlations)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 3 : PRÃ‰PARATION DES DONNÃ‰ES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "="*80)
print("Ã‰TAPE 3 : PRÃ‰PARATION DES DONNÃ‰ES")
print("="*80)

# 3.1 SÃ©parer features et target
X = df.drop('target', axis=1)
y = df['target']
print(f"âœ“ Features : {X.shape}")
print(f"âœ“ Target : {y.shape}")

# 3.2 Traiter valeurs manquantes
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='median')
X_imputed = pd.DataFrame(
    imputer.fit_transform(X),
    columns=X.columns
)
print(f"âœ“ Valeurs manquantes traitÃ©es")

# 3.3 Encoder variables catÃ©gorielles
cat_cols = X_imputed.select_dtypes(include=['object']).columns
if len(cat_cols) > 0:
    X_encoded = pd.get_dummies(X_imputed, columns=cat_cols, drop_first=True)
    print(f"âœ“ Variables catÃ©gorielles encodÃ©es : {len(cat_cols)} colonnes")
else:
    X_encoded = X_imputed

# 3.4 Split Train/Val/Test
X_train, X_temp, y_train, y_temp = train_test_split(
    X_encoded, y, test_size=0.3, random_state=42, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

print(f"\nâœ“ DonnÃ©es divisÃ©es :")
print(f"  Train : {len(X_train)} ({len(X_train)/len(X_encoded)*100:.1f}%)")
print(f"  Val   : {len(X_val)} ({len(X_val)/len(X_encoded)*100:.1f}%)")
print(f"  Test  : {len(X_test)} ({len(X_test)/len(X_encoded)*100:.1f}%)")

# 3.5 Normalisation
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)
print(f"âœ“ DonnÃ©es normalisÃ©es")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 4 : BASELINE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "="*80)
print("Ã‰TAPE 4 : BASELINE")
print("="*80)

from sklearn.dummy import DummyClassifier

baseline = DummyClassifier(strategy='most_frequent')
baseline.fit(X_train_scaled, y_train)
baseline_score = baseline.score(X_val_scaled, y_val)

print(f"ğŸ“Š Baseline (most_frequent) : {baseline_score:.4f}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 5 : MODÃ‰LISATION - TESTER PLUSIEURS MODÃˆLES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "="*80)
print("Ã‰TAPE 5 : MODÃ‰LISATION")
print("="*80)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
}

results = {}

print("\nğŸ¤– EntraÃ®nement des modÃ¨les...\n")
for name, model in models.items():
    # EntraÃ®ner
    model.fit(X_train_scaled, y_train)

    # PrÃ©dire
    y_train_pred = model.predict(X_train_scaled)
    y_val_pred = model.predict(X_val_scaled)

    # Scores
    train_score = accuracy_score(y_train, y_train_pred)
    val_score = accuracy_score(y_val, y_val_pred)

    # Cross-validation
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)

    results[name] = {
        'model': model,
        'train_score': train_score,
        'val_score': val_score,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std()
    }

    print(f"{name}")
    print(f"  Train    : {train_score:.4f}")
    print(f"  Val      : {val_score:.4f}")
    print(f"  CV       : {cv_scores.mean():.4f} (Â± {cv_scores.std():.4f})")
    print(f"  Overfit  : {train_score - val_score:.4f}")
    print()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 6 : SÃ‰LECTION DU MEILLEUR MODÃˆLE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("="*80)
print("Ã‰TAPE 6 : SÃ‰LECTION DU MEILLEUR MODÃˆLE")
print("="*80)

best_model_name = max(results, key=lambda k: results[k]['val_score'])
best_model = results[best_model_name]['model']

print(f"\nğŸ† Meilleur modÃ¨le : {best_model_name}")
print(f"   Val Score : {results[best_model_name]['val_score']:.4f}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 7 : OPTIMISATION DES HYPERPARAMÃˆTRES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "="*80)
print("Ã‰TAPE 7 : OPTIMISATION DES HYPERPARAMÃˆTRES")
print("="*80)

from sklearn.model_selection import GridSearchCV

# DÃ©finir grille selon le modÃ¨le
if best_model_name == 'XGBoost':
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [3, 5, 7],
        'learning_rate': [0.01, 0.1],
    }
elif best_model_name == 'Random Forest':
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5],
    }
else:
    param_grid = {}

if param_grid:
    print(f"\nğŸ”§ Optimisation de {best_model_name}...\n")

    grid_search = GridSearchCV(
        best_model,
        param_grid,
        cv=5,
        scoring='accuracy',
        n_jobs=-1,
        verbose=1
    )

    grid_search.fit(X_train_scaled, y_train)

    print(f"\nâœ“ Meilleurs paramÃ¨tres : {grid_search.best_params_}")
    print(f"âœ“ Meilleur score CV : {grid_search.best_score_:.4f}")

    best_model = grid_search.best_estimator_

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 8 : Ã‰VALUATION FINALE SUR TEST SET
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "="*80)
print("Ã‰TAPE 8 : Ã‰VALUATION FINALE")
print("="*80)

y_test_pred = best_model.predict(X_test_scaled)
test_score = accuracy_score(y_test, y_test_pred)

print(f"\nğŸ“Š Score final sur test set : {test_score:.4f}")
print(f"\nğŸ“‹ Classification Report :\n")
print(classification_report(y_test, y_test_pred))

# Matrice de confusion
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_test_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('PrÃ©diction')
plt.ylabel('RÃ©alitÃ©')
plt.title('Matrice de Confusion')
plt.show()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 9 : SAUVEGARDE DU MODÃˆLE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "="*80)
print("Ã‰TAPE 9 : SAUVEGARDE")
print("="*80)

import joblib

# Sauvegarder le modÃ¨le et le scaler
joblib.dump(best_model, 'best_model.pkl')
joblib.dump(scaler, 'scaler.pkl')

print("âœ“ ModÃ¨le sauvegardÃ© : best_model.pkl")
print("âœ“ Scaler sauvegardÃ© : scaler.pkl")

print("\n" + "="*80)
print("ğŸ‰ WORKFLOW TERMINÃ‰ AVEC SUCCÃˆS !")
print("="*80)
```

---

## Workflow d'Optimisation

### Diagramme d'Optimisation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  WORKFLOW D'OPTIMISATION                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. MODÃˆLE INITIAL
   â”‚
   â†“
2. DIAGNOSTIC
   â”‚
   â”œâ”€ Overfitting ? (train >> val)
   â”‚  â””â”€ OUI â†’ [A] RÃ©duire complexitÃ©
   â”‚            - Diminuer max_depth (trees)
   â”‚            - Augmenter rÃ©gularisation (L1/L2)
   â”‚            - Dropout (neural networks)
   â”‚            - Moins de features
   â”‚            - Plus de donnÃ©es
   â”‚
   â”œâ”€ Underfitting ? (train et val faibles)
   â”‚  â””â”€ OUI â†’ [B] Augmenter complexitÃ©
   â”‚            - Augmenter max_depth
   â”‚            - Plus de features
   â”‚            - ModÃ¨le plus complexe
   â”‚            - Diminuer rÃ©gularisation
   â”‚
   â””â”€ Performance OK mais amÃ©liorable ?
      â””â”€ OUI â†’ [C] Optimisation fine
                - Grid Search / Random Search
                - Bayesian Optimization
                - Feature Engineering
                - Ensemble methods
   â”‚
   â†“
3. APPLIQUER MODIFICATIONS
   â”‚
   â†“
4. Ã‰VALUER
   â”‚
   â”œâ”€ AmÃ©lioration ? â†’ Continuer
   â””â”€ Pas d'amÃ©lioration ? â†’ Retour au meilleur modÃ¨le
   â”‚
   â†“
5. VALIDATION FINALE
```

### Code d'Optimisation ComplÃ¨te

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# WORKFLOW D'OPTIMISATION COMPLÃˆTE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import numpy as np
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import make_scorer

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 1 : DIAGNOSTIC
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def diagnostic_modele(model, X_train, y_train, X_val, y_val):
    """
    Diagnostique overfitting/underfitting
    """
    train_score = model.score(X_train, y_train)
    val_score = model.score(X_val, y_val)

    print("="*60)
    print("DIAGNOSTIC DU MODÃˆLE")
    print("="*60)
    print(f"Score Train : {train_score:.4f}")
    print(f"Score Val   : {val_score:.4f}")
    print(f"DiffÃ©rence  : {train_score - val_score:.4f}")

    if train_score - val_score > 0.1:
        print("\nâš ï¸  OVERFITTING DÃ‰TECTÃ‰")
        print("Recommandations :")
        print("  - Augmenter rÃ©gularisation")
        print("  - Diminuer complexitÃ© du modÃ¨le")
        print("  - Ajouter plus de donnÃ©es")
        print("  - Feature selection")
        print("  - Early stopping")
        return "overfitting"

    elif train_score < 0.7 and val_score < 0.7:
        print("\nâš ï¸  UNDERFITTING DÃ‰TECTÃ‰")
        print("Recommandations :")
        print("  - Augmenter complexitÃ© du modÃ¨le")
        print("  - Ajouter plus de features")
        print("  - Diminuer rÃ©gularisation")
        print("  - Feature engineering")
        return "underfitting"

    else:
        print("\nâœ“ ModÃ¨le Ã©quilibrÃ©")
        print("Recommandations :")
        print("  - Optimiser hyperparamÃ¨tres")
        print("  - Feature engineering avancÃ©")
        print("  - Ensemble methods")
        return "balanced"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 2 : OPTIMISATION HYPERPARAMÃˆTRES - GRID SEARCH
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def optimisation_grid_search(model, param_grid, X_train, y_train, cv=5):
    """
    Grid Search pour optimiser hyperparamÃ¨tres
    """
    print("\n" + "="*60)
    print("GRID SEARCH")
    print("="*60)
    print(f"ParamÃ¨tres testÃ©s : {param_grid}")

    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        cv=cv,
        scoring='accuracy',
        n_jobs=-1,
        verbose=1
    )

    grid_search.fit(X_train, y_train)

    print(f"\nâœ“ Meilleurs paramÃ¨tres : {grid_search.best_params_}")
    print(f"âœ“ Meilleur score CV : {grid_search.best_score_:.4f}")

    # RÃ©sultats dÃ©taillÃ©s
    results = pd.DataFrame(grid_search.cv_results_)
    results = results.sort_values('rank_test_score')
    print("\nTop 5 configurations :")
    print(results[['params', 'mean_test_score', 'std_test_score']].head())

    return grid_search.best_estimator_

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 3 : OPTIMISATION HYPERPARAMÃˆTRES - RANDOM SEARCH
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def optimisation_random_search(model, param_distributions, X_train, y_train,
                                n_iter=50, cv=5):
    """
    Random Search (plus rapide que Grid Search)
    """
    print("\n" + "="*60)
    print("RANDOM SEARCH")
    print("="*60)
    print(f"ParamÃ¨tres : {param_distributions}")
    print(f"ItÃ©rations : {n_iter}")

    random_search = RandomizedSearchCV(
        estimator=model,
        param_distributions=param_distributions,
        n_iter=n_iter,
        cv=cv,
        scoring='accuracy',
        n_jobs=-1,
        verbose=1,
        random_state=42
    )

    random_search.fit(X_train, y_train)

    print(f"\nâœ“ Meilleurs paramÃ¨tres : {random_search.best_params_}")
    print(f"âœ“ Meilleur score CV : {random_search.best_score_:.4f}")

    return random_search.best_estimator_

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 4 : FEATURE IMPORTANCE ET SÃ‰LECTION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def feature_importance_analysis(model, X, feature_names):
    """
    Analyse l'importance des features
    """
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1]

        print("\n" + "="*60)
        print("IMPORTANCE DES FEATURES")
        print("="*60)

        print("\nTop 10 features :")
        for i in range(min(10, len(feature_names))):
            print(f"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}")

        # Visualisation
        plt.figure(figsize=(10, 6))
        plt.bar(range(min(20, len(feature_names))),
                importances[indices[:min(20, len(feature_names))]])
        plt.xticks(range(min(20, len(feature_names))),
                   [feature_names[i] for i in indices[:min(20, len(feature_names))]],
                   rotation=90)
        plt.xlabel('Features')
        plt.ylabel('Importance')
        plt.title('Top 20 Features les Plus Importantes')
        plt.tight_layout()
        plt.show()

        return importances, indices

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‰TAPE 5 : LEARNING CURVES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from sklearn.model_selection import learning_curve

def plot_learning_curves(model, X, y, cv=5):
    """
    Trace les courbes d'apprentissage
    """
    train_sizes, train_scores, val_scores = learning_curve(
        model, X, y,
        cv=cv,
        n_jobs=-1,
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring='accuracy'
    )

    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)

    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mean, label='Train Score', marker='o')
    plt.fill_between(train_sizes, train_mean - train_std,
                     train_mean + train_std, alpha=0.15)

    plt.plot(train_sizes, val_mean, label='Validation Score', marker='o')
    plt.fill_between(train_sizes, val_mean - val_std,
                     val_mean + val_std, alpha=0.15)

    plt.xlabel('Taille du Training Set')
    plt.ylabel('Score')
    plt.title('Courbes d\'Apprentissage')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

    # Diagnostic
    final_gap = train_mean[-1] - val_mean[-1]
    if final_gap > 0.1:
        print("âš ï¸  Overfitting : Ã©cart important entre train et val")
        print("   â†’ Ajouter plus de donnÃ©es ou rÃ©gulariser")
    elif val_mean[-1] < 0.7:
        print("âš ï¸  Underfitting : scores faibles")
        print("   â†’ Augmenter complexitÃ© du modÃ¨le")
    else:
        print("âœ“ Courbes saines")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXEMPLE D'UTILISATION COMPLÃˆTE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# 1. Diagnostic
status = diagnostic_modele(model, X_train_scaled, y_train,
                           X_val_scaled, y_val)

# 2. Optimisation selon diagnostic
if status == "overfitting":
    # Augmenter rÃ©gularisation
    param_grid = {
        'max_depth': [3, 5, 7],
        'min_samples_split': [10, 20, 30],
        'n_estimators': [100]
    }
elif status == "underfitting":
    # Augmenter complexitÃ©
    param_grid = {
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5],
        'n_estimators': [200, 300]
    }
else:
    # Optimisation fine
    param_grid = {
        'max_depth': [5, 7, 10],
        'min_samples_split': [5, 10],
        'n_estimators': [100, 200]
    }

# 3. Grid Search
best_model = optimisation_grid_search(
    RandomForestClassifier(random_state=42),
    param_grid,
    X_train_scaled, y_train
)

# 4. Feature Importance
importances, indices = feature_importance_analysis(
    best_model, X_train_scaled, X_train.columns
)

# 5. Learning Curves
plot_learning_curves(best_model, X_train_scaled, y_train)

# 6. Ã‰valuation finale
y_val_pred = best_model.predict(X_val_scaled)
val_score = accuracy_score(y_val, y_val_pred)
print(f"\nğŸ“Š Score final aprÃ¨s optimisation : {val_score:.4f}")
```

---

## Workflow de Validation

### StratÃ©gies de Validation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STRATÃ‰GIES DE VALIDATION                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. HOLDOUT (Train/Val/Test Split)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
   â”‚    Train    â”‚ Val  â”‚ Test â”‚
   â”‚    70%      â”‚ 15%  â”‚ 15%  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
   Usage : Grandes donnÃ©es (>10k)

2. K-FOLD CROSS-VALIDATION
   Fold 1: [Test][Train][Train][Train][Train]
   Fold 2: [Train][Test][Train][Train][Train]
   Fold 3: [Train][Train][Test][Train][Train]
   Fold 4: [Train][Train][Train][Test][Train]
   Fold 5: [Train][Train][Train][Train][Test]
   Usage : Petites/moyennes donnÃ©es

3. STRATIFIED K-FOLD
   - Comme K-Fold mais prÃ©serve la distribution des classes
   Usage : Classes dÃ©sÃ©quilibrÃ©es

4. TIME SERIES SPLIT
   Fold 1: [Train][Test]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Fold 2: [Trainâ”€â”€â”€â”€â”€â”€][Test]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Fold 3: [Trainâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€][Test]â”€â”€â”€â”€â”€â”€â”€â”€
   Fold 4: [Trainâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€][Test]â”€
   Usage : SÃ©ries temporelles

5. LEAVE-ONE-OUT (LOO)
   - Chaque sample utilisÃ© une fois comme test
   Usage : TrÃ¨s petites donnÃ©es (<100)
```

### Code de Validation

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STRATÃ‰GIES DE VALIDATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from sklearn.model_selection import (
    cross_val_score, cross_validate,
    KFold, StratifiedKFold, TimeSeriesSplit, LeaveOneOut
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. K-FOLD CROSS-VALIDATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def k_fold_validation(model, X, y, k=5):
    """
    K-Fold Cross-Validation standard
    """
    kfold = KFold(n_splits=k, shuffle=True, random_state=42)

    scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')

    print(f"K-Fold CV ({k} folds)")
    print(f"  Scores : {scores}")
    print(f"  Moyenne : {scores.mean():.4f} (Â± {scores.std():.4f})")

    return scores

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. STRATIFIED K-FOLD (pour classes dÃ©sÃ©quilibrÃ©es)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def stratified_k_fold_validation(model, X, y, k=5):
    """
    Stratified K-Fold : prÃ©serve distribution des classes
    """
    skfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)

    scores = cross_val_score(model, X, y, cv=skfold, scoring='accuracy')

    print(f"Stratified K-Fold CV ({k} folds)")
    print(f"  Scores : {scores}")
    print(f"  Moyenne : {scores.mean():.4f} (Â± {scores.std():.4f})")

    # VÃ©rifier Ã©quilibre des classes dans chaque fold
    for fold, (train_idx, val_idx) in enumerate(skfold.split(X, y)):
        train_dist = np.bincount(y.iloc[train_idx]) / len(train_idx)
        val_dist = np.bincount(y.iloc[val_idx]) / len(val_idx)
        print(f"  Fold {fold+1} - Train: {train_dist}, Val: {val_dist}")

    return scores

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. CROSS-VALIDATE (mÃ©triques multiples)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cross_validate_multimetrics(model, X, y, k=5):
    """
    Cross-validation avec plusieurs mÃ©triques
    """
    scoring = {
        'accuracy': 'accuracy',
        'precision': 'precision_weighted',
        'recall': 'recall_weighted',
        'f1': 'f1_weighted'
    }

    cv_results = cross_validate(
        model, X, y,
        cv=k,
        scoring=scoring,
        return_train_score=True
    )

    print(f"Cross-Validation ({k} folds) - MÃ©triques multiples")
    for metric in scoring.keys():
        train_scores = cv_results[f'train_{metric}']
        test_scores = cv_results[f'test_{metric}']
        print(f"\n{metric.upper()}")
        print(f"  Train : {train_scores.mean():.4f} (Â± {train_scores.std():.4f})")
        print(f"  Test  : {test_scores.mean():.4f} (Â± {test_scores.std():.4f})")

    return cv_results

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. TIME SERIES SPLIT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def time_series_validation(model, X, y, n_splits=5):
    """
    Validation pour sÃ©ries temporelles
    """
    tscv = TimeSeriesSplit(n_splits=n_splits)

    scores = []
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
        X_train_fold = X.iloc[train_idx]
        y_train_fold = y.iloc[train_idx]
        X_val_fold = X.iloc[val_idx]
        y_val_fold = y.iloc[val_idx]

        model.fit(X_train_fold, y_train_fold)
        score = model.score(X_val_fold, y_val_fold)
        scores.append(score)

        print(f"Fold {fold+1}: Train [{train_idx[0]}:{train_idx[-1]}], "
              f"Val [{val_idx[0]}:{val_idx[-1]}], Score: {score:.4f}")

    print(f"\nMoyenne : {np.mean(scores):.4f} (Â± {np.std(scores):.4f})")

    return scores
```

---

## Workflow de Deep Learning

### Diagramme DL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WORKFLOW DEEP LEARNING SPÃ‰CIFIQUE                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. ARCHITECTURE
   â”‚
   â”œâ”€ DÃ©finir architecture (layers, neurons, activations)
   â”œâ”€ Choisir loss function
   â””â”€ Choisir optimizer
   â”‚
   â†“
2. ENTRAÃNEMENT INITIAL
   â”‚
   â”œâ”€ Petits epochs (10-20)
   â”œâ”€ Learning rate par dÃ©faut
   â””â”€ Monitoring (loss, accuracy)
   â”‚
   â†“
3. DIAGNOSTIC
   â”‚
   â”œâ”€ Overfitting ?
   â”‚  â””â”€ Ajouter : Dropout, L2 regularization, Data augmentation
   â”‚
   â”œâ”€ Underfitting ?
   â”‚  â””â”€ Augmenter : CapacitÃ© du modÃ¨le, epochs, complÃ©xitÃ©
   â”‚
   â””â”€ Convergence lente ?
      â””â”€ Ajuster : Learning rate, optimizer, batch size
   â”‚
   â†“
4. OPTIMISATION
   â”‚
   â”œâ”€ Learning Rate Scheduling
   â”œâ”€ Early Stopping
   â”œâ”€ Callbacks (ModelCheckpoint, ReduceLROnPlateau)
   â””â”€ Data Augmentation
   â”‚
   â†“
5. FINE-TUNING
   â”‚
   â”œâ”€ Transfer Learning (si applicable)
   â”œâ”€ Unfreeze layers
   â””â”€ Fine-tune avec LR faible
   â”‚
   â†“
6. ENSEMBLE (optionnel)
   â”‚
   â””â”€ Combiner plusieurs modÃ¨les
```

### Code Deep Learning Workflow

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# WORKFLOW DEEP LEARNING COMPLET
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, callbacks

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. DÃ‰FINIR L'ARCHITECTURE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def creer_modele(input_shape, num_classes):
    """
    CrÃ©e un modÃ¨le Neural Network
    """
    model = keras.Sequential([
        layers.Input(shape=input_shape),

        # Hidden layers
        layers.Dense(128, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),

        layers.Dense(64, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),

        layers.Dense(32, activation='relu'),

        # Output layer
        layers.Dense(num_classes, activation='softmax')
    ])

    return model

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. COMPILER LE MODÃˆLE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

model = creer_modele(input_shape=(X_train.shape[1],), num_classes=3)

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

print(model.summary())

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. CALLBACKS POUR OPTIMISATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Early Stopping : arrÃªte si pas d'amÃ©lioration
early_stop = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

# Model Checkpoint : sauvegarde meilleur modÃ¨le
checkpoint = callbacks.ModelCheckpoint(
    'best_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1
)

# Reduce LR on Plateau : rÃ©duit LR si stagnation
reduce_lr = callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-7,
    verbose=1
)

# TensorBoard : visualisation
tensorboard = callbacks.TensorBoard(
    log_dir='./logs',
    histogram_freq=1
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ENTRAÃNEMENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

history = model.fit(
    X_train_scaled, y_train,
    validation_data=(X_val_scaled, y_val),
    epochs=100,
    batch_size=32,
    callbacks=[early_stop, checkpoint, reduce_lr, tensorboard],
    verbose=1
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. VISUALISATION DE L'ENTRAÃNEMENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def plot_training_history(history):
    """
    Visualise les courbes d'entraÃ®nement
    """
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Loss
    axes[0].plot(history.history['loss'], label='Train Loss')
    axes[0].plot(history.history['val_loss'], label='Val Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Training and Validation Loss')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # Accuracy
    axes[1].plot(history.history['accuracy'], label='Train Accuracy')
    axes[1].plot(history.history['val_accuracy'], label='Val Accuracy')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy')
    axes[1].set_title('Training and Validation Accuracy')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

plot_training_history(history)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. Ã‰VALUATION FINALE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)
print(f"\nğŸ“Š Test Accuracy : {test_acc:.4f}")
print(f"ğŸ“Š Test Loss : {test_loss:.4f}")
```

---

## Pipeline de Production

### Architecture ComplÃ¨te

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PIPELINE DE PRODUCTION                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RAW DATA   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATA PROCESSING    â”‚
â”‚  - Cleaning         â”‚
â”‚  - Feature Eng.     â”‚
â”‚  - Encoding         â”‚
â”‚  - Scaling          â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MODEL TRAINING     â”‚
â”‚  - Train/Val/Test   â”‚
â”‚  - Cross-validation â”‚
â”‚  - Hyperparameter   â”‚
â”‚    Tuning           â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MODEL EVALUATION   â”‚
â”‚  - Metrics          â”‚
â”‚  - Error Analysis   â”‚
â”‚  - A/B Testing      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MODEL DEPLOYMENT   â”‚
â”‚  - API (Flask/      â”‚
â”‚    FastAPI)         â”‚
â”‚  - Docker           â”‚
â”‚  - CI/CD            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MONITORING         â”‚
â”‚  - Performance      â”‚
â”‚  - Data Drift       â”‚
â”‚  - Model Drift      â”‚
â”‚  - Retraining       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Code Pipeline Scikit-Learn

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PIPELINE SCIKIT-LEARN COMPLET
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Identifier colonnes numÃ©riques et catÃ©gorielles
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

# Pipeline pour features numÃ©riques
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Pipeline pour features catÃ©gorielles
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))
])

# Combiner les transformers
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Pipeline complet : preprocessing + modÃ¨le
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

# EntraÃ®ner le pipeline
pipeline.fit(X_train, y_train)

# PrÃ©dire
y_pred = pipeline.predict(X_test)

# Score
score = pipeline.score(X_test, y_test)
print(f"Score : {score:.4f}")

# Sauvegarder le pipeline complet
import joblib
joblib.dump(pipeline, 'pipeline_complet.pkl')

# Charger et utiliser
pipeline_loaded = joblib.load('pipeline_complet.pkl')
predictions = pipeline_loaded.predict(new_data)
```

---

## Diagrammes de DÃ©cision

### ProblÃ¨me d'Overfitting

```
OVERFITTING DÃ‰TECTÃ‰ (train >> val)
â”‚
â”œâ”€ RÃ©gularisation
â”‚  â”œâ”€ L1 (Lasso) â†’ SÃ©lection de features
â”‚  â”œâ”€ L2 (Ridge) â†’ RÃ©duire coefficients
â”‚  â””â”€ Dropout (NN) â†’ DÃ©sactiver neurones alÃ©atoirement
â”‚
â”œâ”€ RÃ©duire ComplexitÃ©
â”‚  â”œâ”€ Diminuer max_depth (trees)
â”‚  â”œâ”€ Diminuer nombre de layers (NN)
â”‚  â””â”€ Feature selection
â”‚
â”œâ”€ Augmenter DonnÃ©es
â”‚  â”œâ”€ Collecter plus de donnÃ©es
â”‚  â””â”€ Data augmentation (images)
â”‚
â””â”€ Early Stopping
   â””â”€ ArrÃªter entraÃ®nement avant overfitting
```

### ProblÃ¨me d'Underfitting

```
UNDERFITTING DÃ‰TECTÃ‰ (train et val faibles)
â”‚
â”œâ”€ Augmenter ComplexitÃ©
â”‚  â”œâ”€ Augmenter max_depth
â”‚  â”œâ”€ Ajouter layers (NN)
â”‚  â””â”€ Utiliser modÃ¨le plus complexe
â”‚
â”œâ”€ Feature Engineering
â”‚  â”œâ”€ CrÃ©er nouvelles features
â”‚  â”œâ”€ Interactions
â”‚  â””â”€ Transformations (log, sqrt, etc.)
â”‚
â”œâ”€ Diminuer RÃ©gularisation
â”‚  â”œâ”€ Diminuer alpha (Lasso/Ridge)
â”‚  â””â”€ Diminuer dropout rate
â”‚
â””â”€ EntraÃ®ner Plus Longtemps
   â””â”€ Augmenter epochs/iterations
```

---

**ğŸ¯ Ces workflows vous guident Ã©tape par Ã©tape dans vos projets ML !**

---

**Navigation :**

- [â¬…ï¸ Guide de DÃ©cision ML](00_Guide_Decision_ML.md)
- [â¡ï¸ Notebooks Tutoriels](README_ML.md)
- [ğŸ  Retour au Sommaire](README_ML.md)
