# Guide de DÃ©cision : Quel ModÃ¨le ML pour Quel ProblÃ¨me ?

## ğŸ“‹ Table des MatiÃ¨res
1. [Arbre de DÃ©cision Global](#arbre-de-dÃ©cision-global)
2. [Classification](#classification)
3. [RÃ©gression](#rÃ©gression)
4. [Clustering](#clustering)
5. [RÃ©duction de DimensionnalitÃ©](#rÃ©duction-de-dimensionnalitÃ©)
6. [DÃ©tection d'Anomalies](#dÃ©tection-danomalies)
7. [SÃ©ries Temporelles](#sÃ©ries-temporelles)
8. [Traitement d'Images](#traitement-dimages)
9. [Traitement du Langage Naturel (NLP)](#traitement-du-langage-naturel-nlp)
10. [Guide des Techniques d'Optimisation](#guide-des-techniques-doptimisation)
11. [Quand Utiliser Quoi ?](#quand-utiliser-quoi-)

---

## Arbre de DÃ©cision Global

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              QUEL EST VOTRE TYPE DE DONNÃ‰ES ?               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚           â”‚           â”‚
            â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
            â”‚ Tabulairesâ”‚ â”‚Imagesâ”‚ â”‚  Texte  â”‚
            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                  â”‚          â”‚           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”    â”‚           â”‚
        â”‚         â”‚     â”‚    â”‚           â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚Labelsâ”‚ â”‚Sans  â”‚ â”‚  CNN   â”‚   â”‚   NLP   â”‚
    â”‚      â”‚ â”‚Labelsâ”‚ â”‚ResNet  â”‚   â”‚Transformâ”‚
    â””â”€â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”˜ â”‚EfficNetâ”‚   â”‚  BERT   â”‚
        â”‚        â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”   â”‚
   â”‚    â”‚    â”‚   â”‚
 â”Œâ”€â–¼â”€â”â”Œâ”€â–¼â”€â”â”Œâ”€â–¼â”€â”€â–¼â”€â”€â”
 â”‚Clsâ”‚â”‚Regâ”‚â”‚Clusterâ”‚
 â””â”€â”€â”€â”˜â””â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Classification

### ğŸ“Š Arbre de DÃ©cision pour Classification

```
Vous avez un problÃ¨me de CLASSIFICATION
â”‚
â”œâ”€ Combien de classes ?
â”‚  â”œâ”€ 2 classes â†’ Classification BINAIRE
â”‚  â””â”€ >2 classes â†’ Classification MULTI-CLASSE
â”‚
â”œâ”€ Quelle est la taille de vos donnÃ©es ?
â”‚  â”œâ”€ <1,000 samples
â”‚  â”‚  â”œâ”€ LinÃ©aire â†’ Logistic Regression, Naive Bayes
â”‚  â”‚  â””â”€ Non-linÃ©aire â†’ Decision Tree, KNN
â”‚  â”‚
â”‚  â”œâ”€ 1,000 - 100,000 samples
â”‚  â”‚  â”œâ”€ InterprÃ©tabilitÃ© requise â†’ Logistic Regression, Decision Tree
â”‚  â”‚  â”œâ”€ Performance max â†’ Random Forest, XGBoost, LightGBM
â”‚  â”‚  â””â”€ DonnÃ©es texte â†’ Naive Bayes, SVM
â”‚  â”‚
â”‚  â””â”€ >100,000 samples
â”‚     â”œâ”€ DonnÃ©es tabulaires â†’ XGBoost, LightGBM, CatBoost
â”‚     â”œâ”€ DonnÃ©es images â†’ CNN (ResNet, EfficientNet)
â”‚     â”œâ”€ DonnÃ©es texte â†’ Transformers (BERT, RoBERTa)
â”‚     â””â”€ Temps rÃ©el â†’ Linear models, small trees
â”‚
â””â”€ Contraintes spÃ©cifiques ?
   â”œâ”€ InterprÃ©tabilitÃ© â†’ Logistic Regression, Decision Tree, Linear SVM
   â”œâ”€ Temps d'infÃ©rence court â†’ Linear models, small Decision Trees
   â”œâ”€ Classes dÃ©sÃ©quilibrÃ©es â†’ XGBoost, Random Forest + class_weight
   â””â”€ Peu de features â†’ SVM (kernel), Neural Networks
```

### ğŸ“‹ Tableau de DÃ©cision Classification

| CritÃ¨re | ModÃ¨le RecommandÃ© | Raison |
|---------|-------------------|--------|
| **DonnÃ©es linÃ©airement sÃ©parables** | Logistic Regression, Linear SVM | Simple, rapide, interprÃ©table |
| **DonnÃ©es non-linÃ©aires, petites** | SVM (RBF kernel), Decision Tree | Capture non-linÃ©aritÃ©, peu de donnÃ©es |
| **DonnÃ©es non-linÃ©aires, moyennes** | Random Forest, XGBoost | Meilleure performance, robuste |
| **DonnÃ©es non-linÃ©aires, grandes** | XGBoost, LightGBM, Neural Networks | Scalable, haute performance |
| **Classes dÃ©sÃ©quilibrÃ©es** | XGBoost + scale_pos_weight, Random Forest + class_weight | GÃ¨re dÃ©sÃ©quilibre nativement |
| **InterprÃ©tabilitÃ© critique** | Logistic Regression, Decision Tree | Coefficients/rÃ¨gles clairs |
| **Haute dimensionnalitÃ©** | Linear SVM, Logistic Regression + rÃ©gularisation | Ã‰vite overfitting |
| **DonnÃ©es catÃ©gorielles** | CatBoost, LightGBM | Gestion native des catÃ©gories |
| **Temps d'entraÃ®nement court** | Naive Bayes, Logistic Regression | TrÃ¨s rapides |
| **Temps d'infÃ©rence court** | Linear models, small trees | PrÃ©dictions instantanÃ©es |

### ğŸ” DÃ©tail des ModÃ¨les de Classification

#### 1. Logistic Regression

**Quand l'utiliser ?**
- âœ… Classes linÃ©airement sÃ©parables
- âœ… Besoin d'interprÃ©tabilitÃ© (coefficients)
- âœ… Baseline rapide
- âœ… ProbabilitÃ©s calibrÃ©es nÃ©cessaires
- âœ… Haute dimensionnalitÃ© (avec rÃ©gularisation)

**Quand ne PAS l'utiliser ?**
- âŒ Relations fortement non-linÃ©aires
- âŒ Interactions complexes entre features
- âŒ Besoin de performance maximale sur donnÃ©es complexes

**Exemple d'usage :**
```python
from sklearn.linear_model import LogisticRegression

# Standard
model = LogisticRegression(max_iter=1000)

# Avec rÃ©gularisation L1 (sÃ©lection de features)
model = LogisticRegression(penalty='l1', solver='liblinear', C=0.1)

# Avec rÃ©gularisation L2 (Ridge)
model = LogisticRegression(penalty='l2', C=1.0)

# Classes dÃ©sÃ©quilibrÃ©es
model = LogisticRegression(class_weight='balanced')
```

#### 2. Decision Tree

**Quand l'utiliser ?**
- âœ… Besoin d'interprÃ©tabilitÃ© visuelle
- âœ… Relations non-linÃ©aires
- âœ… Pas besoin de normalisation
- âœ… GÃ¨re valeurs manquantes naturellement
- âœ… Variables catÃ©gorielles et continues mÃ©langÃ©es

**Quand ne PAS l'utiliser ?**
- âŒ DonnÃ©es bruitÃ©es (overfitting facile)
- âŒ Besoin de performance maximale
- âŒ Extrapolation nÃ©cessaire

**Exemple d'usage :**
```python
from sklearn.tree import DecisionTreeClassifier

# Standard
model = DecisionTreeClassifier(random_state=42)

# Limiter la profondeur pour Ã©viter overfitting
model = DecisionTreeClassifier(
    max_depth=5,
    min_samples_split=20,
    min_samples_leaf=10,
    random_state=42
)
```

#### 3. Random Forest

**Quand l'utiliser ?**
- âœ… DonnÃ©es non-linÃ©aires
- âœ… Peu de preprocessing nÃ©cessaire
- âœ… Importance des features requise
- âœ… Robustesse aux outliers
- âœ… Taille moyenne Ã  grande

**Quand ne PAS l'utiliser ?**
- âŒ TrÃ¨s grandes donnÃ©es (prÃ©fÃ©rer LightGBM)
- âŒ Temps d'infÃ©rence critique
- âŒ MÃ©moire limitÃ©e
- âŒ InterprÃ©tabilitÃ© au niveau individuel requise

**Exemple d'usage :**
```python
from sklearn.ensemble import RandomForestClassifier

# Standard
model = RandomForestClassifier(
    n_estimators=100,
    max_depth=None,
    random_state=42,
    n_jobs=-1
)

# Classes dÃ©sÃ©quilibrÃ©es
model = RandomForestClassifier(
    n_estimators=100,
    class_weight='balanced',
    random_state=42
)

# Performance optimale
model = RandomForestClassifier(
    n_estimators=500,
    max_depth=20,
    min_samples_split=10,
    min_samples_leaf=4,
    max_features='sqrt',
    bootstrap=True,
    random_state=42
)
```

#### 4. XGBoost / LightGBM / CatBoost

**Quand l'utiliser ?**
- âœ… Performance maximale requise
- âœ… DonnÃ©es tabulaires
- âœ… CompÃ©titions Kaggle
- âœ… Grandes donnÃ©es
- âœ… Tuning d'hyperparamÃ¨tres possible

**DiffÃ©rences :**
- **XGBoost** : Standard, trÃ¨s performant, bien documentÃ©
- **LightGBM** : Plus rapide, gÃ¨re grandes donnÃ©es, Ã©conome en mÃ©moire
- **CatBoost** : GÃ¨re catÃ©gories nativement, peu de tuning nÃ©cessaire

**Exemple d'usage :**
```python
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

# XGBoost
xgb = XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

# LightGBM (plus rapide)
lgbm = LGBMClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    num_leaves=31,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

# CatBoost (gÃ¨re catÃ©gories)
catboost = CatBoostClassifier(
    iterations=100,
    depth=6,
    learning_rate=0.1,
    random_state=42,
    verbose=0
)
```

#### 5. Support Vector Machine (SVM)

**Quand l'utiliser ?**
- âœ… Haute dimensionnalitÃ© (features >> samples)
- âœ… DonnÃ©es non-linÃ©aires (kernel RBF)
- âœ… Classes bien sÃ©parÃ©es
- âœ… Petites/moyennes donnÃ©es

**Quand ne PAS l'utiliser ?**
- âŒ TrÃ¨s grandes donnÃ©es (lent, O(nÂ²))
- âŒ Classes dÃ©sÃ©quilibrÃ©es (nÃ©cessite tuning)
- âŒ Besoin de probabilitÃ©s calibrÃ©es

**Exemple d'usage :**
```python
from sklearn.svm import SVC

# LinÃ©aire (haute dimensionnalitÃ©)
model = SVC(kernel='linear', C=1.0)

# RBF (non-linÃ©aire)
model = SVC(kernel='rbf', C=1.0, gamma='scale')

# Avec probabilitÃ©s
model = SVC(kernel='rbf', probability=True)

# Classes dÃ©sÃ©quilibrÃ©es
model = SVC(kernel='rbf', class_weight='balanced')
```

#### 6. Naive Bayes

**Quand l'utiliser ?**
- âœ… DonnÃ©es texte (NLP)
- âœ… Besoin de rapiditÃ©
- âœ… Baseline simple
- âœ… Features indÃ©pendantes
- âœ… Streaming/online learning

**Quand ne PAS l'utiliser ?**
- âŒ Features corrÃ©lÃ©es
- âŒ Besoin de performance maximale

**Exemple d'usage :**
```python
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB

# Gaussian (features continues, distribution normale)
model = GaussianNB()

# Multinomial (comptages, ex: TF-IDF)
model = MultinomialNB(alpha=1.0)

# Bernoulli (features binaires)
model = BernoulliNB(alpha=1.0)
```

#### 7. K-Nearest Neighbors (KNN)

**Quand l'utiliser ?**
- âœ… Patterns locaux importants
- âœ… DonnÃ©es peu volumineuses
- âœ… Pas de phase d'entraÃ®nement nÃ©cessaire
- âœ… DonnÃ©es non-linÃ©aires

**Quand ne PAS l'utiliser ?**
- âŒ Grandes donnÃ©es (lent Ã  prÃ©dire)
- âŒ Haute dimensionnalitÃ© (curse of dimensionality)
- âŒ Features de diffÃ©rentes Ã©chelles (nÃ©cessite normalisation)

**Exemple d'usage :**
```python
from sklearn.neighbors import KNeighborsClassifier

# Standard
model = KNeighborsClassifier(n_neighbors=5)

# PondÃ©ration par distance
model = KNeighborsClassifier(n_neighbors=5, weights='distance')

# MÃ©trique personnalisÃ©e
model = KNeighborsClassifier(n_neighbors=5, metric='manhattan')
```

### ğŸ¯ StratÃ©gie de SÃ©lection Rapide

**Workflow recommandÃ© :**

1. **Baseline rapide** : Logistic Regression
2. **AmÃ©lioration** : Random Forest ou XGBoost
3. **Optimisation** : Tuning d'hyperparamÃ¨tres du meilleur modÃ¨le
4. **Si insatisfait** : Tester SVM, Neural Networks

---

## RÃ©gression

### ğŸ“Š Arbre de DÃ©cision pour RÃ©gression

```
Vous avez un problÃ¨me de RÃ‰GRESSION
â”‚
â”œâ”€ La relation est-elle linÃ©aire ?
â”‚  â”‚
â”‚  â”œâ”€ OUI â†’ ModÃ¨les LinÃ©aires
â”‚  â”‚  â”œâ”€ Peu de features â†’ Linear Regression
â”‚  â”‚  â”œâ”€ Beaucoup de features â†’ Ridge, Lasso
â”‚  â”‚  â””â”€ SÃ©lection de features â†’ Lasso, ElasticNet
â”‚  â”‚
â”‚  â””â”€ NON â†’ ModÃ¨les Non-LinÃ©aires
â”‚     â”œâ”€ <10k samples â†’ SVR, Decision Tree
â”‚     â”œâ”€ 10k-100k samples â†’ Random Forest, XGBoost
â”‚     â””â”€ >100k samples â†’ XGBoost, LightGBM, Neural Networks
â”‚
â”œâ”€ Y a-t-il des outliers ?
â”‚  â”œâ”€ OUI â†’ Ridge, Random Forest, Huber Regression
â”‚  â””â”€ NON â†’ Tous modÃ¨les OK
â”‚
â””â”€ Contraintes ?
   â”œâ”€ InterprÃ©tabilitÃ© â†’ Linear Regression, Decision Tree
   â”œâ”€ RÃ©gularisation â†’ Ridge (L2), Lasso (L1), ElasticNet
   â””â”€ Performance max â†’ XGBoost, LightGBM
```

### ğŸ“‹ Tableau de DÃ©cision RÃ©gression

| CritÃ¨re | ModÃ¨le RecommandÃ© | Raison |
|---------|-------------------|--------|
| **Relation linÃ©aire** | Linear Regression | Simple, interprÃ©table |
| **Relation linÃ©aire + multicollinÃ©aritÃ©** | Ridge Regression | Stabilise les coefficients |
| **Relation linÃ©aire + beaucoup de features** | Lasso, ElasticNet | SÃ©lection de features |
| **Relation non-linÃ©aire, petites donnÃ©es** | SVR, Decision Tree | Capture non-linÃ©aritÃ© |
| **Relation non-linÃ©aire, moyennes donnÃ©es** | Random Forest, XGBoost | Performance, robustesse |
| **Relation non-linÃ©aire, grandes donnÃ©es** | XGBoost, LightGBM | Scalable, performant |
| **Outliers prÃ©sents** | Huber Regression, Random Forest | Robuste aux outliers |
| **InterprÃ©tabilitÃ©** | Linear Regression, Decision Tree | Coefficients/rÃ¨gles clairs |

### ğŸ” DÃ©tail des ModÃ¨les de RÃ©gression

#### 1. Linear Regression

**Quand l'utiliser ?**
- âœ… Relation linÃ©aire claire
- âœ… Besoin d'interprÃ©tabilitÃ©
- âœ… Baseline rapide
- âœ… Peu de features
- âœ… Pas de multicollinÃ©aritÃ©

**Quand ne PAS l'utiliser ?**
- âŒ Relation non-linÃ©aire
- âŒ MulticollinÃ©aritÃ© forte
- âŒ Beaucoup de features inutiles

**Exemple d'usage :**
```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)

# InterprÃ©ter les coefficients
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)
```

#### 2. Ridge Regression (L2)

**Quand l'utiliser ?**
- âœ… MulticollinÃ©aritÃ© prÃ©sente
- âœ… Beaucoup de features
- âœ… PrÃ©vention de l'overfitting
- âœ… Garder toutes les features

**ParamÃ¨tre clÃ© :** `alpha` (force de rÃ©gularisation)
- Petit alpha â†’ proche Linear Regression
- Grand alpha â†’ coefficients plus petits

**Exemple d'usage :**
```python
from sklearn.linear_model import Ridge
from sklearn.model_selection import RidgeCV

# Avec alpha fixe
model = Ridge(alpha=1.0)

# Avec cross-validation pour choisir alpha
model = RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0], cv=5)
model.fit(X_train, y_train)
print(f"Best alpha: {model.alpha_}")
```

#### 3. Lasso Regression (L1)

**Quand l'utiliser ?**
- âœ… Beaucoup de features inutiles
- âœ… SÃ©lection automatique de features
- âœ… Features parcimonieuses souhaitÃ©es
- âœ… InterprÃ©tabilitÃ© avec peu de features

**Avantage :** Met certains coefficients Ã  0 (sÃ©lection de features)

**Exemple d'usage :**
```python
from sklearn.linear_model import Lasso, LassoCV

# Avec alpha fixe
model = Lasso(alpha=0.1)

# Avec cross-validation
model = LassoCV(alphas=[0.001, 0.01, 0.1, 1.0], cv=5)
model.fit(X_train, y_train)

# Features sÃ©lectionnÃ©es
selected_features = X_train.columns[model.coef_ != 0]
print(f"Features sÃ©lectionnÃ©es: {len(selected_features)}/{len(X_train.columns)}")
```

#### 4. ElasticNet (L1 + L2)

**Quand l'utiliser ?**
- âœ… Compromis entre Ridge et Lasso
- âœ… Beaucoup de features corrÃ©lÃ©es
- âœ… SÃ©lection de groupes de features corrÃ©lÃ©es

**ParamÃ¨tres :**
- `alpha` : Force de rÃ©gularisation
- `l1_ratio` : Mix L1/L2 (0=Ridge, 1=Lasso, 0.5=Ã©quilibre)

**Exemple d'usage :**
```python
from sklearn.linear_model import ElasticNet, ElasticNetCV

model = ElasticNetCV(
    l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99],
    alphas=[0.001, 0.01, 0.1, 1.0],
    cv=5
)
model.fit(X_train, y_train)
print(f"Best l1_ratio: {model.l1_ratio_}, Best alpha: {model.alpha_}")
```

#### 5. Decision Tree / Random Forest / XGBoost

**Quand l'utiliser ?** (mÃªme logique que classification)
- âœ… Relation non-linÃ©aire
- âœ… Interactions complexes
- âœ… Pas besoin de normalisation
- âœ… Performance maximale (XGBoost)

**Exemple d'usage :**
```python
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

# Random Forest
rf = RandomForestRegressor(
    n_estimators=100,
    max_depth=20,
    random_state=42,
    n_jobs=-1
)

# XGBoost
xgb = XGBRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    random_state=42
)
```

### ğŸ¯ StratÃ©gie de SÃ©lection Rapide

1. **Baseline** : Linear Regression
2. **Si multicollinÃ©aritÃ©** : Ridge
3. **Si beaucoup de features** : Lasso ou ElasticNet
4. **Si non-linÃ©aire** : XGBoost ou Random Forest

---

## Clustering

### ğŸ“Š Arbre de DÃ©cision pour Clustering

```
Vous voulez faire du CLUSTERING
â”‚
â”œâ”€ Connaissez-vous le nombre de clusters K ?
â”‚  â”‚
â”‚  â”œâ”€ OUI
â”‚  â”‚  â”œâ”€ Clusters sphÃ©riques ? â†’ K-Means
â”‚  â”‚  â””â”€ Formes arbitraires ? â†’ Spectral Clustering
â”‚  â”‚
â”‚  â””â”€ NON
â”‚     â”œâ”€ DensitÃ© variable ? â†’ DBSCAN, HDBSCAN
â”‚     â”œâ”€ HiÃ©rarchie ? â†’ Hierarchical Clustering
â”‚     â””â”€ Automatique ? â†’ HDBSCAN, Gaussian Mixture
â”‚
â”œâ”€ Quelle est la taille des donnÃ©es ?
â”‚  â”œâ”€ <10k â†’ Tous modÃ¨les OK
â”‚  â”œâ”€ 10k-100k â†’ K-Means, DBSCAN
â”‚  â””â”€ >100k â†’ K-Means, Mini-Batch K-Means
â”‚
â””â”€ Contraintes ?
   â”œâ”€ Vitesse â†’ K-Means, Mini-Batch K-Means
   â”œâ”€ Outliers importants â†’ DBSCAN
   â””â”€ Clusters imbriquÃ©s â†’ Hierarchical
```

### ğŸ“‹ Comparaison des Algorithmes de Clustering

| Algorithme | Avantages | InconvÃ©nients | Usage |
|------------|-----------|---------------|-------|
| **K-Means** | Rapide, scalable | K fixe, clusters sphÃ©riques | Grandes donnÃ©es, clusters bien dÃ©finis |
| **DBSCAN** | DÃ©tecte outliers, K automatique | Sensible Ã  eps et min_samples | DensitÃ© variable, outliers |
| **HDBSCAN** | DBSCAN + hiÃ©rarchie | Lent sur grandes donnÃ©es | Meilleure alternative Ã  DBSCAN |
| **Hierarchical** | Dendrogramme, K flexible | Lent O(nÂ²), pas scalable | Petites donnÃ©es, hiÃ©rarchie |
| **Gaussian Mixture** | Clusters probabilistes | K fixe, suppose Gaussiennes | Clusters ellipsoÃ¯des, incertitude |
| **Spectral** | Formes complexes | Lent, K fixe | Clusters non-convexes |

### ğŸ” DÃ©tail des Algorithmes

#### 1. K-Means

**Quand l'utiliser ?**
- âœ… Clusters sphÃ©riques et de taille similaire
- âœ… Grandes donnÃ©es
- âœ… K connu ou estimable
- âœ… Besoin de rapiditÃ©

**Quand ne PAS l'utiliser ?**
- âŒ Clusters de formes arbitraires
- âŒ Clusters de densitÃ©s diffÃ©rentes
- âŒ Outliers nombreux

**Exemple d'usage :**
```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# MÃ©thode du coude pour choisir K
inertias = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

plt.plot(K_range, inertias, 'bo-')
plt.xlabel('K')
plt.ylabel('Inertie')
plt.title('MÃ©thode du Coude')
plt.show()

# K-Means final
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
labels = kmeans.fit_predict(X)
```

#### 2. DBSCAN

**Quand l'utiliser ?**
- âœ… Clusters de formes arbitraires
- âœ… Outliers Ã  dÃ©tecter
- âœ… K inconnu
- âœ… DensitÃ© variable

**ParamÃ¨tres critiques :**
- `eps` : Rayon du voisinage
- `min_samples` : Nombre minimum de points pour un cluster

**Exemple d'usage :**
```python
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors

# Trouver eps optimal avec k-distance graph
nn = NearestNeighbors(n_neighbors=5)
nn.fit(X)
distances, indices = nn.kneighbors(X)
distances = np.sort(distances[:, -1])

plt.plot(distances)
plt.ylabel('5-NN distance')
plt.xlabel('Points')
plt.title('K-distance Graph (chercher le coude)')
plt.show()

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X)

n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_outliers = list(labels).count(-1)
print(f"Clusters: {n_clusters}, Outliers: {n_outliers}")
```

#### 3. Hierarchical Clustering

**Quand l'utiliser ?**
- âœ… HiÃ©rarchie de clusters importante
- âœ… Petites donnÃ©es (<10k)
- âœ… Dendrogramme souhaitÃ©
- âœ… K flexible aprÃ¨s coup

**MÃ©thodes de linkage :**
- `ward` : Minimise variance (recommandÃ©)
- `average` : Distance moyenne
- `complete` : Distance maximale
- `single` : Distance minimale

**Exemple d'usage :**
```python
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering

# CrÃ©er dendrogramme
linkage_matrix = linkage(X, method='ward')

plt.figure(figsize=(12, 5))
dendrogram(linkage_matrix)
plt.title('Dendrogramme')
plt.show()

# Clustering agglomÃ©ratif
agg = AgglomerativeClustering(n_clusters=3, linkage='ward')
labels = agg.fit_predict(X)
```

---

## RÃ©duction de DimensionnalitÃ©

### ğŸ“Š Guide de DÃ©cision

```
Objectif de RÃ‰DUCTION DE DIMENSIONNALITÃ‰
â”‚
â”œâ”€ Quel est l'objectif ?
â”‚  â”‚
â”‚  â”œâ”€ Visualisation (2D/3D)
â”‚  â”‚  â”œâ”€ Structure globale â†’ PCA
â”‚  â”‚  â”œâ”€ Structure locale â†’ t-SNE
â”‚  â”‚  â””â”€ Structure globale+locale â†’ UMAP
â”‚  â”‚
â”‚  â”œâ”€ Compression de donnÃ©es
â”‚  â”‚  â”œâ”€ LinÃ©aire â†’ PCA
â”‚  â”‚  â”œâ”€ Non-linÃ©aire â†’ Autoencoder
â”‚  â”‚  â””â”€ DonnÃ©es images â†’ CNN Autoencoder
â”‚  â”‚
â”‚  â”œâ”€ Preprocessing avant ML
â”‚  â”‚  â”œâ”€ LinÃ©aire â†’ PCA
â”‚  â”‚  â”œâ”€ SÃ©lection de features â†’ Feature Selection (Lasso, RFE)
â”‚  â”‚  â””â”€ Non-linÃ©aire â†’ Kernel PCA
â”‚  â”‚
â”‚  â””â”€ GÃ©nÃ©ration de donnÃ©es
â”‚     â””â”€ Variational Autoencoder (VAE)
â”‚
â””â”€ Taille des donnÃ©es ?
   â”œâ”€ <10k â†’ Tous algorithmes OK
   â”œâ”€ 10k-100k â†’ PCA, UMAP, Autoencoder
   â””â”€ >100k â†’ PCA, UMAP, Autoencoder
```

### ğŸ“‹ Comparaison des Techniques

| Technique | Type | Usage Principal | PrÃ©serve | Vitesse |
|-----------|------|-----------------|----------|---------|
| **PCA** | LinÃ©aire | Compression, preprocessing | Variance globale | TrÃ¨s rapide |
| **t-SNE** | Non-linÃ©aire | Visualisation | Structure locale | Lent |
| **UMAP** | Non-linÃ©aire | Visualisation, gÃ©nÃ©ral | Structure locale+globale | Rapide |
| **Autoencoder** | Non-linÃ©aire | Compression, gÃ©nÃ©ration | Features apprises | Moyen |
| **Kernel PCA** | Non-linÃ©aire | Preprocessing | Variance (kernel space) | Lent |

### ğŸ” Quand Utiliser Quoi ?

#### PCA (Principal Component Analysis)

**Utiliser POUR :**
- âœ… Compression de donnÃ©es
- âœ… RÃ©duction de bruit
- âœ… Visualisation rapide
- âœ… Preprocessing avant ML
- âœ… DonnÃ©es avec variance linÃ©aire

**NE PAS utiliser pour :**
- âŒ DonnÃ©es avec structure non-linÃ©aire complexe
- âŒ Visualisation fine de clusters
- âŒ InterprÃ©tation sÃ©mantique des composantes

```python
from sklearn.decomposition import PCA

# PCA standard
pca = PCA(n_components=0.95)  # 95% de variance
X_reduced = pca.fit_transform(X)

print(f"Dimensions: {X.shape} â†’ {X_reduced.shape}")
print(f"Variance expliquÃ©e: {pca.explained_variance_ratio_.sum():.2%}")
```

#### t-SNE

**Utiliser POUR :**
- âœ… Visualisation 2D/3D de clusters
- âœ… Exploration de donnÃ©es
- âœ… Structure locale importante

**NE PAS utiliser pour :**
- âŒ Preprocessing pour ML (non dÃ©terministe)
- âŒ Nouvelles donnÃ©es (pas de transform)
- âŒ Grandes donnÃ©es (>50k, trÃ¨s lent)
- âŒ InterprÃ©tation des distances globales

```python
from sklearn.manifold import TSNE

# t-SNE pour visualisation
tsne = TSNE(
    n_components=2,
    perplexity=30,  # 5-50 typiquement
    n_iter=1000,
    random_state=42
)
X_tsne = tsne.fit_transform(X)

plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10')
plt.title('t-SNE Visualization')
plt.show()
```

#### UMAP

**Utiliser POUR :**
- âœ… Visualisation (alternative Ã  t-SNE)
- âœ… Preprocessing pour ML
- âœ… Structure globale ET locale
- âœ… Plus rapide que t-SNE
- âœ… Peut transformer nouvelles donnÃ©es

**NE PAS utiliser pour :**
- âŒ Besoin d'interprÃ©tabilitÃ© des axes

```python
import umap

# UMAP
reducer = umap.UMAP(
    n_components=2,
    n_neighbors=15,
    min_dist=0.1,
    random_state=42
)
X_umap = reducer.fit_transform(X)

# Peut transformer nouvelles donnÃ©es
X_new_umap = reducer.transform(X_new)
```

---

## Guide des Techniques d'Optimisation

### Ã€ Quoi Sert la Descente de Gradient ?

**DÃ©finition :** Algorithme d'optimisation pour minimiser une fonction de coÃ»t.

**Principe :**
$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t)
$$

oÃ¹ :
- $\theta$ = paramÃ¨tres du modÃ¨le
- $\eta$ = learning rate
- $\nabla_\theta \mathcal{L}$ = gradient de la fonction de coÃ»t

### Quand Utiliser la Descente de Gradient ?

#### ModÃ¨les qui l'utilisent :

| ModÃ¨le | Utilise Gradient Descent ? | Algorithme |
|--------|----------------------------|------------|
| **Linear Regression** | Non (solution analytique) | Normal Equation |
| **Logistic Regression** | Oui | Gradient Descent, LBFGS, Newton |
| **Neural Networks** | Oui | SGD, Adam, RMSprop |
| **SVM** | Oui (si SGD) | SGD, SMO |
| **XGBoost** | Non | Gradient Boosting (diffÃ©rent) |
| **Decision Tree** | Non | Algorithme glouton |

### Types de Descente de Gradient

```python
# 1. Batch Gradient Descent (tout le dataset)
for epoch in range(n_epochs):
    gradients = compute_gradients(X, y, theta)
    theta = theta - learning_rate * gradients

# 2. Stochastic Gradient Descent (1 sample Ã  la fois)
for epoch in range(n_epochs):
    for i in range(n_samples):
        gradients = compute_gradients(X[i], y[i], theta)
        theta = theta - learning_rate * gradients

# 3. Mini-Batch Gradient Descent (batch de samples)
for epoch in range(n_epochs):
    for batch in get_batches(X, y, batch_size):
        gradients = compute_gradients(batch_X, batch_y, theta)
        theta = theta - learning_rate * gradients
```

### Optimiseurs pour Neural Networks

```python
from tensorflow.keras.optimizers import SGD, Adam, RMSprop, Adagrad

# 1. SGD (simple)
optimizer = SGD(learning_rate=0.01)

# 2. SGD with Momentum
optimizer = SGD(learning_rate=0.01, momentum=0.9)

# 3. Adam (recommandÃ© par dÃ©faut)
optimizer = Adam(learning_rate=0.001)

# 4. RMSprop
optimizer = RMSprop(learning_rate=0.001)
```

**Guide de choix :**
- **SGD** : Baseline simple
- **SGD + Momentum** : AccÃ©lÃ¨re la convergence
- **Adam** : RecommandÃ© par dÃ©faut (adaptatif)
- **RMSprop** : Bon pour RNN

---

## Quand Utiliser Quoi ?

### Tableau RÃ©capitulatif Global

| Situation | Technique RecommandÃ©e | Raison |
|-----------|----------------------|--------|
| **Baseline rapide** | Logistic Regression / Linear Regression | Simple, rapide, interprÃ©table |
| **Performance maximale sur tabulaire** | XGBoost / LightGBM | Ã‰tat de l'art pour donnÃ©es tabulaires |
| **Images** | CNN (ResNet, EfficientNet) | SpÃ©cialisÃ© pour images |
| **Texte** | Transformers (BERT) / RNN | SpÃ©cialisÃ© pour NLP |
| **SÃ©ries temporelles** | LSTM, GRU, Prophet | Capture dÃ©pendances temporelles |
| **InterprÃ©tabilitÃ© requise** | Logistic Regression, Decision Tree | Coefficients/rÃ¨gles clairs |
| **Peu de donnÃ©es** | Linear models, Decision Tree, SVM | Ã‰vite overfitting |
| **Beaucoup de donnÃ©es** | XGBoost, LightGBM, Deep Learning | Scalable, performant |
| **Temps d'infÃ©rence critique** | Linear models, small trees | PrÃ©dictions instantanÃ©es |
| **Classes dÃ©sÃ©quilibrÃ©es** | XGBoost + class_weight, SMOTE | GÃ¨re dÃ©sÃ©quilibre |
| **Clustering avec K inconnu** | DBSCAN, HDBSCAN | K automatique |
| **Visualisation** | t-SNE, UMAP | Excellente visualisation 2D/3D |
| **Compression** | PCA, Autoencoder | RÃ©duit dimensionnalitÃ© |
| **DÃ©tection d'anomalies** | Isolation Forest, Autoencoder | SpÃ©cialisÃ© pour outliers |

---

**ğŸ¯ Ce guide vous aide Ã  choisir le bon modÃ¨le dans toutes les situations !**

---

**Navigation :**
- [â¬…ï¸ Guide Projet ML](00_Guide_Projet_ML.md)
- [â¡ï¸ Workflows ML](00_Workflows_ML.md)
- [ğŸ  Retour au Sommaire](README.md)
