# Module 3 : Th√©orie des Probabilit√©s pour le Machine Learning

## üìã Table des Mati√®res

1. [Introduction](#introduction)
2. [Pourquoi les Probabilit√©s en ML ?](#pourquoi-les-probabilit√©s-en-ml-)
3. [Fondements Math√©matiques](#fondements-math√©matiques)
4. [Probabilit√©s Conditionnelles et Ind√©pendance](#probabilit√©s-conditionnelles-et-ind√©pendance)
5. [Variables Al√©atoires](#variables-al√©atoires)
6. [Variables Discr√®tes](#variables-discr√®tes)
7. [Variables Continues](#variables-continues)
8. [Esp√©rance et Variance](#esp√©rance-et-variance)
9. [Vecteurs Al√©atoires](#vecteurs-al√©atoires)
10. [Covariance et Corr√©lation](#covariance-et-corr√©lation)
11. [Th√©or√®me de Bayes](#th√©or√®me-de-bayes)
12. [Applications au Machine Learning](#applications-au-machine-learning)
13. [Exercices Pratiques](#exercices-pratiques)
14. [R√©sum√©](#r√©sum√©)

---

## Introduction

La **th√©orie des probabilit√©s** est le langage math√©matique de l'incertitude et du hasard. Elle constitue le fondement th√©orique du Machine Learning, permettant de mod√©liser l'incertitude inh√©rente aux donn√©es et aux pr√©dictions.

**Domaines d'application** :

- Mod√©lisation de l'incertitude dans les donn√©es
- Inf√©rence statistique
- Apprentissage bay√©sien
- √âvaluation de la confiance des pr√©dictions
- Th√©orie de l'information

---

## Pourquoi les Probabilit√©s en ML ?

### L'Incertitude en Machine Learning

Un concept cl√© en Machine Learning et Data Science est l'**incertitude** :

1. **Bruit dans les mesures** : Les donn√©es r√©elles sont toujours bruit√©es
2. **Information incompl√®te** : On ne dispose jamais de toutes les informations
3. **Variabilit√© naturelle** : Les ph√©nom√®nes naturels sont intrins√®quement al√©atoires

### R√¥le de la Th√©orie des Probabilit√©s

La th√©orie des probabilit√©s fournit un cadre coh√©rent pour :

- **Quantifier l'incertitude** : Mesurer le degr√© de certitude/incertitude
- **Manipuler l'incertitude** : Combiner et propager les incertitudes
- **Pr√©dictions optimales** : Faire des pr√©dictions m√™me avec information incompl√®te
- **Prise de d√©cision** : Choisir l'action optimale sous incertitude

### Exemples Concrets

```python
import numpy as np
import matplotlib.pyplot as plt

# Exemple 1: Bruit dans les mesures
np.random.seed(42)
x = np.linspace(0, 10, 100)
y_true = 2 * x + 1
y_observed = y_true + np.random.normal(0, 2, 100)  # Donn√©es bruit√©es

plt.figure(figsize=(10, 5))
plt.scatter(x, y_observed, alpha=0.5, label='Donn√©es observ√©es (bruit√©es)')
plt.plot(x, y_true, 'r-', linewidth=2, label='Vraie relation')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Incertitude due au bruit de mesure')
plt.legend()
plt.grid(True)
plt.show()

# Exemple 2: Classification probabiliste
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression

X, y = make_classification(n_samples=100, n_features=2, n_redundant=0,
                           n_informative=2, random_state=42)
model = LogisticRegression()
model.fit(X, y)

# Probabilit√©s de pr√©diction (pas juste 0 ou 1)
probabilities = model.predict_proba(X[:5])
print("Probabilit√©s de classification:")
print(probabilities)
```

---

## Fondements Math√©matiques

### Espace Probabilis√©

Un **espace probabilis√©** est un triplet $(\Omega, \mathcal{A}, \mathbb{P})$ o√π :

1. **$\Omega$** : Espace √©chantillonal (ensemble de tous les r√©sultats possibles)
2. **$\mathcal{A}$** : œÉ-alg√®bre (ensemble des √©v√©nements mesurables)
3. **$\mathbb{P}$** : Mesure de probabilit√©

### œÉ-Alg√®bre

Une famille $\mathcal{A}$ de sous-ensembles de $\Omega$ est une **œÉ-alg√®bre** si elle satisfait :

1. **$\Omega \in \mathcal{A}$** : L'espace total est un √©v√©nement
2. **Stabilit√© par compl√©mentaire** : Si $A \in \mathcal{A}$, alors $A^c \in \mathcal{A}$
3. **Stabilit√© par union** : Si $A, B \in \mathcal{A}$, alors $A \cup B \in \mathcal{A}$

**Les √©l√©ments de $\mathcal{A}$ sont appel√©s √©v√©nements.**

### Mesure de Probabilit√©

Une **mesure de probabilit√©** $\mathbb{P} : \mathcal{A} \to [0,1]$ satisfait les **axiomes de Kolmogorov** :

1. **Non-n√©gativit√©** : $0 \leq \mathbb{P}(A) \leq 1$ pour tout √©v√©nement $A$
2. **Normalisation** : $\mathbb{P}(\Omega) = 1$
3. **Additivit√©** : Pour √©v√©nements disjoints $A_1, A_2, A_3, \ldots$ :

$$
\mathbb{P}(A_1 \cup A_2 \cup A_3 \cup \cdots) = \mathbb{P}(A_1) + \mathbb{P}(A_2) + \mathbb{P}(A_3) + \cdots
$$

### Propri√©t√©s Fondamentales

**Compl√©mentaire** :

$$
\mathbb{P}(A^c) = 1 - \mathbb{P}(A)
$$

**Union (formule d'inclusion-exclusion)** :

$$
\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)
$$

**Monotonie** :

$$
A \subseteq B \Rightarrow \mathbb{P}(A) \leq \mathbb{P}(B)
$$

### Exemple Python

```python
import numpy as np

# Simulation de lancers de d√©
def simuler_de(n_lancers=1000):
    """Simule n lancers d'un d√© √©quilibr√©"""
    return np.random.randint(1, 7, n_lancers)

# Exp√©rience
lancers = simuler_de(10000)

# √âv√©nement A: obtenir un nombre pair
A = (lancers % 2 == 0)
prob_A = np.mean(A)
print(f"P(nombre pair) = {prob_A:.4f} (th√©orique = 0.5)")

# √âv√©nement B: obtenir un nombre ‚â• 4
B = (lancers >= 4)
prob_B = np.mean(B)
print(f"P(‚â•4) = {prob_B:.4f} (th√©orique = 0.5)")

# Intersection A ‚à© B
prob_A_inter_B = np.mean(A & B)
print(f"P(pair ET ‚â•4) = {prob_A_inter_B:.4f} (th√©orique = 1/3)")

# Union A ‚à™ B
prob_A_union_B = np.mean(A | B)
print(f"P(pair OU ‚â•4) = {prob_A_union_B:.4f}")

# V√©rification: P(A‚à™B) = P(A) + P(B) - P(A‚à©B)
prob_union_calculee = prob_A + prob_B - prob_A_inter_B
print(f"V√©rification formule: {prob_union_calculee:.4f}")
```

---

## Probabilit√©s Conditionnelles et Ind√©pendance

### Probabilit√© Conditionnelle

La **probabilit√© conditionnelle** de $A$ sachant $B$ (avec $\mathbb{P}(B) > 0$) :

$$
\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
$$

**Interpr√©tation** : Probabilit√© que $A$ se produise sachant que $B$ s'est produit.

**Formule des probabilit√©s compos√©es** :

$$
\mathbb{P}(A \cap B) = \mathbb{P}(A|B) \cdot \mathbb{P}(B) = \mathbb{P}(B|A) \cdot \mathbb{P}(A)
$$

### Ind√©pendance

Deux √©v√©nements $A$ et $B$ sont **ind√©pendants** si et seulement si :

$$
\mathbb{P}(A \cap B) = \mathbb{P}(A) \cdot \mathbb{P}(B)
$$

**√âquivalence** : $A$ et $B$ ind√©pendants $\Leftrightarrow \mathbb{P}(A|B) = \mathbb{P}(A)$

**Interpr√©tation** : La r√©alisation de $B$ n'apporte aucune information sur $A$.

### Th√©or√®me de Bayes (Forme Simple)

$$
\mathbb{P}(A|B) = \frac{\mathbb{P}(B|A) \cdot \mathbb{P}(A)}{\mathbb{P}(B)}
$$

### Formule des Probabilit√©s Totales

Soit $A_1, \ldots, A_n$ une partition de $\Omega$ (√©v√©nements disjoints dont l'union est $\Omega$), alors :

$$
\mathbb{P}(B) = \sum_{i=1}^{n} \mathbb{P}(B|A_i) \cdot \mathbb{P}(A_i)
$$

### Th√©or√®me de Bayes (Forme G√©n√©rale)

$$
\mathbb{P}(A_i|B) = \frac{\mathbb{P}(B|A_i) \cdot \mathbb{P}(A_i)}{\sum_{j=1}^{n} \mathbb{P}(B|A_j) \cdot \mathbb{P}(A_j)}
$$

### Exemple Pratique : Test M√©dical

```python
import numpy as np

# Param√®tres
P_maladie = 0.01          # P(Malade) = 1%
P_pos_sachant_malade = 0.95   # Sensibilit√© (True Positive Rate)
P_neg_sachant_sain = 0.90     # Sp√©cificit√© (True Negative Rate)

# P(Test+|Sain) = 1 - Sp√©cificit√© (Faux positif)
P_pos_sachant_sain = 1 - P_neg_sachant_sain

# P(Sain)
P_sain = 1 - P_maladie

# Probabilit√© totale: P(Test+)
P_test_positif = (P_pos_sachant_malade * P_maladie +
                  P_pos_sachant_sain * P_sain)

# Th√©or√®me de Bayes: P(Malade|Test+)
P_malade_sachant_pos = (P_pos_sachant_malade * P_maladie) / P_test_positif

print(f"Pr√©valence de la maladie: {P_maladie:.1%}")
print(f"Sensibilit√© du test: {P_pos_sachant_malade:.1%}")
print(f"Sp√©cificit√© du test: {P_neg_sachant_sain:.1%}")
print(f"\nP(Test positif) = {P_test_positif:.4f}")
print(f"P(Malade | Test+) = {P_malade_sachant_pos:.4f} = {P_malade_sachant_pos:.1%}")

# Simulation
n_population = 100000
malade = np.random.random(n_population) < P_maladie

# Test
test_result = np.zeros(n_population, dtype=bool)
test_result[malade] = np.random.random(malade.sum()) < P_pos_sachant_malade
test_result[~malade] = np.random.random((~malade).sum()) < P_pos_sachant_sain

# V√©rification empirique
positifs = test_result
malades_parmi_positifs = malade[positifs]
prob_empirique = malades_parmi_positifs.mean()

print(f"\nV√©rification par simulation: {prob_empirique:.4f} = {prob_empirique:.1%}")
```

---

## Variables Al√©atoires

### D√©finition

Une **variable al√©atoire** $X$ est une fonction qui associe √† chaque r√©sultat de l'espace $\Omega$ une valeur dans un ensemble $E$ :

$$
X : \Omega \to E
$$

**Notations** :

- $\mathbb{P}(X = x)$ d√©signe $\mathbb{P}(\{\omega \in \Omega : X(\omega) = x\})$
- $\mathbb{P}(X \in I)$ d√©signe $\mathbb{P}(\{\omega \in \Omega : X(\omega) \in I\})$

### Fonction de R√©partition (CDF)

La **fonction de r√©partition** (Cumulative Distribution Function) de $X$ :

$$
F_X(t) = \mathbb{P}(X \leq t)
$$

**Propri√©t√©s** :

- $F_X$ est croissante
- $\lim_{t \to -\infty} F_X(t) = 0$ et $\lim_{t \to +\infty} F_X(t) = 1$
- $F_X$ est continue √† droite

### Distribution de Probabilit√©

La **distribution de probabilit√©** de $X$ est l'ensemble :

$$
\{\mathbb{P}(X = x), \, x \in E\}
$$

Elle d√©crit comment la probabilit√© est r√©partie sur les valeurs possibles de $X$.

---

## Variables Discr√®tes

Une variable al√©atoire est **discr√®te** si elle prend un nombre fini ou d√©nombrable de valeurs.

### R√®gle de Somme

Pour toute variable discr√®te $X$ :

$$
\sum_{x \in E} \mathbb{P}(X = x) = 1
$$

### Distribution Marginale

Soit $X$ et $Y$ deux variables discr√®tes :

$$
\mathbb{P}(X = x) = \sum_{y} \mathbb{P}(X = x, Y = y)
$$

### Distribution Conditionnelle

$$
\mathbb{P}(X = x | Y = y) = \frac{\mathbb{P}(X = x, Y = y)}{\mathbb{P}(Y = y)}
$$

### 1. Loi de Bernoulli $\mathcal{B}(p)$

**D√©finition** : Mod√©lise une exp√©rience √† deux issues (succ√®s/√©chec).

$$
X \sim \mathcal{B}(p) \Leftrightarrow
\begin{cases}
\mathbb{P}(X = 1) = p \\
\mathbb{P}(X = 0) = 1 - p
\end{cases}
$$

**Param√®tres** : $p \in [0, 1]$ (probabilit√© de succ√®s)

**Propri√©t√©s** :

- **Esp√©rance** : $\mathbb{E}[X] = p$
- **Variance** : $\mathbb{V}[X] = p(1-p)$

**Exemple** : Lancer de pi√®ce, r√©ussite/√©chec d'un test

```python
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

# Loi de Bernoulli avec p=0.3
p = 0.3
X_bern = stats.bernoulli(p)

# Simulation
n_simulations = 10000
echantillon = X_bern.rvs(n_simulations)

print(f"Bernoulli(p={p})")
print(f"Esp√©rance th√©orique: {p}")
print(f"Esp√©rance empirique: {echantillon.mean():.4f}")
print(f"Variance th√©orique: {p*(1-p):.4f}")
print(f"Variance empirique: {echantillon.var():.4f}")

# Visualisation
valeurs = [0, 1]
probas = [1-p, p]

plt.figure(figsize=(8, 5))
plt.bar(valeurs, probas, width=0.3, alpha=0.7, edgecolor='black')
plt.xlabel('Valeur')
plt.ylabel('Probabilit√©')
plt.title(f'Loi de Bernoulli B({p})')
plt.xticks([0, 1])
plt.grid(axis='y', alpha=0.3)
plt.show()
```

### 2. Loi Binomiale $\mathcal{B}(n, p)$

**D√©finition** : Nombre de succ√®s dans $n$ r√©p√©titions ind√©pendantes d'une exp√©rience de Bernoulli.

$$
X \sim \mathcal{B}(n, p) \Rightarrow \mathbb{P}(X = k) = \binom{n}{k} p^k (1-p)^{n-k}
$$

o√π $\binom{n}{k} = \frac{n!}{k!(n-k)!}$ est le coefficient binomial.

**Param√®tres** :

- $n \in \mathbb{N}$ : nombre d'essais
- $p \in [0, 1]$ : probabilit√© de succ√®s

**Propri√©t√©s** :

- **Esp√©rance** : $\mathbb{E}[X] = np$
- **Variance** : $\mathbb{V}[X] = np(1-p)$

**Exemple** : Nombre de faces dans 10 lancers de pi√®ce

```python
from scipy import stats
import matplotlib.pyplot as plt
import numpy as np

# Loi binomiale B(n=10, p=0.5)
n, p = 10, 0.5
X_binom = stats.binom(n, p)

# PMF (Probability Mass Function)
k_values = np.arange(0, n+1)
probas = X_binom.pmf(k_values)

# Visualisation
plt.figure(figsize=(10, 6))
plt.bar(k_values, probas, alpha=0.7, edgecolor='black')
plt.xlabel('Nombre de succ√®s (k)')
plt.ylabel('Probabilit√© P(X=k)')
plt.title(f'Loi Binomiale B(n={n}, p={p})')
plt.xticks(k_values)
plt.grid(axis='y', alpha=0.3)
plt.axvline(n*p, color='r', linestyle='--', label=f'Esp√©rance = {n*p}')
plt.legend()
plt.show()

# Simulation
echantillon = X_binom.rvs(10000)
print(f"Esp√©rance th√©orique: {n*p}")
print(f"Esp√©rance empirique: {echantillon.mean():.4f}")
print(f"Variance th√©orique: {n*p*(1-p):.4f}")
print(f"Variance empirique: {echantillon.var():.4f}")
```

### 3. Loi G√©om√©trique $\mathcal{G}(p)$

**D√©finition** : Rang du premier succ√®s dans une suite d'essais de Bernoulli ind√©pendants.

$$
X \sim \mathcal{G}(p) \Rightarrow \mathbb{P}(X = k) = p(1-p)^{k-1}, \quad k \in \mathbb{N}^*
$$

**Param√®tres** : $p \in ]0, 1]$ (probabilit√© de succ√®s)

**Propri√©t√©s** :

- **Esp√©rance** : $\mathbb{E}[X] = \frac{1}{p}$
- **Variance** : $\mathbb{V}[X] = \frac{1-p}{p^2}$

**Propri√©t√© sans m√©moire** : $\mathbb{P}(X > n + m | X > n) = \mathbb{P}(X > m)$

**Exemple** : Nombre de lancers avant le premier 6

```python
from scipy import stats
import matplotlib.pyplot as plt
import numpy as np

# Loi g√©om√©trique G(p=1/6)
p = 1/6
X_geom = stats.geom(p)

# PMF
k_values = np.arange(1, 21)
probas = X_geom.pmf(k_values)

# Visualisation
plt.figure(figsize=(10, 6))
plt.bar(k_values, probas, alpha=0.7, edgecolor='black')
plt.xlabel('Nombre d\'essais jusqu\'au premier succ√®s')
plt.ylabel('Probabilit√©')
plt.title(f'Loi G√©om√©trique G(p={p:.4f})')
plt.grid(axis='y', alpha=0.3)
plt.axvline(1/p, color='r', linestyle='--', label=f'Esp√©rance = {1/p:.2f}')
plt.legend()
plt.show()

# Simulation
echantillon = X_geom.rvs(10000)
print(f"Esp√©rance th√©orique: {1/p:.4f}")
print(f"Esp√©rance empirique: {echantillon.mean():.4f}")
```

### 4. Loi de Poisson $\mathcal{P}(\lambda)$

**D√©finition** : Nombre d'√©v√©nements se produisant dans un intervalle de temps fix√©, sachant qu'ils se produisent en moyenne $\lambda$ fois.

$$
X \sim \mathcal{P}(\lambda) \Rightarrow \mathbb{P}(X = k) = \frac{\lambda^k}{k!} e^{-\lambda}, \quad k \in \mathbb{N}
$$

**Param√®tres** : $\lambda > 0$ (taux moyen d'occurrence)

**Propri√©t√©s** :

- **Esp√©rance** : $\mathbb{E}[X] = \lambda$
- **Variance** : $\mathbb{V}[X] = \lambda$

**Exemple** : Nombre d'appels t√©l√©phoniques par heure, nombre d'erreurs par page

```python
from scipy import stats
import matplotlib.pyplot as plt
import numpy as np

# Loi de Poisson P(Œª=3)
lambda_param = 3
X_poisson = stats.poisson(lambda_param)

# PMF
k_values = np.arange(0, 15)
probas = X_poisson.pmf(k_values)

# Visualisation
plt.figure(figsize=(10, 6))
plt.bar(k_values, probas, alpha=0.7, edgecolor='black')
plt.xlabel('Nombre d\'√©v√©nements (k)')
plt.ylabel('Probabilit√© P(X=k)')
plt.title(f'Loi de Poisson P(Œª={lambda_param})')
plt.grid(axis='y', alpha=0.3)
plt.axvline(lambda_param, color='r', linestyle='--',
            label=f'Esp√©rance = Variance = {lambda_param}')
plt.legend()
plt.show()

# Comparaison de diff√©rentes valeurs de Œª
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
lambdas = [1, 4, 10]

for ax, lam in zip(axes, lambdas):
    X = stats.poisson(lam)
    k_vals = np.arange(0, lam*3)
    ax.bar(k_vals, X.pmf(k_vals), alpha=0.7, edgecolor='black')
    ax.set_title(f'Poisson(Œª={lam})')
    ax.set_xlabel('k')
    ax.set_ylabel('P(X=k)')
    ax.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## Variables Continues

Une variable al√©atoire est **continue** si elle prend ses valeurs dans un ensemble continu (typiquement $\mathbb{R}$ ou un intervalle).

### Fonction de Densit√© de Probabilit√© (PDF)

Une fonction $f : \mathbb{R} \to \mathbb{R}_+$ est une **densit√© de probabilit√©** si :

$$
\int_{\mathbb{R}} f(x) \, dx = 1
$$

Pour une variable continue $X$ de densit√© $f$ :

$$
\mathbb{P}(a \leq X \leq b) = \int_a^b f(x) \, dx
$$

**Remarque importante** : $\mathbb{P}(X = x) = 0$ pour tout $x$ (probabilit√© nulle en un point).

### Fonction de R√©partition

$$
F_X(t) = \mathbb{P}(X \leq t) = \int_{-\infty}^{t} f(x) \, dx
$$

**Relation** : $f(x) = F_X'(x)$ (la densit√© est la d√©riv√©e de la fonction de r√©partition)

### 1. Loi Uniforme $\mathcal{U}([a, b])$

**D√©finition** : Tous les points de l'intervalle $[a, b]$ ont la m√™me "densit√© de probabilit√©".

**Densit√©** :

$$
f(x) = \begin{cases}
\frac{1}{b-a} & \text{si } x \in [a, b] \\
0 & \text{sinon}
\end{cases}
$$

**Fonction de r√©partition** :

$$
F_X(t) = \begin{cases}
0 & \text{si } t < a \\
\frac{t-a}{b-a} & \text{si } t \in [a, b] \\
1 & \text{si } t > b
\end{cases}
$$

**Propri√©t√©s** :

- **Esp√©rance** : $\mathbb{E}[X] = \frac{a+b}{2}$
- **Variance** : $\mathbb{V}[X] = \frac{(b-a)^2}{12}$

**Exemple** : Nombre al√©atoire entre 0 et 1

```python
from scipy import stats
import matplotlib.pyplot as plt
import numpy as np

# Loi uniforme U([0, 5])
a, b = 0, 5
X_unif = stats.uniform(a, b-a)

# Visualisation
x = np.linspace(-1, 6, 1000)
pdf = X_unif.pdf(x)
cdf = X_unif.cdf(x)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# PDF
ax1.plot(x, pdf, 'b-', linewidth=2)
ax1.fill_between(x, 0, pdf, alpha=0.3)
ax1.set_xlabel('x')
ax1.set_ylabel('f(x)')
ax1.set_title(f'Densit√© de probabilit√© - Uniforme([{a}, {b}])')
ax1.grid(True, alpha=0.3)
ax1.axvline((a+b)/2, color='r', linestyle='--', label='Esp√©rance')
ax1.legend()

# CDF
ax2.plot(x, cdf, 'g-', linewidth=2)
ax2.set_xlabel('x')
ax2.set_ylabel('F(x)')
ax2.set_title(f'Fonction de r√©partition - Uniforme([{a}, {b}])')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# G√©n√©ration d'√©chantillons
echantillon = X_unif.rvs(10000)
print(f"Esp√©rance th√©orique: {(a+b)/2}")
print(f"Esp√©rance empirique: {echantillon.mean():.4f}")
print(f"Variance th√©orique: {(b-a)**2/12:.4f}")
print(f"Variance empirique: {echantillon.var():.4f}")
```

### 2. Loi Exponentielle $\mathcal{E}(\lambda)$

**D√©finition** : Temps d'attente avant le premier √©v√©nement dans un processus de Poisson.

**Densit√©** :

$$
f(x) = \lambda e^{-\lambda x}, \quad x \geq 0
$$

**Fonction de r√©partition** :

$$
F_X(t) = 1 - e^{-\lambda t}, \quad t \geq 0
$$

**Propri√©t√©s** :

- **Esp√©rance** : $\mathbb{E}[X] = \frac{1}{\lambda}$
- **Variance** : $\mathbb{V}[X] = \frac{1}{\lambda^2}$

**Propri√©t√© sans m√©moire** : $\mathbb{P}(X > s + t | X > s) = \mathbb{P}(X > t)$

**Exemple** : Dur√©e de vie d'un composant √©lectronique

```python
from scipy import stats
import matplotlib.pyplot as plt
import numpy as np

# Loi exponentielle E(Œª=0.5)
lambda_param = 0.5
X_exp = stats.expon(scale=1/lambda_param)

# Visualisation pour diff√©rentes valeurs de Œª
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

lambdas = [0.5, 1.0, 1.5]
x = np.linspace(0, 5, 1000)
colors = ['b', 'g', 'r']

for lam, color in zip(lambdas, colors):
    X = stats.expon(scale=1/lam)
    ax1.plot(x, X.pdf(x), color=color, linewidth=2, label=f'Œª={lam}')
    ax2.plot(x, X.cdf(x), color=color, linewidth=2, label=f'Œª={lam}')

ax1.set_xlabel('x')
ax1.set_ylabel('f(x)')
ax1.set_title('Densit√© - Loi Exponentielle')
ax1.legend()
ax1.grid(True, alpha=0.3)

ax2.set_xlabel('x')
ax2.set_ylabel('F(x)')
ax2.set_title('Fonction de r√©partition - Loi Exponentielle')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Propri√©t√© sans m√©moire
lambda_param = 1.0
X = stats.expon(scale=1/lambda_param)
s, t = 2.0, 1.0

# P(X > s+t | X > s) = P(X > t)
prob_sans_memoire = 1 - X.cdf(t)
prob_conditionnelle = (1 - X.cdf(s+t)) / (1 - X.cdf(s))

print(f"Propri√©t√© sans m√©moire:")
print(f"P(X > {t}) = {prob_sans_memoire:.4f}")
print(f"P(X > {s+t} | X > {s}) = {prob_conditionnelle:.4f}")
```

### 3. Loi Normale (Gaussienne) $\mathcal{N}(\mu, \sigma^2)$

**D√©finition** : Distribution la plus importante en statistiques (Th√©or√®me Central Limite).

**Densit√©** :

$$
f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

**Propri√©t√©s** :

- **Esp√©rance** : $\mathbb{E}[X] = \mu$
- **Variance** : $\mathbb{V}[X] = \sigma^2$
- **Sym√©trie** : Sym√©trique autour de $\mu$
- **Forme en cloche**

**Loi normale standard** $\mathcal{N}(0, 1)$ :

Si $X \sim \mathcal{N}(\mu, \sigma^2)$, alors $Z = \frac{X - \mu}{\sigma} \sim \mathcal{N}(0, 1)$

**R√®gle empirique (68-95-99.7)** :

- 68% des valeurs dans $[\mu - \sigma, \mu + \sigma]$
- 95% des valeurs dans $[\mu - 2\sigma, \mu + 2\sigma]$
- 99.7% des valeurs dans $[\mu - 3\sigma, \mu + 3\sigma]$

```python
from scipy import stats
import matplotlib.pyplot as plt
import numpy as np

# Diff√©rentes lois normales
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. Effet de Œº (moyenne)
x = np.linspace(-10, 10, 1000)
means = [0, 2, -2]
ax = axes[0, 0]
for mu in means:
    X = stats.norm(mu, 1)
    ax.plot(x, X.pdf(x), linewidth=2, label=f'Œº={mu}, œÉ¬≤=1')
ax.set_title('Effet de Œº (moyenne)')
ax.set_xlabel('x')
ax.set_ylabel('f(x)')
ax.legend()
ax.grid(True, alpha=0.3)

# 2. Effet de œÉ¬≤ (variance)
x = np.linspace(-10, 10, 1000)
stds = [0.5, 1, 2]
ax = axes[0, 1]
for sigma in stds:
    X = stats.norm(0, sigma)
    ax.plot(x, X.pdf(x), linewidth=2, label=f'Œº=0, œÉ¬≤={sigma**2}')
ax.set_title('Effet de œÉ¬≤ (variance)')
ax.set_xlabel('x')
ax.set_ylabel('f(x)')
ax.legend()
ax.grid(True, alpha=0.3)

# 3. Fonction de r√©partition
x = np.linspace(-4, 4, 1000)
X_std = stats.norm(0, 1)
ax = axes[1, 0]
ax.plot(x, X_std.cdf(x), 'b-', linewidth=2)
ax.set_title('Fonction de r√©partition - N(0,1)')
ax.set_xlabel('x')
ax.set_ylabel('F(x)')
ax.grid(True, alpha=0.3)
ax.axhline(0.5, color='r', linestyle='--', alpha=0.5)
ax.axvline(0, color='r', linestyle='--', alpha=0.5)

# 4. R√®gle empirique 68-95-99.7
x = np.linspace(-4, 4, 1000)
ax = axes[1, 1]
ax.plot(x, X_std.pdf(x), 'b-', linewidth=2)
ax.fill_between(x, 0, X_std.pdf(x), where=(np.abs(x) <= 1),
                 alpha=0.3, color='green', label='68% (¬±œÉ)')
ax.fill_between(x, 0, X_std.pdf(x), where=(np.abs(x) <= 2),
                 alpha=0.2, color='blue', label='95% (¬±2œÉ)')
ax.fill_between(x, 0, X_std.pdf(x), where=(np.abs(x) <= 3),
                 alpha=0.1, color='red', label='99.7% (¬±3œÉ)')
ax.set_title('R√®gle empirique 68-95-99.7')
ax.set_xlabel('x')
ax.set_ylabel('f(x)')
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# V√©rification num√©rique
mu, sigma = 0, 1
X = stats.norm(mu, sigma)

print("R√®gle empirique:")
print(f"P(Œº-œÉ ‚â§ X ‚â§ Œº+œÉ) = {X.cdf(1) - X.cdf(-1):.4f} ‚âà 0.68")
print(f"P(Œº-2œÉ ‚â§ X ‚â§ Œº+2œÉ) = {X.cdf(2) - X.cdf(-2):.4f} ‚âà 0.95")
print(f"P(Œº-3œÉ ‚â§ X ‚â§ Œº+3œÉ) = {X.cdf(3) - X.cdf(-3):.4f} ‚âà 0.997")
```

### Th√©or√®me Central Limite

**√ânonc√© simplifi√©** : La somme (ou moyenne) d'un grand nombre de variables al√©atoires ind√©pendantes tend vers une loi normale.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# D√©monstration du TCL avec des uniformes
n_samples = 10000
sample_sizes = [1, 2, 5, 30]

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.ravel()

for idx, n in enumerate(sample_sizes):
    # G√©n√©rer n variables uniformes et calculer leur moyenne
    moyennes = np.mean(np.random.uniform(0, 1, (n_samples, n)), axis=1)

    ax = axes[idx]
    ax.hist(moyennes, bins=50, density=True, alpha=0.7, edgecolor='black')

    # Superposer la loi normale th√©orique
    mu = 0.5  # E[Unif(0,1)] = 0.5
    sigma = np.sqrt(1/12 / n)  # Var[Unif(0,1)] / n
    x = np.linspace(moyennes.min(), moyennes.max(), 100)
    ax.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2,
            label='Normale th√©orique')

    ax.set_title(f'Moyenne de {n} variables uniformes')
    ax.set_xlabel('Valeur de la moyenne')
    ax.set_ylabel('Densit√©')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## Esp√©rance et Variance

### Esp√©rance (Valeur Moyenne)

**Variable discr√®te** :

$$
\mathbb{E}[X] = \sum_{x \in E} x \cdot \mathbb{P}(X = x)
$$

**Variable continue** :

$$
\mathbb{E}[X] = \int_{-\infty}^{+\infty} x \cdot f(x) \, dx
$$

**Interpr√©tation** : Valeur moyenne que prendrait $X$ sur un grand nombre de r√©p√©titions.

### Propri√©t√©s de l'Esp√©rance

1. **Lin√©arit√©** : $\mathbb{E}[aX + b] = a\mathbb{E}[X] + b$
2. **Additivit√©** : $\mathbb{E}[X + Y] = \mathbb{E}[X] + \mathbb{E}[Y]$
3. **Constante** : $\mathbb{E}[c] = c$
4. **Positivit√©** : Si $X \geq 0$ alors $\mathbb{E}[X] \geq 0$
5. **Monotonie** : Si $X \geq Y$ alors $\mathbb{E}[X] \geq \mathbb{E}[Y]$

**Si $X$ et $Y$ sont ind√©pendantes** :

$$
\mathbb{E}[XY] = \mathbb{E}[X] \cdot \mathbb{E}[Y]
$$

### Variance (Dispersion)

$$
\mathbb{V}[X] = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2
$$

**Interpr√©tation** : Mesure de la dispersion autour de la moyenne.

### Propri√©t√©s de la Variance

1. **Translation** : $\mathbb{V}[X + b] = \mathbb{V}[X]$
2. **Homoth√©tie** : $\mathbb{V}[aX] = a^2 \mathbb{V}[X]$
3. **Positivit√©** : $\mathbb{V}[X] \geq 0$
4. **Nullit√©** : $\mathbb{V}[X] = 0 \Leftrightarrow X$ est constante

**Si $X$ et $Y$ sont ind√©pendantes** :

$$
\mathbb{V}[X + Y] = \mathbb{V}[X] + \mathbb{V}[Y]
$$

### √âcart-Type

$$
\sigma(X) = \sqrt{\mathbb{V}[X]}
$$

**Avantage** : M√™me unit√© que $X$ (contrairement √† la variance).

### Exemple Python

```python
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

# Comparer diff√©rentes distributions avec m√™me esp√©rance
mu = 5

# Distribution 1: Normale avec faible variance
X1 = stats.norm(mu, 0.5)

# Distribution 2: Normale avec forte variance
X2 = stats.norm(mu, 2)

# Distribution 3: Uniforme centr√©e sur mu
a, b = mu - 3, mu + 3
X3 = stats.uniform(a, b-a)

# Visualisation
x = np.linspace(0, 10, 1000)

plt.figure(figsize=(12, 6))
plt.plot(x, X1.pdf(x), label=f'N({mu}, 0.25) - œÉ=0.5', linewidth=2)
plt.plot(x, X2.pdf(x), label=f'N({mu}, 4) - œÉ=2', linewidth=2)
plt.plot(x, X3.pdf(x), label=f'U([{a},{b}]) - œÉ={np.sqrt((b-a)**2/12):.2f}', linewidth=2)

plt.axvline(mu, color='r', linestyle='--', linewidth=2, label='Esp√©rance commune')
plt.xlabel('x')
plt.ylabel('Densit√©')
plt.title('Distributions avec m√™me esp√©rance mais variances diff√©rentes')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Simulation
n_samples = 100000
samples1 = X1.rvs(n_samples)
samples2 = X2.rvs(n_samples)
samples3 = X3.rvs(n_samples)

print("Esp√©rance empirique:")
print(f"X1: {samples1.mean():.4f}")
print(f"X2: {samples2.mean():.4f}")
print(f"X3: {samples3.mean():.4f}")

print("\nVariance empirique:")
print(f"X1: {samples1.var():.4f} (th√©orique: 0.25)")
print(f"X2: {samples2.var():.4f} (th√©orique: 4)")
print(f"X3: {samples3.var():.4f} (th√©orique: {(b-a)**2/12:.4f})")
```

---

## Vecteurs Al√©atoires

### D√©finition

Un **vecteur al√©atoire** $\mathbf{X} = (X_1, \ldots, X_n)$ est un vecteur dont les composantes sont des variables al√©atoires.

### Fonction de R√©partition Jointe

$$
F_{\mathbf{X}}(t_1, \ldots, t_n) = \mathbb{P}(X_1 \leq t_1, \ldots, X_n \leq t_n)
$$

### Densit√© Jointe

Pour un vecteur al√©atoire continu :

$$
\mathbb{P}(\mathbf{X} \in A) = \int_A f(x_1, \ldots, x_n) \, dx_1 \cdots dx_n
$$

### Esp√©rance d'un Vecteur

$$
\mathbb{E}[\mathbf{X}] = (\mathbb{E}[X_1], \ldots, \mathbb{E}[X_n]) \in \mathbb{R}^n
$$

### Ind√©pendance

Les variables $X_1, \ldots, X_n$ sont **ind√©pendantes** si et seulement si :

$$
f_{\mathbf{X}}(x_1, \ldots, x_n) = \prod_{i=1}^{n} f_{X_i}(x_i)
$$

**Cons√©quence** : Si $X_1, \ldots, X_n$ ind√©pendantes alors :

$$
\forall i \neq j, \quad \text{Cov}(X_i, X_j) = 0
$$

### Loi Normale Multivari√©e

$$
\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})
$$

**Densit√©** :

$$
f(\mathbf{x}) = \frac{1}{\sqrt{(2\pi)^d \det(\boldsymbol{\Sigma})}} e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu})}
$$

o√π :

- $\boldsymbol{\mu} \in \mathbb{R}^d$ : vecteur des moyennes
- $\boldsymbol{\Sigma} \in \mathbb{R}^{d \times d}$ : matrice de covariance (sym√©trique d√©finie positive)

```python
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

# Loi normale bivari√©e
mu = np.array([0, 0])
Sigma = np.array([[1, 0.8],
                   [0.8, 1]])

# Cr√©ation de la distribution
X = stats.multivariate_normal(mu, Sigma)

# Grille pour visualisation
x1 = np.linspace(-3, 3, 100)
x2 = np.linspace(-3, 3, 100)
X1, X2 = np.meshgrid(x1, x2)
pos = np.dstack((X1, X2))

# Densit√©
Z = X.pdf(pos)

# Visualisation 3D
fig = plt.figure(figsize=(14, 6))

# Surface 3D
ax1 = fig.add_subplot(121, projection='3d')
ax1.plot_surface(X1, X2, Z, cmap='viridis', alpha=0.8)
ax1.set_xlabel('X‚ÇÅ')
ax1.set_ylabel('X‚ÇÇ')
ax1.set_zlabel('Densit√©')
ax1.set_title('Densit√© normale bivari√©e')

# Contours
ax2 = fig.add_subplot(122)
contour = ax2.contour(X1, X2, Z, levels=10, cmap='viridis')
ax2.clabel(contour, inline=True, fontsize=8)
ax2.set_xlabel('X‚ÇÅ')
ax2.set_ylabel('X‚ÇÇ')
ax2.set_title('Lignes de niveau')
ax2.grid(True, alpha=0.3)
ax2.axis('equal')

plt.tight_layout()
plt.show()

# √âchantillonnage
samples = X.rvs(1000)
plt.figure(figsize=(8, 6))
plt.scatter(samples[:, 0], samples[:, 1], alpha=0.5)
plt.xlabel('X‚ÇÅ')
plt.ylabel('X‚ÇÇ')
plt.title('1000 √©chantillons de la loi normale bivari√©e')
plt.grid(True, alpha=0.3)
plt.axis('equal')
plt.show()
```

---

## Covariance et Corr√©lation

### Covariance

La **covariance** entre deux variables $X$ et $Y$ :

$$
\text{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
$$

**Interpr√©tation** :

- $\text{Cov}(X, Y) > 0$ : $X$ et $Y$ varient dans le m√™me sens
- $\text{Cov}(X, Y) < 0$ : $X$ et $Y$ varient en sens oppos√©
- $\text{Cov}(X, Y) = 0$ : $X$ et $Y$ ne sont pas lin√©airement li√©es

### Propri√©t√©s de la Covariance

1. **Sym√©trie** : $\text{Cov}(X, Y) = \text{Cov}(Y, X)$
2. **Avec soi-m√™me** : $\text{Cov}(X, X) = \mathbb{V}[X]$
3. **Bilin√©arit√©** : $\text{Cov}(aX + b, cY + d) = ac \cdot \text{Cov}(X, Y)$
4. **Variance d'une somme** : $\mathbb{V}[X + Y] = \mathbb{V}[X] + \mathbb{V}[Y] + 2\text{Cov}(X, Y)$

### Coefficient de Corr√©lation

$$
\rho(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\mathbb{V}[X]} \sqrt{\mathbb{V}[Y]}} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}
$$

**Propri√©t√©s** :

- $\rho(X, Y) \in [-1, 1]$
- $|\rho(X, Y)| = 1$ : relation lin√©aire parfaite
- $\rho(X, Y) = 0$ : variables non corr√©l√©es (mais pas n√©cessairement ind√©pendantes !)

**Interpr√©tation** :

- $\rho = 1$ : corr√©lation positive parfaite ($Y = aX + b$ avec $a > 0$)
- $\rho = -1$ : corr√©lation n√©gative parfaite ($Y = aX + b$ avec $a < 0$)
- $\rho = 0$ : pas de corr√©lation lin√©aire

### Matrice de Covariance

Pour un vecteur al√©atoire $\mathbf{X} = (X_1, \ldots, X_n)$ :

$$
\boldsymbol{\Sigma} = \text{Cov}(\mathbf{X}) = (\text{Cov}(X_i, X_j))_{1 \leq i,j \leq n}
$$

**Propri√©t√©s** :

- $\boldsymbol{\Sigma}$ est **sym√©trique** : $\boldsymbol{\Sigma}^T = \boldsymbol{\Sigma}$
- $\boldsymbol{\Sigma}$ est **semi-d√©finie positive**

### Exemples Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# G√©n√©rer des donn√©es avec diff√©rentes corr√©lations
n = 500
correlations = [0.9, 0.5, 0, -0.5, -0.9]

fig, axes = plt.subplots(1, 5, figsize=(20, 4))

for ax, rho in zip(axes, correlations):
    # Matrice de covariance
    Sigma = np.array([[1, rho],
                       [rho, 1]])

    # G√©n√©rer √©chantillon
    samples = np.random.multivariate_normal([0, 0], Sigma, n)

    # Calcul corr√©lation empirique
    rho_empirique = np.corrcoef(samples.T)[0, 1]

    # Visualisation
    ax.scatter(samples[:, 0], samples[:, 1], alpha=0.5)
    ax.set_title(f'œÅ = {rho} (empirique: {rho_empirique:.2f})')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.grid(True, alpha=0.3)
    ax.axis('equal')
    ax.set_xlim(-3, 3)
    ax.set_ylim(-3, 3)

plt.tight_layout()
plt.show()

# Matrice de covariance pour 3 variables
n = 1000
mu = np.array([0, 0, 0])
Sigma = np.array([[1.0, 0.8, 0.3],
                   [0.8, 1.0, 0.5],
                   [0.3, 0.5, 1.0]])

samples = np.random.multivariate_normal(mu, Sigma, n)

# Matrice de covariance empirique
cov_empirique = np.cov(samples.T)

# Matrice de corr√©lation empirique
corr_empirique = np.corrcoef(samples.T)

print("Matrice de covariance th√©orique:")
print(Sigma)
print("\nMatrice de covariance empirique:")
print(cov_empirique)
print("\nMatrice de corr√©lation empirique:")
print(corr_empirique)

# Visualisation avec heatmap
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

im1 = ax1.imshow(cov_empirique, cmap='coolwarm', vmin=-1, vmax=1)
ax1.set_title('Matrice de covariance')
for i in range(3):
    for j in range(3):
        text = ax1.text(j, i, f'{cov_empirique[i, j]:.2f}',
                       ha="center", va="center", color="black")
plt.colorbar(im1, ax=ax1)

im2 = ax2.imshow(corr_empirique, cmap='coolwarm', vmin=-1, vmax=1)
ax2.set_title('Matrice de corr√©lation')
for i in range(3):
    for j in range(3):
        text = ax2.text(j, i, f'{corr_empirique[i, j]:.2f}',
                       ha="center", va="center", color="black")
plt.colorbar(im2, ax=ax2)

plt.tight_layout()
plt.show()
```

**Attention** : Corr√©lation nulle n'implique PAS ind√©pendance !

```python
# Exemple: variables non corr√©l√©es mais d√©pendantes
n = 1000
X = np.random.uniform(-2, 2, n)
Y = X**2 + np.random.normal(0, 0.1, n)

# Corr√©lation
rho = np.corrcoef(X, Y)[0, 1]

plt.figure(figsize=(8, 6))
plt.scatter(X, Y, alpha=0.5)
plt.xlabel('X')
plt.ylabel('Y = X¬≤')
plt.title(f'Variables d√©pendantes mais non corr√©l√©es (œÅ = {rho:.3f})')
plt.grid(True, alpha=0.3)
plt.show()

print(f"Corr√©lation: {rho:.4f} (proche de 0)")
print("Pourtant Y d√©pend clairement de X (relation quadratique) !")
```

---

## Th√©or√®me de Bayes

### Formulation G√©n√©rale

Soit $A_1, \ldots, A_n$ une partition de $\Omega$ (√©v√©nements disjoints avec $\bigcup_{i=1}^n A_i = \Omega$).

**Th√©or√®me de Bayes** :

$$
\mathbb{P}(A_i | B) = \frac{\mathbb{P}(B|A_i) \cdot \mathbb{P}(A_i)}{\sum_{j=1}^{n} \mathbb{P}(B|A_j) \cdot \mathbb{P}(A_j)}
$$

**Terminologie** :

- $\mathbb{P}(A_i)$ : **Probabilit√© a priori** (avant observation de $B$)
- $\mathbb{P}(A_i|B)$ : **Probabilit√© a posteriori** (apr√®s observation de $B$)
- $\mathbb{P}(B|A_i)$ : **Vraisemblance** (likelihood)
- $\mathbb{P}(B)$ : **√âvidence** (probabilit√© marginale)

### Formulation pour Variables Continues

Soit $\Theta$ un param√®tre et $X$ une observation :

$$
f(\theta | x) = \frac{f(x|\theta) \cdot f(\theta)}{\int f(x|\theta') \cdot f(\theta') \, d\theta'}
$$

o√π :

- $f(\theta)$ : **distribution a priori**
- $f(x|\theta)$ : **vraisemblance**
- $f(\theta|x)$ : **distribution a posteriori**

### Application : Classification Bay√©sienne Na√Øve

**Hypoth√®se na√Øve** : Les features sont conditionnellement ind√©pendantes sachant la classe.

$$
\mathbb{P}(C_k | x_1, \ldots, x_n) \propto \mathbb{P}(C_k) \prod_{i=1}^{n} \mathbb{P}(x_i | C_k)
$$

### Exemple Complet : Diagnostic M√©dical

```python
import numpy as np
import matplotlib.pyplot as plt

def diagnostic_bayesien(prevalence, sensibilite, specificite):
    """
    Calcul de P(Malade|Test+) en utilisant le th√©or√®me de Bayes

    Parameters:
    - prevalence: P(Malade)
    - sensibilite: P(Test+|Malade) (True Positive Rate)
    - specificite: P(Test-|Sain) (True Negative Rate)
    """
    # P(Test+|Sain) = 1 - Sp√©cificit√© (Faux positif)
    P_pos_sain = 1 - specificite

    # P(Sain)
    P_sain = 1 - prevalence

    # P(Test+) = P(Test+|Malade)P(Malade) + P(Test+|Sain)P(Sain)
    P_test_pos = sensibilite * prevalence + P_pos_sain * P_sain

    # Bayes: P(Malade|Test+)
    P_malade_test_pos = (sensibilite * prevalence) / P_test_pos

    return P_malade_test_pos

# √âtude de l'effet de la pr√©valence
prevalences = np.linspace(0.001, 0.1, 100)
sensibilite = 0.95
specificite = 0.90

probs_posteriori = [diagnostic_bayesien(p, sensibilite, specificite)
                     for p in prevalences]

plt.figure(figsize=(10, 6))
plt.plot(prevalences * 100, np.array(probs_posteriori) * 100,
         linewidth=2, label='P(Malade|Test+)')
plt.axhline(sensibilite * 100, color='r', linestyle='--',
            label=f'Sensibilit√© = {sensibilite:.0%}')
plt.xlabel('Pr√©valence (%)')
plt.ylabel('P(Malade|Test+) (%)')
plt.title(f'Probabilit√© d\'√™tre malade sachant test positif\n(Sensibilit√©={sensibilite:.0%}, Sp√©cificit√©={specificite:.0%})')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Cas concret
print("="*60)
print("CAS PRATIQUE: Test COVID-19")
print("="*60)
prevalence = 0.05  # 5% de la population infect√©e
sensibilite = 0.95  # 95% de d√©tection des vrais positifs
specificite = 0.98  # 98% de d√©tection des vrais n√©gatifs

P_malade_test_pos = diagnostic_bayesien(prevalence, sensibilite, specificite)

print(f"\nPr√©valence: {prevalence:.1%}")
print(f"Sensibilit√©: {sensibilite:.1%}")
print(f"Sp√©cificit√©: {specificite:.1%}")
print(f"\nSi le test est positif:")
print(f"P(Malade|Test+) = {P_malade_test_pos:.1%}")
print(f"P(Sain|Test+) = {1-P_malade_test_pos:.1%}")

# Validation par simulation
n = 100000
malades = np.random.random(n) < prevalence

# R√©sultats des tests
tests = np.zeros(n, dtype=bool)
tests[malades] = np.random.random(malades.sum()) < sensibilite
tests[~malades] = np.random.random((~malades).sum()) < (1-specificite)

# P(Malade|Test+)
pos_et_malade = malades[tests].sum()
total_pos = tests.sum()
prob_simulation = pos_et_malade / total_pos

print(f"\nValidation par simulation (n={n}):")
print(f"P(Malade|Test+) = {prob_simulation:.1%}")
```

### Naive Bayes Classifier

```python
from sklearn.naive_bayes import GaussianNB
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# G√©n√©rer donn√©es
X, y = make_classification(n_samples=1000, n_features=4, n_informative=3,
                           n_redundant=0, n_classes=3, random_state=42)

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Mod√®le Naive Bayes
model = GaussianNB()
model.fit(X_train, y_train)

# Pr√©dictions
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)

# √âvaluation
print("Classification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Exemple de probabilit√©s a posteriori
print("\nExemples de probabilit√©s a posteriori:")
for i in range(5):
    print(f"Exemple {i+1}: P(Classe 0)={y_proba[i,0]:.3f}, "
          f"P(Classe 1)={y_proba[i,1]:.3f}, P(Classe 2)={y_proba[i,2]:.3f}")
```

---

## Applications au Machine Learning

### 1. Maximum de Vraisemblance (MLE)

**Principe** : Trouver les param√®tres qui maximisent la probabilit√© d'observer les donn√©es.

$$
\hat{\theta}_{\text{MLE}} = \arg\max_{\theta} \mathbb{P}(X_1, \ldots, X_n | \theta)
$$

```python
import numpy as np
from scipy import stats

# Donn√©es observ√©es (suppos√©es Gaussiennes)
data = np.array([1.2, 2.3, 1.8, 2.1, 1.9, 2.5, 1.7, 2.2])

# MLE pour Œº et œÉ¬≤ d'une loi normale
mu_mle = np.mean(data)
sigma2_mle = np.var(data, ddof=0)  # ddof=0 pour MLE (pas estimateur non biais√©)

print(f"MLE: ŒºÃÇ = {mu_mle:.4f}, œÉÃÇ¬≤ = {sigma2_mle:.4f}")

# Comparaison avec scipy
params = stats.norm.fit(data)
print(f"Scipy: ŒºÃÇ = {params[0]:.4f}, œÉÃÇ = {params[1]:.4f}")
```

### 2. Maximum A Posteriori (MAP)

**Principe** : Trouver les param√®tres qui maximisent la probabilit√© a posteriori.

$$
\hat{\theta}_{\text{MAP}} = \arg\max_{\theta} \mathbb{P}(\theta | X_1, \ldots, X_n) \propto \arg\max_{\theta} \mathbb{P}(X_1, \ldots, X_n | \theta) \cdot \mathbb{P}(\theta)
$$

### 3. Intervalles de Confiance

```python
from scipy import stats
import numpy as np

# Donn√©es
data = np.random.normal(5, 2, 100)

# Intervalle de confiance √† 95% pour la moyenne
conf_level = 0.95
mean = np.mean(data)
se = stats.sem(data)  # Standard error
interval = stats.t.interval(conf_level, len(data)-1, loc=mean, scale=se)

print(f"Moyenne: {mean:.4f}")
print(f"Intervalle de confiance √† 95%: [{interval[0]:.4f}, {interval[1]:.4f}]")
```

### 4. Tests d'Hypoth√®ses

```python
from scipy import stats

# Deux √©chantillons
sample1 = np.random.normal(5, 1, 100)
sample2 = np.random.normal(5.3, 1, 100)

# Test t de Student (H0: les moyennes sont √©gales)
t_stat, p_value = stats.ttest_ind(sample1, sample2)

print(f"t-statistic: {t_stat:.4f}")
print(f"p-value: {p_value:.4f}")

if p_value < 0.05:
    print("Rejet de H0: Les moyennes sont significativement diff√©rentes")
else:
    print("On ne rejette pas H0: Pas de diff√©rence significative")
```

### 5. Mod√®le G√©n√©ratif vs Discriminatif

**Mod√®le g√©n√©ratif** (Naive Bayes, GMM) : Mod√©lise $P(X, Y)$ ou $P(X|Y)$ et $P(Y)$

**Mod√®le discriminatif** (R√©gression logistique, SVM) : Mod√©lise directement $P(Y|X)$

---

## Exercices Pratiques

### Exercice 1 : Lois de Probabilit√©

**√ânonc√©** : On lance un d√© √©quilibr√© 20 fois. Quelle est la probabilit√© d'obtenir exactement 5 fois le chiffre 6 ?

**Solution** :

```python
from scipy import stats

# Param√®tres
n = 20  # Nombre de lancers
p = 1/6  # Probabilit√© d'obtenir un 6
k = 5    # Nombre de 6 souhait√©s

# Loi binomiale
X = stats.binom(n, p)

# Probabilit√©
prob = X.pmf(k)
print(f"P(X = {k}) = {prob:.6f} = {prob:.2%}")

# V√©rification par simulation
n_simulations = 100000
resultats = np.random.binomial(n, p, n_simulations)
prob_simulation = (resultats == k).mean()
print(f"Simulation: {prob_simulation:.6f}")
```

### Exercice 2 : Th√©or√®me de Bayes

**√ânonc√©** : Une usine a 3 machines A, B, C qui produisent respectivement 50%, 30%, 20% de la production totale. Les taux de d√©fauts sont 2%, 3%, 4% respectivement. Un produit est tir√© au hasard et est d√©fectueux. Quelle est la probabilit√© qu'il provienne de la machine A ?

**Solution** :

```python
# Probabilit√©s a priori
P_A = 0.50
P_B = 0.30
P_C = 0.20

# Vraisemblances
P_D_A = 0.02  # P(D√©faut|A)
P_D_B = 0.03
P_D_C = 0.04

# P(D√©faut) - Probabilit√© totale
P_D = P_D_A * P_A + P_D_B * P_B + P_D_C * P_C

# Bayes: P(A|D√©faut)
P_A_D = (P_D_A * P_A) / P_D

print(f"P(Machine A | D√©fectueux) = {P_A_D:.4f} = {P_A_D:.1%}")
print(f"P(Machine B | D√©fectueux) = {(P_D_B * P_B) / P_D:.1%}")
print(f"P(Machine C | D√©fectueux) = {(P_D_C * P_C) / P_D:.1%}")
```

### Exercice 3 : Covariance et Corr√©lation

**√ânonc√©** : G√©n√©rer deux variables al√©atoires normales avec une corr√©lation de 0.7 et v√©rifier empiriquement.

**Solution** :

```python
import numpy as np

# Param√®tres
n = 10000
rho = 0.7

# Matrice de covariance
Sigma = np.array([[1, rho],
                   [rho, 1]])

# G√©n√©ration
samples = np.random.multivariate_normal([0, 0], Sigma, n)

# Corr√©lation empirique
corr_empirique = np.corrcoef(samples.T)[0, 1]
cov_empirique = np.cov(samples.T)[0, 1]

print(f"Corr√©lation th√©orique: {rho}")
print(f"Corr√©lation empirique: {corr_empirique:.4f}")
print(f"Covariance empirique: {cov_empirique:.4f}")

# Visualisation
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.scatter(samples[:, 0], samples[:, 1], alpha=0.3)
plt.xlabel('X')
plt.ylabel('Y')
plt.title(f'Variables normales corr√©l√©es (œÅ = {rho})')
plt.grid(True, alpha=0.3)
plt.axis('equal')
plt.show()
```

---

## R√©sum√©

### Points Cl√©s √† Retenir

1. **Fondements** :

   - Espace probabilis√© $(\Omega, \mathcal{A}, \mathbb{P})$
   - Axiomes de Kolmogorov
   - Probabilit√©s conditionnelles

2. **Variables Al√©atoires** :

   - **Discr√®tes** : PMF $\mathbb{P}(X = x)$
   - **Continues** : PDF $f(x)$, CDF $F_X(t)$

3. **Lois Discr√®tes Importantes** :

   - **Bernoulli** $\mathcal{B}(p)$ : $\mathbb{E}=p$, $\mathbb{V}=p(1-p)$
   - **Binomiale** $\mathcal{B}(n,p)$ : $\mathbb{E}=np$, $\mathbb{V}=np(1-p)$
   - **Poisson** $\mathcal{P}(\lambda)$ : $\mathbb{E}=\mathbb{V}=\lambda$

4. **Lois Continues Importantes** :

   - **Uniforme** $\mathcal{U}([a,b])$ : $\mathbb{E}=\frac{a+b}{2}$
   - **Exponentielle** $\mathcal{E}(\lambda)$ : $\mathbb{E}=\frac{1}{\lambda}$
   - **Normale** $\mathcal{N}(\mu, \sigma^2)$ : $\mathbb{E}=\mu$, $\mathbb{V}=\sigma^2$

5. **Moments** :

   - **Esp√©rance** : $\mathbb{E}[X]$ (valeur moyenne)
   - **Variance** : $\mathbb{V}[X] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$

6. **Vecteurs Al√©atoires** :

   - **Covariance** : $\text{Cov}(X,Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]$
   - **Corr√©lation** : $\rho(X,Y) = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y} \in [-1,1]$
   - **Matrice de covariance** : sym√©trique, semi-d√©finie positive

7. **Th√©or√®me de Bayes** :

$$
\mathbb{P}(A|B) = \frac{\mathbb{P}(B|A) \cdot \mathbb{P}(A)}{\mathbb{P}(B)}
$$

### Applications ML Essentielles

| Concept                  | Application ML                                  |
| ------------------------ | ----------------------------------------------- |
| Loi Normale              | Hypoth√®se dans de nombreux mod√®les (r√©gression) |
| Bernoulli/Binomiale      | Classification binaire                          |
| Poisson                  | Mod√©lisation d'√©v√©nements rares                 |
| Th√©or√®me de Bayes        | Classificateurs bay√©siens, filtres spam         |
| Covariance               | PCA, analyse de corr√©lation                     |
| Maximum de vraisemblance | Estimation de param√®tres                        |
| Loi normale multivari√©e  | Mod√®les g√©n√©ratifs gaussiens                    |

### Checklist de Comp√©tences

- [ ] Comprendre les axiomes de probabilit√©
- [ ] Calculer probabilit√©s conditionnelles
- [ ] Utiliser le th√©or√®me de Bayes
- [ ] Identifier et utiliser les lois de probabilit√© courantes
- [ ] Calculer esp√©rance et variance
- [ ] Manipuler vecteurs al√©atoires
- [ ] Calculer et interpr√©ter covariance et corr√©lation
- [ ] Appliquer ces concepts en ML (MLE, MAP, Naive Bayes)

### Formules Essentielles √† Retenir

```
P(A|B) = P(A‚à©B) / P(B)

E[X] = Œ£ x¬∑P(X=x)  (discret)
E[X] = ‚à´ x¬∑f(x)dx  (continu)

Var[X] = E[X¬≤] - (E[X])¬≤

Cov(X,Y) = E[XY] - E[X]E[Y]

œÅ(X,Y) = Cov(X,Y) / (œÉ_X œÉ_Y)
```

### Biblioth√®ques Python Essentielles

```python
import numpy as np                 # Calculs num√©riques
from scipy import stats            # Distributions, tests statistiques
import matplotlib.pyplot as plt    # Visualisation
import seaborn as sns              # Visualisation statistique
from sklearn.naive_bayes import *  # Classificateurs bay√©siens
```

### Prochaine √âtape

**Module 4 : Statistiques Descriptives** - Analyse exploratoire des donn√©es

---

**Navigation :**

- [‚¨ÖÔ∏è Module 2 : Alg√®bre Lin√©aire](02_Algebre_Lineaire.md)
- [üè† Retour au Sommaire](README_ML.md)
- [‚û°Ô∏è Module 4 : Statistiques Descriptives](04_Statistiques_Descriptives.md)
